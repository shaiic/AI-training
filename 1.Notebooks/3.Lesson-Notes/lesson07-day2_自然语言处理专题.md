lesson07-day2_计算机视觉专题

## 57

这个今天这么大的雨，大家还能准时到场，我绝对相信是慕名而来，大家应该也知道今天我不去我们授课的是我们亚洲研究院的周院长，大家掌声鼓励一下，那个周院长的抬头太复杂，所以我只能照着念，首先它是国际顶级的自然语言处理专家。大家都可以看得见我们的课程。有一个很著名的很重要的抬头，是国际计算机语言协会的主席。带领团队进行了微软的输入法音库词典，第一个词典中英翻译，微软对联，微软绝句等一系列大家耳熟能详的重要产品的研发，并且对微软OFFICE必应搜索引擎的自然语言作出了重大贡献。

所以今天，我们有请到那个周院长还有他所带领的团队，给我们来上微软亚洲研究院的自然语言处理这个课程，对今天课程非常清晰，刚才我在边上这样跟我讲，他说其实现在的技术更新的非常快，如果再过半年可能开新的班，所以我想在座各位今天非常有幸，学到了目前来说这样已经最新的技术，所以我也不多说了，好吧，大家掌声再次欢迎。

今天我们讲座的主题就是自然处理，这里，大家可以看到就是我们今天课程的一个安排，介绍自然语言的历史，发展趋势，然后呢我请段楠介绍一下nlp的一些概要，处理。对包括词嵌入解码与训练模型等目前最重要资源技术。

然后，会接着讲问答系统，是我们一般英文叫QA包括问题的理解知识图谱排序对比等，这就是基本是咱们上午的安排。

然后两点左右，就是开始由我们刘树杰高级研究员做机器翻译，包括神经网络，机器翻译的概要，还有机器翻译册，然后，刘树杰会跟大家有自主自由的讨论环节，大家可以针对自然语言处理的一些一般的问题，或者是专门针对今天这几门课问题，作一些讨论。
那我跟段楠的讲完了之后我们有别的事情就可能期间离开了，好那就进入这个本次讲座的第一堂课，就是这样自然语言，可能大家不是特别熟悉自然语言，所以先简单介绍一下自然语言处理。

先把我们盖茨微软创始人，一句著名的话大家说一下，他说自然语言是人工智能的皇冠上明珠，他非常非常欣赏自然语言处理，为什么？微软基本上是靠自然语言处理起家的，大家知道最开始就是自然语言拼写检查、语法检查、后面做什么输入法，还有搜索引擎，所以他认为就是来源处理是对微软最最关键的技术。我就把他的话理解，翻译成中文就是自然语言处理是人工智能明珠。

计算机对人类语言理解，听说，读写的，其实这个不光是对咱们微软重要，对所有计算机行业，互联网都非常重要。

讲两个著名的例子，大家知道他还是不容易做，做冬天能穿多少穿多少，夏天能穿多少穿多少是吧，第二个就是我比较喜欢第二个就是剩女和剩男产生的原因有两个，一是谁都看不上，一个是谁都看不上。那你就说好这两句话，你看前后有这前后两个分句，他们的剧用词几乎完全完全一样，意思截然相反，你说你还让我活吗，大家都说的人话，我就是听不懂，但是我们人听听明白了，这是为什么，人有背景知识。

最基本的常识和冬天就要多穿点，夏天就要少穿点。电脑就不行，电脑没有常识，没有知识就一个字符串，基本上很难做到很好的，这也是我们这个学科的难度。

所以说，你们搞自然语言处理的有什么了不起，图像那么复杂图像输入进去就是识别他有没有错，但是真自然语言这个，难度真的是差一点，我想多说几句，第一个自然语言需要常识，自然语言是多轮的，跟着要讲很多句话。
一开始都是用一种规则去做一种语言，按规则写词典，写那个语法语义，比如说做机器翻译，什么样的词在什么情况下发生，一个意思是什么等等。

然后可以语音有关系，语音的时候就是1970年到1980的时候，语音也不好直接分开，80年代，著名的就是日本的机器翻译带动的这个日本的发言，日本就跟中国马来西亚的很多国家，带动了一件事就是大家要做大规模的数据库。美国大规模的数据库就是那时开始的，但是它实际上是失败的，那个日本那个五国语言翻译计划也是没做出特别令人振奋了不起的事情，但是它对后来人们思索点应该怎么做产生影响。

一直到2008年前后全部转入统计。理论基础就是统计学习，到了那当时我们做机器翻译的人用了统计，这用统计方法做翻译，一下子就比用规则方法做这些翻译提高了很大，而且扩展性强。

因为你把语法资料准备好，自己做了中英翻译再做英法，就完事儿了。你要用规则的话，你每一种语言不完美，要找人写规则，那代价非常之大了。 然后到2008年，一直到现在神经网络技术一点点引入到100%，大家都知道神经网络一开始是一点不像，李飞飞做那个数据库一下提高，我们研究院同事让赛德和邓利就最早把这个IP用在语音识别上。

一开始做的时候都是把统计方法做框架，然后用神经网络做一些特征。然后看能不能改变方法，比如统计，后来就说干脆咱们做端对端吧，都是神经网络。这些东西就是人工智能的金字塔最底层就是一段记忆，存储这些计算，第二层就是刚才说过的感觉知这个听觉视觉，触觉像人一样。在往上就相当于人的大脑语言知识，对我们很不幸的或者很荣幸也是属于我们，不是贬低他们搞得还真不一样。这过去几年了，就是我们自然语言发展很快。我们这个数据集说一会介绍。

第二很多算法一会儿介绍，你有新算法你就赶紧去试，就提到了还有那就很多框架，这个运算原来是好几天，现在可能很少时间或者十分钟。但是这也就形成了一个壁垒，就是有的公司太牛，用的一套规定方法或者平台别的公司都没有，所以只能望洋兴叹，这块儿一会儿再讨论了。随着这个三大要素推动了很多典型的自然延伸，也取得突破进展，那个我这里列出了微软亚洲研究院所从事的一些项目，也是被世界认可。我们聊天机器人中医英日三国语言的聊天市场，还有印尼，，中文叫小冰，日文叫莉娜，还有其他国家比如，比如美国job，他这个聊天机器人，它有一个衡量标准就是人体聊天聊多少，我们叫CPS。、就是、一次聊天能聊多少轮，这个标准，就是一般两轮就知道你是机器人了，这个标准是业界目前23轮是最高的，能不能聊到100轮也是我们努力的方向。

第二个就是机器阅读，给篇文章然后说，这文章写啥东西，有这文章讲美国选举来说，这个体现人类的智能。机器翻译，一种语言转这种语言。 现在都转了神经，大家也在看自己的水平，这个我们水平就超过了普通的人发音。有很多句子都看不出来是机器翻译的。
三个数据及全部是业界最高，而且都达到了人类做这个数据集的专业人士做这个数据的水平，那刚才说比尔盖茨这么欣赏自然语言有一定的道理，所以我列出了自然语言处理的重要部件，局部不是全部，我们很幸运的是这些贡献几乎都来自于我们，一个输入法中、必应词典，我们英语学习词短语翻译语音的，所我们这个部门一直做微软多种语言，那种语言的互相反应，手与手与翻译。

然后我的同事，会、介绍问答系统翻译，我这边就介绍一些别的项目。
我觉得大家都喜欢写诗作曲唱歌，对吧。第二个就是写诗，再赋一个标题

## 58

 在这基础上进行修改，这个也是很有特色，这是我们最早做的对联，也是猜，这个也不容易做然后，只有提示什么，就是一个图片里，这个图片的感觉也是用关键词来表达的。
怎么生成的呢，就是把这些东西作为一个输入通过一个编码解码过程，然后就生成第一句话。再把这些也同时做俩编码解码，然后就是咱们一直可以做下去，让机器人给大家演示一小段。

下面就给大家介绍一下肯神经网络，技术体系，我的同事会介绍细节，这个技术体系其实就是词怎么表达，多维向量表达这个词的句子的表达，好多人叫词的编码编码，所以方法是2014年的方法用到CBOW，预测的概率就当损失函数没有传过来，把这些进行迭代比较，假如有大规模的培训之后稳定下来就得到了每一次的这个周围是预测。也可以用这中间的词预测周围，最后都得到了一个稳定的一个词语表示，也可以算控制同义词，近义词反义词等等，那词能做的话，下面能把句子也可以做了或者说了一句话，把词简单的叠加，所以他用一个循环神经网络，每个词叫隐蔽状态，它吸收前一段状态，做一个编码，然后做一个这个非线性变化。
然后还有一个卷神经网络。一个最重要的就是把编码解码做一个输入，把一个句子作为输入，然后得到一个新的数机器翻译。
比如说出一个具体的句子，然后把这个句子表示出来这个红点代表全局的影响，然后总是依赖最后一个词，他强势在编码的时候，它是不好的，因为有的词输出的时候，他对这个前面那个这个不同的荧光的，它对应的词，它的依赖程度不一样，所以你注意动态的这些在每一个编码和解码的引发了动态的计算，跟输入这个句子对应的，他的全局，他认为重要的话就给他更多一些，然后动态的生成，它不需要从ARM内容方式，这个词它可以计算其他的，同时计算，所以他是并行。第二个他有多个头结构。解码的时候，他不光是看到输入这种编码的，已经解码也是，所以他解码这个时候就比那个单纯很多，然后后面，就引入了这个预训练模型，就是一个句子，从左到右进行编码，或者从右到左边，当时他用的是STM，然后那是第一层的这个数据就是第一层的编码。
然后，在这基础上，你得到编码，然后再走一层编码同样然后在从左到右得到一层编码，他我们要搞这么多层呢，这就是深度学习的一个优势。
一起作业来判断每个词是不是达到，或者序列标注你说一个句子，你看他每个词是否标准。现在预训练模型就是成为潮流了，有两个来自微软研究院，铁岩帮别人那边做了一个，然后最近比较火的AImet，就是各种排列组合，后面就是请段楠详细介绍了其中的一个新的范式 。
然后，你用这个大的一些模型的去弥补这个小鱼苗，为什么说现在大的模型就可能把人类的知识语言知识领域知识有某种方式进行编码，但是我们也看不明白，很多编码，然后你在做小的任务时候，就是我们的人一样，他是让背后的这些知识背景的话是起作用，就等于这个自然的嵌入进去，所以现在叫预训练。
好，前面就是对自然语言就是神经网络的基本介绍，总结一下就是词怎么表达，句子怎么表达， 又怎么通过编码解码做一些任务，比如进行翻译任务，然后现在预训练模型，大概怎么样，为什么厉害形成一个新的一个自然语言理解，后面就谈谈自然语言，未来的发展，我的一些观点。

第一个就是说现在都是要用大规模的计算机，有个公司搞芯片。
那个GPU，、都这个计算量非常大，有的时候是有很有效，因为多数情况下你计算资源越高，效果会好，但是有的时候你m又不是那么好的话，令你失望，那还造成很多污染，这也是这篇文章介绍，比如说碳排放就有一个碳排放对应。计算资源确实是代价太大不是很实际发展模式。

第二个数据也是有问题，，现在有些词，它天然就特别相关，跟罪犯就特别相关。 也有些数据代价比较大，只能请多少人标是吧，，一个思维习惯驱动，谁都不想花太大的功夫把所有的数据标出来。好像水平挺高，实际情况数据库的一般。
这个是说到两个数据，一个是计算数据，一个计算资源现在都有一些问题，再说现在这个国情上在原点，也有一些问题。我这里就觉得三大类的问题，一个就是rich resources task就是有很多语言，大家来学东西。第二就是low resources task，没什么要学的。第三就是多轮，比如客户问答，对三个方面如果都做的很好，发现就是神经网络也都有一些问题，高兴的是什么，rich做的还真不错，虽然普通话咱们几个研究现在基本都一样。

所以，我们要是想做研究或者研发的话，首先把这些做好，第二，这个这两行看大家听不听懂。我们拿那个中英机器翻译的做实验，然后他做的统计的话，它有一些错误或措施的符合语法确实、翻译错了，其实这个翻译的特别好，但是有些东西翻译丢了。

## 59

因为他这个机器翻译，黑箱的，你也不知道哪个错，还有摄影师都做得不好 。不能翻成地球的地方或者土地方。这个很难。

目前做的不好，所以你说好的机器翻译厉害了，把人的那个一元代替了，所以你还是跟人的翻译有区别。一带一路原来你必须没见过没去过，还有像咱们中国人写东西都搞一个成语，但是成语不能体现出来，成语根据上下文的意思，重新表达。

所以呢神经网络机器翻译有很多缺点。 这里就列出了一些，上文理解某些自适应，如何对领域自适应，如何理解和翻译，甚至的文档或者片段，来做翻译。

那我们推广的，rich上就有很多问题，我就总结四个，如何在这里获得体现融入到神经网络的训练中。

第二典型问题就是low resource。就是没什么训练一样，但是你要做一个系统，一般来讲，就是三个spro，一个就是说rich是训练的不错，有有充足的任务，任务学到的东西做迁移到其他。第二个就是有一种语言是rich的，其他的不是的迁移到rich的语言，语言的迁移。第三，就是说给一些小词典小语量不大。你再做个小任务再去发展演绎这个道路，，所以这是一个迁移学习。
跨语言学习，其他语言没有，比如说，英文的分类器的不错，可能讲德语没有，但是你要是有英法双语的，用这个双语做一个跨语言的，预训练模型。
两个词库不能用的，如果他的语音相近，就是把这两种语言凑合在一起，两个双发一个句，然后你就match。你把世界所有语言串一起算一种语言的话，基本上就是做一个大圆，这个这么大，这样的结果就每个词都有一个。不同语言的词之间就可以算同意了。当然算的时候是有技巧，有的词太长切成字词，有很多语言里面，他字词不同的用法有很多的，英文有很强的分类。

最后就说利用种子来迭代学习，翻译为例，就是小狗的，小词表，或者有些延迟数字词12345。
通过这些种子，把不同的词做一些简化连接在一起久了，当然连接连接完之后，你把那个可信度高的一些词提取出了，加上目标里面的词，让他加到了一起，水平不高，没事儿，然后你就可以学一个神经网络，学完之后，进行迭代。
有的语言他们双语或者量很小，也可以得到一个返息，等会儿下午的时候有详细介绍，那你就看上面的low resource主要研究的课题，这里还有一个就是人的角色，你也可以在这里起到一定的方法，好， 最后一个就是MARKETN。 现在的这个做对话，缺乏常识的做的不好，我们举例子，比如说特拉普是美国第54届这是事实存在的，你问题是美国总统，机器一下就回答出来。美国总统肯定是最有权利，再问他是一时竞选俄罗斯，可以的。  说到这些都是人小孩子基本都能回答。
还有就是前后这个机器人回答不一致，它基本上是没法记忆的，他缺乏记忆机制然，空间时间逻辑等等。好，那你说我要推理，要办什么具体事儿，我拿个例子说这个，这个例子就是告诉他们适合每个人俩人两个电影。
机器推理他看哈电视说，然后这个推理的，用这个推理，得到这个。第一步推理，好，第二个就是我们知道这里表达一般就可以知道知识图谱，得到的答案，人可能要继续问什么时候出生的。推理干嘛呢，理解代表是谁，这是常识了，男的，然后把这些图谱又做一次推理，机器需要理解，好吧，这是什么意思，原来他写想的对吧，理解这是谁。

第一件事儿就是典型的例子，但是也不容易，所以你要要做好多事。第二个知识体系，这个体系，比如说客服回答的机器维修的问题，那个这些东西都围在一起转，我为什么回答，因为我看出是男的，我了解他们的演员，他演过什么电影，所以我认为这个答案解释，没有解释那谁敢相信你说的话。 是吧，你跟我一说有道理，我就信。这才是实现人类真正的理解计算机真正的件事儿。
理解完了之后，找哪些东西跟你有关了，把有关的拿出来，然后，你的状态就要变化，变化之后，你要真心写回去，你得到一个结果，还要更新这个，下一个把就在这个新的memory基础上进行一个对话。基本上这是一个就是一个逻辑的，一个是一个图，但做起来都都比较大。所以就也引发未来多任务的一些重要研究问题，比如说如何表达如何存储或者怎么来建模，怎么来推理，怎么来这些解释的。
 那么未来我们是希望做成可解释的，有知识有道德的，可以自我学习的，这个都说清楚了。这个系统不可能一蹴而就学了多少，然后天天就不什么都不用改，就天天用了，跟人一样，每天用起来之后，他就一天天的迭代，新的知识迭代，系统越来越好。
现在这个咱们现在这些还有很大距离，那怎么做呢，毛主席说，从实践中来到实践中去，真的，一个从实践中来实践中遇到了什么问题。对针对这个问题，不可能一天都解决，解决多少是多少。这是第二个问题，第三类遇到的问题，细节不说，认知问题，调研，理解推理，回答问题，概括能力，写报告，对话聊天等，这都是我们人的认知能力。好那，那你再说我们做人工智能的话，我们说了信号，不可能纯粹的自然语言，所以要跟相关的一些研究之后，链接在一起。
好，最后一张PPT， 就知道未来目标了，我们怎么去做，这个事情还真不是说一个人一个单位，一个公司就能解决的事，需要全社会不同的人，不同的角色都一起去做贡献。

我概括六大方面，一个计算资源还是有关的人或者单位要继续研究芯片，还有云计算，资源管理，如果知道我们做研究的话，还有一个模型压缩加速模型。要么转不起来，要么就是存不下。第二个数据这块儿，虽然我们说有很多数据，就是说大家都做出贡献，其实就是说自己做数据贡献给产品，大家如果都做的话，就很多任务细分，很清楚也能推动。还有什么别的，这个数据纠偏问题，一直保护下去，现在比如联邦学习，保护隐私，无监督，有监督的学习。人才培养人的话，就是人工智能时代日新月异，你说照本宣科，不行。

最后就是强调这个应用，  人工智能不能做纸上谈兵，学到东西快速解决一个问题，解决这件事儿，你反过来去想哪一种应该刚做，然后，你针对这个问题用什么方法，那个集成的词，还有对市场对这种要充分了解商品模型，所以我们对人工智能的人才要求不是说写两篇文章，实际上它应该有一个广阔的使用。
好，最后 总结一下，过去十年，nlp经历的规则统计，现在神经网络现在发展很好，取得很好的进步，但是有很多不足，计算资源数据、资源太过依赖，然后建模，可解释有很多不足，然后未来怎么发展。理了一下目前存在问题，大概的技术路线，然后我强调的是六大方面、计算数据、技术人才合作，这个讲座就是作为一个开场。
我们休息十分钟，下面就再回来的请我同事来讲。

## 61

大家上午好，我叫段楠，主要是要介绍一下那个，周老师刚刚已经就是介绍一下RP件很多进展，然后我在这里，主要先回顾一些和QA相关的一些深度学习一些技术。

这里面和周老师刚刚讲的很多东西也是有一个很好的照应，然后我这张图就是列了一下，LP，这里最近发生了一些事情，很多东西周老师也是介绍了。但是我不知道我这边讲的QA相关的这些相关的核心技术，就把这个东西稍微整理一下。

那实际上现在这个深度学习，深度学习这个东西兴起之后，很多东西都变成熟，和表示学习相关，比如说对自然语言的任务，比如说如何去表示一个词的个含义非常重要，还有就是说词最基本的单位，但是在里面还有很多，像句子段落文章，所以如何表示这个science也是在来宾非常重要的一个TOPIC。

以及最近就说如果大家比较关注这种智能相关的发展的话，可能最近就是比较火的项目，周老师的介绍，比如说像这种容易训练的东西，反正按照我的理解，就是这种东西，它就是一种DATA表示，学到一个模型，然后这种模型就相当于说对人类知识的一种表示，然后在这个模型上层。有很多下游的nlp任务切细条，所以说有这种准确的model后，nlP有不同的方法，比如说，不同的任务，其实不同的方法都对应很多不同的方法。

比如搞机器翻译的QA的等，他们没有很有自己的特色，但是有的时候出现准确的model之后，好像大家把精力更多的放在如何去表示上去了，当然这个也不一定是说可能是未来的趋势就是这个样子。不过从目前来看，他确实能够达到很好的效果，我在这里主要介绍几个就是说包括周老师也提到了。

然后我下面就分别介绍这些可能会对大家理解，有一些有很大的帮助，第一个就是word2vec，我这个周老师刚刚已经介绍了，就是这个就稍微再简单回顾一下，就是说它的输入，就是目的是什么，计算出一个然后扣掉中间这个词，然后我希望我们model能够根据这个词左边和右边的这些词预测中间那个词，然后，他的做法就是说input稍微具体点，就是这个因素，是等价于整个这个就是词汇表长度，比如说我只关注，因为你们3字，那个长度3万，然后，其中有一个就是那个词出现在所有表的位置，对这个词来说就是为1其他位置为零，这样的话，每一个词有了这么一个01一个表示，然后这个matchW他就是一个矩阵，其实那个东西就是我们想要学的那个，就是每一个词，通过这个矩阵做乘法就相当于说从这个群里面抽出来，他自己对应的，然后左边的加起来，加起来然后再除以n就得到了一个LV的就是这种平均的这种上下文表示，然后有了这个上下文表示，output输出的那个矩阵再创一个WPS，然后就可以一个把输出的这么一个向量表示。

上下文的这种信息的传递，其实内容是可以在很大程度上说可能就是有那个缺陷，另一个是说就是说很多词，它有一些歧义，比如说这个河岸和银行都叫bank，你要取钱，你可能会去取钱一般来说可能不会去河岸，你去河岸也不太合适，所以说为了要区分这种同一个词在不同发现语境中的这种不同的意思，就是说他也是早期的这种工作，他没有办法去做策划，所以后续很多工作包括M包括word他们都是在这个上下文上，来对这个词表示做了很多的改进。

第二波想说的这个这个概念就是这种人他人来过，就说我之所以要提的东西，因为一个是说他和我们后续要提的，就是说QA的一些任务有直接相关。算是一个上一代的这么一个句子进行建模的这么一种方法，那他的这个大概思路就是说这么一个公式。就说他是按照因为原有一个自左向右的一个天然的顺序，所以说他就对这个东西建模，那比如我们一句话，就是说截止到当前这个第T个词代表的整个的意思，它怎么计算，它是有信息传导的这个体检意识，在综合当前的这个现场SHE，然后来共同决定就是截止到这个时候，这个三个字的mini具体是怎么表示，所以他的这种规范的过程就是这种从左到右。

最简单最简单的一个方法，就是说整个信息通过用户一遍，让我最后一个词就是黑的C就可以去做这个词的向量表示。 当然这个东西就是说在后续有很多变化，比如说像RSDM和GRU引入一些门的概念，但实际上就是从这个一个最最基本的就是这个，自然语言的这种表示，就是命令表示的这么一个角度，就是我觉得大家理解一个这样一个结构对后续的理解应该就可以，有了RNN能干什么，就是这个里面举个例子，因为RNN他就是从左到右去刻画一个训练，就像预测，比如说我热爱我的祖国下一代，一般来说就是可能是祖国，因为大家脑子里都有这样一个大model，当然也可能是祖父祖母当然，整个数据里面，祖国这个词在前面那个前面下面它出现那个次数要比父母这种可能要稍微多一点，所以，model实际上就是记录了一个类似于我下面一个词的分布的这个东西。

在神经网络之前，就说要统计的东西，就是那个时候就数数，就是说如果要去计算static那个词是什么，实际上他就是把出现国的这个次数去处以。他就出现了一个就是就是这种非常数数的这么一个问题，而神经网络就说他数数要稍微比较高级一点，虽然他也是一种输出，但是他数的时候数就比较复杂一点，就是说你这边就是零一操作，当然这个神经网络来说，他的数数靠一个的所谓的model去做手术，后面可以稍微介绍一下，所以和统计，神经，他实际上是在数数的。

还是有一点例子，我觉得可能比较容易理解，假如说现在我整个世界四个词，，其实这个输入，就是你这种因为它很特殊的其实，然后之前也没有什么信息，所以他先通过这个我们刚刚讲的那个过程的计算出的第一个词对应的这个黑的，就是这个颜色的数字不重要，就是大家看看这个运行行的意思。

有了这个之后就稍微把这个转换成一个所词汇表，同样大小的同一个上去就是蓝色的，然后他这个上面再做一个sprice，其实做意义也不是特别大，就是说在就是说想在这个里面，把它就是一个根，然后从这个里面选一个，最大的一个词，比如说这在这个里面最后，他觉得我应该是如此了，第四个字的，但是大概四，所以他就从那个词汇表第四个位置第一次输入，然后就这么走了，你这一步走了之后，其实剩下的中间一部分之前的部分一模一样，这这个词，然后一传过来就完全要走了，这个就是前一次的这个词，然后再去output，然后最后算一个概率。

而且是我觉得那个第一次的感觉，所以说整个过程就是这么一个过程，有了这个RNN之后，就说这块儿就是数学老师他在下午做更详细的一些介绍，但是我在这里我介绍这个东西是因为引发他对QA来说也是一个比较重要的东西，比如说当老师提到语音分析，我们要根据结构化的知识，这个自然的一个输入转换成一个结构化的表示，这实际上就是一个就是一个编码解码的过程，实际上这个东西非常容易理解了。

就是说第一步，给我们输入这个就是那个做一个编码，当然这是一个最简单的形式。就说我只把这个词儿当成是整个句子表示，然后把它作为另一段类似于刚刚那种初始化它，然后再去做输出，所以整个的过程实际上是一个非常直接的过程，比如说在原来这个地方是没有，但是现在对input讲，它的输入是输入的整个的一个项目表示，非常相似的，但是其实这和人的区别也不太大，那就是说他在做的时候要考虑一些input的一些信息，当然这个这些简单的东西在不在，输入的时候考虑一下这个以后的信息。

但这个时候就不对了， 就比如说拿这个机器翻译为例，然后我就拿着举个例子，下午还要数学老师，所以说，人在做翻译的时候就说翻译一个词，实际上并不是比如说我把中文翻译成英文，如果翻译英文，比如说这个词的时候，并不是所有的词都对我翻译这个词起到非常重要的一个作用，而只是其中的某一些词，就是说我怎么样能够找到那个对我当前的输出最重要的一个词，这就是要干的事儿。这个就是同样一个方向可能对不同的任务，不同的意图，可能是他注意的地方，实际上是不一样。对这个机器翻译来讲，就是说他的这个相当于什么，假设说我要翻译，第三个词什么，我要从这个SE的XN的词里面找到最近的词对我这第三个词非常重要，所以我就用SE不是截止到这个时刻的接近SR去和每一个SR身都所以你看这个SR，他们做了一个相似度，然后这个SR和HR的相似度，然后以此类推，HR和HF的相似度算完之后，就相当于说，这个就是做这个测试计算的过程。有了这个之后，他会对这个输入同样输入不变，只不过是说不同的上下文要对输入注意的权重，然后再把他们做一个简单的相加，其实就得到了一个就是说所谓的对当前的输入最重要的，就是这种输出最重要的那种信息计算的这么一个过程。

所以这个就是一个比较典型的例子，就是悲观的人看到的永远是空的那半杯水，乐观的人看到的是这个有水那段，整个这张图是不变的，只不过是大家的脑子不太一样，所以他对这个产品的这个计算方式，所以最终导致提出来的信息不一样。然后在这个很多任务要参选，实际上能够帮助我们去，就是怎么说，就是可视化一些就是我们学到什么东西，比如说在KTV那种就是这个狗在哪趴着，然后给你们通过这个计算，这个问题实际上注意的是，这些部位什么样的对应的是为什么散落一些小道的，这个人上次是唱什么。就是大家对于这个机器翻译的东西来说，对于关系，当然在实际的分析里面非常注意选择非常好的，在时机的时候拿出来看的就是基本上也大多数情况下，我们也不知道他就是在追求，然后，有了这个之后，这个就是一些基本功公司，我们现在想往那个方向去走，所以说，有了这个，我们在介绍一些传统方法和相关的第一个是种scaled，起始时间配置一个QA相称算矩阵，然后求和来进行这个过程，只不过是他有一个维度大小开放的东西，其实这个东西它就是起到一个就是让这个神经网络训练更加，就是说平稳的这么一个作用，它是一种优化的机制本身。

## 62

并没有什么特殊的一些，然后这个例子就是给了一个，在这个就是对上下文相关的，这种表示的这个计算的这么一个例子，比如说给了一个今天的天气还不错，但是显示说今天我们想要计算这个词对应这个在这个上面的信息，那在这个传统方法，后续我们介绍这个大概是怎么计算的，首先，它会把这个的和QKV，是一样的，就是在这个里面P还有每个词的T等于它的这个词向量的表示。

在这个问题上是这个表示这个P去算一个，然后就是在这个地方就是这个的，然后自己算一下的意思，说完之后就是把一个东西搞到一个概率分布下，就得到了一个w1和W，然后就能去QK，做完提取之后那个得到这个位置就成了这个上提取信息，就得到了这个上下文相关的向量，其实传统方法就是这么个东西，都算一遍之后，然后基于这些列子再算，当然里面有一些什么就是传差，还有一些内容，就是一些神经网络的技巧，思路就是把这件事情做了很多层，然后最后拿到这五个词对应的表示来作为就是说在这样的状态下一个表示，然后就说在后面的，还有一个概念，就是说你在计算这种信息提取的时候，实际上你能从很多不同的层面去提取信息，就是说多次提取N次这样信息，但是每一次能够在所谓的不同的层。

所以他在做这个刚刚讲的这些参数之前，就是说他们把它分别通过一些矩阵相乘，给你们投影了我们也不知道是什么次元的一些空间上去，然后在那些空间上爱干什么后面干的都是一样的，然后把这些信息都归拢在一起，然后这个空间是什么空间，他们想做在什么特定的景物这个东西都是模型，自己去学，但是总结一句说他就是在不同的空间上去所谓的不同空间不同角度去把这种都算一下，有了这个是怎么治疗的这个中文怎么变形金刚。

Transformer这个就是这个东西大家也不太用关注，因为那个是他本身的工作提出的时候她做机器翻译，就是这一个东西，大家只要看左边这个，就是那几个词，然后加入一些，就是一二三四五六十p，大家注意到每一个字都和其他的服务交互，已经搞不清楚哪个前后，所以要在这个里面把位置信息的这个东西new一下，把这个信息反馈信息了加进去，然后我们讲的东西就是从多个层次来算了，然后，就是当然，这种一些技巧就是他这边现在主要是让他信息都会经过这个项目主要是让那个模型的训练取得更佳的稳定。

有了这样的东西，其实这个东西那实际上就可以用它来替换这个RNN来对这个自然语言的序列进行一个建模，然后这个图就是说，RNN和这个究竟有什么不一样，我找了两个，左边的图是我自己画的，其实我觉得我画不出来，就是这样的方式第一个和第二，就是他就是一个这样的就是说每一个词都和其他的一个词，有一个这样的一个连接。然后他第一层用一个词就是刚刚我们讲了一个这样的，每个点算一次，然后这个输出都第一次输出都可以作为第二次输了再算，得到一个八角形，其实就是说和RNN比，实际上还是直接对自然语言里面就是说任意两个词作了一个建模，所以说它实际上是泛的一种模型的图。所以说他因为他就是这个考虑这种参量很多，基本上现在就是替代RNN成为了一个标配。

然后就说简单的提两句，pre-training这个东西就是说他实际上是干什么的，反正我理解我们借鉴了这个人从小到大的一个过程，去理解这种小孩出生了，摔了之后疼做了什么，学到一些常识，上学的时候就开始学什么语文，数学，地理历史哪里，当然，你当年学的这个很多的地理知识，现在都忘了，但是那个时候是必不可少的，因为整个这些世界知识，事实上对后来我们学习非常专业技能，实际上是很重要的一些作用，虽然作用并不能非常明显的感觉到，然后上去之后，就是每个人都走上了不同的岗位，这个时候相当于机器学习的翻译，就说我所把这些知识都学了个遍。然后之后我想精装到某个具体任务，我觉得要把这些知识，在那个任务上了做一些融会贯通，这个就是对应到我们这个神经网络训练里面就相当于说传统的任务，就是让一个小孩就根据这个具体model这个这事儿也不能承保的，但是可能要费点力气，但是现在这个项目上线上行，从一些政府无关的东西上去学一些乱七八糟的知识，然后用这些知识，在一些具体的事微调，他实际上是模拟的这么一个过程。

然后，刚刚就是说我们讲这个，有了这个方法之后，怎么做一些训练的主要就是我们在最开始的时候还讲那个预测的词，所以还是非常的就是数据几乎是无限的，只要人类积累下来的所有的文本信息我都可以做到的，不管你是语文，数学，历史，政治地理，什么都可以，所以说他是一个目前的，大家来说说用的最常见的一种任务，不让模型去记住这种常识已经关于世界的这种model，当然通过这种就是这种隐含的形式来记录他是怎么做的，所以它对这个机器这个思路，就是说对于很大很大的文本里面拿出一些句子，最后一个词是根据他前面这个词，然后就说出model之后这个东西就相当于说他就把世界上所有的东西都看一下，然后这样的话，他就是一个model，然后他用的时候也非常简单，因为就是说我们实际上很多任务不一样，有的说给一个具体的分类，这个讲的是究竟有没有什么有危害社会的因素，比如说有的时候就看这两个句子是不是表达同一个意思，有的时候想做QA，那他这样就非常聪明，它会把所有的任务，input全都连成一串就变成一个Cpons，就最后一个词儿就表示了他作为是我对当前的这种任务输入的一个理解，然后他在每个任务上面加一个model就是那个橙黄色的东西，他就去专门的训练不同的这种线性模型，就能达到一个很好的效果。

BEAT刚刚周老师已经介绍了，他的JPGPPT是自主行动的过程，现在就是扣掉一词，然后用左边的和右边上下文去测，就是这也算是一种model，刚才我介绍了，所以我在这里就不做过多的介绍。也就是说整个这个叫做判别式的这种微调，实际上就是现在就是在APP里面一个最常见的一个最基本的做法，刚刚有提到现在很多任务变成什么，先搞了个准确的modsel，然后在进行中的程序微调，任务相关的一些参数，就能达到很好的效果，而且就跳过去。实际上他们的思路都是说根据上下文用这个词，只不过就是做的这个东西搞得不复杂，在那个年代，你想把握词法就是随着时间的变化模型变得复杂，但是他实际上内在的思路是都是差不多，所以我觉得其实这种麦克斯非常好。

然后，他说这个GPT和WORD区别，就是说GPT是从左到右取出来的，word并不太适合做一个生成任务，因为很少有机会看到你未来想要生词，所以他一般来做一些内容。而GPT因为他是本身就是自主性训练，所以他有很多自然语言生成任务，有一个非常好的一个作用，说有用的去做一些跨越语言的一些事情，就是说，比较典型的有这么一个有这么工作的事就是他直接就把世界上所有的圆都赢了，把所有一百零四种语言就全都丢到整个一个去做训练训练，方式还是上下文去预测，就是它有一个不一样的地方是说这些所有语言的，都是放在同一个词表里面，就是第二个就是说他实际上并没有真正的说，看到太多的语言和语言之间不同信息对应关系，但是他们就这些信息，他们是同一个这个网络结构。也就是说，你中文英文都发下同一个网络结构，他这个东西，他在里面就就是不可解释的就学到了一些可能做语言相关的一些东西，所以说，通过这种混在一起，或者这种方式，它就能够达到一定程度的这种跨语言的这种能力，就是他尽量他就有很大的可能，能够把苹果APPLE这两个词的意思表示成就是这种空间上非常接近这么一种表示。

然后就是也不用太关注名字，比如说这个东西实际上和SEO多少区别，它只是加了一个何种语言相关的一些信息，就是说他除了之前做那些事之外都加的很随时就是把这两个句子的中文英文这种聚会拼一起，但是在这个过程之中，我预测这个词不仅看到了我的英语上下文还能看到他最后翻译，这就让模型在一定程度上学习到这个词我很好的照应，所以说往下走，就是把更多的真正的带有跨语言的这些信息的数据给他塞到这个类似于BEAT这种东西里面去学习一种关系，能让这个模型能够对跨语言的这种能力有一个更好的一个学习。后续，也是一些相关的工作，反正他就是从更多的角度，加入到不同的就是跨语言的任务，那这些任务能干什么事儿，比如说这个SNAIS什么任务就是很简单就两句话，他判断这两句话之间是不是存在一个以前我们就说这两句话，是不是矛盾会给你第一句话就推导出第二的话，会不会又说这种话不知道是什么关系，找到这个任务是让他就是相当于是一个任务，但是他这个实验的条件就是比较特殊的，也就是说他只在英文上，在其他14个语言上完全没有，在训练得法的时候他根本就没有见过这种问题重要的数据，弄传统的这种方法说，你没法干了，因为没有输入，没有需求，怎么办，我啥也干不了了，那有了这个COSEGOOSEGO这种，准确的model之后，我就可以在英文上的训练，然后在其他14种语言上来做测试。为什么会这样做，因为已经天生的能够取得不同语言之间的这种同一个语音对应关系。对这个语言本身不这么敏感，当然，虽然还是有一点，但是他已经能够在一定程度上作出各种信息，所以说在这任务上也取得了一些很好的效果，然后这是一些实验结果，这就这就不太做一些介绍所。之前是介绍一些和这个QA相关的一些基本技术，不知道大家要不要休息，如果不想休息的话我们继续往下讲。

## 63

这个过程还需要人去看去找一些非常精准的一些答案，能不能把这个过程也做得更被大家对审核一些时间，这个在搜索上的一个体系起的体现，比如说对于一些问题我除了回复你最相关的一些网页之外，我还可以，就是，把一些相对比较精准的答案放在一个显眼的位置，让你看见东西你就知道你要找的是什么，这里指的是一个基于知识图谱的一个问题，就是说之所以基于图谱，是因为这个答案并不是抽出来的，他是从六个那种类似网络结构图里面去通过，就是理解分析推理得到。

另一个就说，就是可以基于网页上的表格，因为实际上在很多领域里面这种表格实际上是非常能够快速的去对信息做一个比较摘要式的简洁明亮的一个整理的一种数据存储结构，比如说一些金融的信息，体育比赛的报告，就这些东西他们往往都是以表格形式存在，所以说，如果我们能够对表格更好的理解，我说的搜索之外，可能对很多企业落地的这种场景也会有非常棒的一个帮助的作用，比如说我想问世界最高的山是哪个，我就直接从网页里面找出来一个表，然后告诉你最高的山就是这个，还有一种典型的问答就是叫基于文本，就是真的就是很多问题，实际上可能现有的就是也不能做一个很好的一个覆盖，还是需要从更加海量的这种网页里去找对的信息，比如说一个说从比如说这个问题什么时候能够做，什么时候被允许在美国是有投票权，他可能根据网页上，把互联网找到这么一篇文章里面偷偷的名画来告诉你在1920年8月18号。

这件事儿，第一个做的更加精准，老师有提到是有机器人已经可以在这个段落的基础之上，更加显著的把握，希望得到的这种更加精准的信息，通过抽取和高亮的方式呈现在这里，所以它也是一个非常重要的一个形式，最后有形式，就是这种基于FEQ就是常见问题，这种就是说就是我们大家，经常遇到，比如说一些套餐问题已经让客服打电话，这个客服他一天接了电话的问题基本上就是几十个，用户也能说能不能就说把这种积累下载来，用到，就是来临时说用户更快地获取到这个答案了，就说也不让这些客服人员闹心，让他们能够把心思花在更难于回答的问题上，可以用一些废旧的机制，他就说，比如说我给一个问题，历史上有很多这样的问题和对应的答案，那我只需要判断，是不是我当前的问题和历史上的某一个问题在于非常相似，我就可以用这个箱子问题历史答案来做一个回复。所以就说，我今天，最简单介绍一下两种不同类型的问题，一个就是基于知识图谱的，他积累知识的问题，可以就是图谱也可以听一些表格，另一个他也是文本问答可以基于这个文档。

最后就是简单的总结，先说这个知识图谱，这个概念也就是说把世界上的这些能总结的知识，总结了一遍，做一些抽象，然后把它们连在一起的一个图，比如说在知识图谱里面有很多的实体是安静型的，它就是对世界上这些客观存在的一些失误和自己名字，比如奥巴马大概只对应一个实体对应的一个实体，实际上可能有100多人。

所以说，这里面的，就是展示方面的文字实际上在知识图谱里面每个实体都有一个特殊的一个ID。同一个名字，可能对不同的ID来来达到的说区分不同的实体，实体的作用，第二个在制图里面一个很重要的这个概念的关系或者谓词，就是在描述实体和实体之间的关系的。他是通过这样的一种为此来决定，他会用统一的一个标签的这种关系来做一个。一个描述这样的好处是说在同样一个关系的知识图谱里面就统一了。但是它的坏处是说当人去查这个支付的时候出生这个比如说这个出生日期这东西有很多种说法，你哪年的，你生日那天，你是什么时候出生的，就有很多人问这样话就存在一个问题，就说我怎么把自然语言的这种变化莫测，这种说法和知识图谱里面这种匹配起来。实际上这个就是说的是语音对人类的理解，还是说查询是核心任务，就是主要会说这个事该怎么做，除了这个。

 比如说，这就是一个事实，一个三元组就是一个是事实。这么一个地方就是这是一个事实，包括两个实体一个关系。当然，对于很多事实，他靠一个三元组描述不全的，比如说一个人的婚姻的一个状态，比如奥巴马是他未来的老婆是谁，他们是哪一天结婚，有这么一个对多个东西做一个综合的，作为一个事实，他就是引入了这么一种虚拟事件结点，并没有名字，但是他表示，就是整个串联起来的这些属性和实体之间所组成的一个事件。

所以整个知识图谱可以非常大，就是可能，就是就是项目大众点评里面很多东西，其实京东，淘宝这些都是知识图谱存在的，现在多少钱什么牌子谁做没钱上货了，然后有个特点都是在说这个东西实际上很多重要意义，这里是一些典型的知识图谱，有了这个知识图谱，实际上这个就是作问答，这个思路就是在这上面挺清楚，反正就是说我给你一个问题，我就继续理解一下这个问题，然后分析推理一下，就是能够得到答案，这个就是他的这个过程。

当然 对整个问答一想到周老师也提到了就说单轮的的都很难做，更别说多轮了。所以说今天就说我还是稍微单轮介绍为主，就是这种知识问答，就说我就给他分是两种两大类，一种就是基于咱们这个分析的，他就说那把这个问题要转换成一个机器可以了解的，基于知识图谱的机器可以理解的，一个结构化的产品，然后用这个查询，所以来讲，就说怎么一旦这个要理解的就是机器会有这种理解方式。就是对应的是一种机器能执行的一种语言，一个去知识图谱里面去找这个答案，就是这样直接的就是那个过程是一个确定的过程输入到里面，所以整个这个问题，就是说你怎么把自然语言转换到我们想要的结构化问题上，这也就是非常重要的一个任务，他想干的事情。为了介绍这个产品，就是还要做一些铺垫，，里面有参数的参数，我也不知道是什么。当然后面我介绍一种方法。

就是说有很多种形式，可以是函数，也可以是很多函数做一些操作，得到了一个新的函数，就说他是、就是这种东西，一个递归的一个过程，所以这个方式它是存在这样的形式，就说他是一个是一个表达式包含一些变量的事情方式，实际上是两个表达式之间都可以有些项目操作。就是如果单纯看这三行，、确实也难以理解，所以后面有几个简单的例子。第一个就是实体，第二个就是变量，第三个就是函数。举例子，我想问，奥巴马是在么出生的，怎么解读这个东西，现在我一个函数，他只有在第一个参数和第二个参数满足这个人和出生地的关系的情况下，才返回true其他情况的话都返回false。所以说一个最笨的方法，我数据库里面几十亿上百亿摆平全都是不是有个实体和满足这个关系，找到了这个东西返回了，这个就是我想要的答案就是，我想说这个答案。

另一个是说这些函数定义的，这个函数定义很多很复杂，还有一种就是相对来说有点复杂，就是说因为这个语言实际上是有一定的这组合递归性就说很多复杂的语音是通过组合一些简单的语言来得到。
也不仅只是说用一个实例化的东西去替换掉一个，就是一些变量，来生成一个就是这个变量，少了这么一个函数太复杂的多，就是说大家有这些概念大概知道这个过程大概是一个什么样的过程。有了这样的一个东西，就是指现在大家要做这个所谓的分析就是他就是反正就是说奥巴马出生，我们就是要把这个语言转换成他，因为这个东西第一次如果说好几次看你觉得这东西不像人话。有两种方式一种，就是在深度学习出来之前，大家都在研究各种不同的所谓的文法，其实什么是文法，都是一些规则，您不管手写好自动化要好，都一搞出来，就是我们想要的。但是内容就深入学习方法，就是直接性的做一些工作，先说一下这个基于文法的分析，这只是其中一种思路，就是他还是从语言的这种一去就有一种可组合性的角度来去定义这个方法过程，说给我们一个这样的句子，我们有很多种方式，把这些句子的片段转换成对应的一些frame，放然后这些通过组成成另一个更大的范围责任片段，然后不停的去做这个过程，这个就是我们想要的整句话。过程实际上是有很多种不同的方式的，所以说再也不用了很多不同的自底向上分析的树，所以说，这就是一个选择的过程，第一是说一些规则，把所有能够自底向上走到头的做一些数据，可以导出来，然后第二个采一些就是那些特征来刻画每一棵树的质量，这个排序选出一个东西。
举个例子，就是说这个文法其实有很多种，这个PASS过程实际上在语言学家说到他那儿就麻烦，就是不同的方式不同的规则，他就定义了不同的名字，举例这个例子大家都比较容易去理解是一种，其实这个东西的话就是就是一些规则，这个规则就是把一些自然语言的片段和它对应的放一起。

## 65

就是自然语言和机器语言相互看一看，来学习到这种对应关系，训练，然后，我们在自己提出一个语音分析的数据集上，做了一个验证，这个东西实际上也挺管用的。他能够在走的是这种自然语言的形式，所以说他还是能够在这个场景下沟通，让方式能够很好的提升分析。TestQA虽然还有几页，当然这个东西非常好讲的就是因为训练图像。在预训练出来之前就是这么干的，问题他们两个之间有一个交互就是一个神经网络把它作为输入，输出一个，然后做一个分类来分。QA是不是满足这个问题的答案的关系，然后，有用word来做一个向量的表示，就是说两个之间成一个我们要学的时候交互的证，然后做一个二分类。

CNN就是卷积，然后把这两个东西算距离，把问题转表示，通过CSR通过这个算距离。就是支持积木，还有很多东西叠起来，也许有这个可能是四五年前的工作，现在是变的简单了，一个就是一个STYLE，举个例子，比如说我想从文章里面找一个段落，第一个词也拿出来做一个分类就对这种做一个排序，然后我们就在这个向量表示上预测这个任务队列，如果在现在的话，大家如果是做英文的任务直接拿来用，就是在自己的任务上直接跟着他翻译去用，这个就很好，所以最后大概有一个总结。就当我的一些任务，没有太多的目标数据，我在原来可能干不了，但是我现在model的这个时代，我就可以通过这种从海量数据里学到的那种泛知识再稍微调一下就可以。非常好的效果，新的环境下就说可能达到目的变得稍微的更加容易，一些QA的角度说，老师提到了很多，比如说常识，所以多轮甚至多动态，有的时候我可能就是想找你说能不能够找到张无忌大战六大门派的那个片段，那你要是人的话，你最多他给你找到这张图像，但实际上后面视觉化内容交互就可以去做这种事情，你想看哪块我就可以抽哪块。或者说是还有多语言，因为英文中文都说不标准，但是很多人没有，最后感谢大家，今天时间有限，那我也就准备那么多。然后就带我们写了两本书记得这些完全好，谢谢。

## 66

今天下午讲那个翻译，其实我是从零五年开始，做机器翻译十多年了，然后今天主要讲一些机器翻译的一些基本知识，然后在中间讲的就是loss source一些进展，然后最后会讲一些机器翻译的其他的一些任务上的一些情况，其实系翻译，这就是微软机器翻译服务在Azure上。然后，他也有一些，手机上有然后各种APP，然后各种可能也有一些相对的，然后他去弄60个语言之间的工艺，就是你说，然后它直接给你翻译过来。

也提供技术支持的，比如你有数据可以用微软然后培训一下，然后你去用。微软做机器翻译是从一点开始，不过我已经基本上一些NLP和一些机器语言处理一起，然后07年微软上架机器翻译系统，一二年的时候就是多了一个，多了一个demo就是在21世纪计算的那个大会上认可的，当时那个研究院的比较先进的就是一个就是实时的做语言翻译一个demo。

然后一五年，然后打电话说中文，对方说英文，这一六年的时候，我们其实现在一年之内都是都是一个统计的简单应用。微软的机器的话，有很多的产品，比如说那个OKOK操作页面，国内外一些项目，一些合作，这大概就是，翻译器那个框架，下面一个文件，你说一些话，源文件的过来做SA是识别中文名字，然后再聚合翻译成中文之后，这样做成转语音的信号，然后再输入这个，这个模型的部署在平台上，然后就可以上面支持各种应用，translator那一本差价，比如软件服务的那个应用。

然后一七年，就是离线系统上线，然后一八年参加了国际上比较有名的一个类似的比赛，就是这种美国的一个国防部组织的一个比赛，然后拿了第一。然后一九年，然后开发了那个，一六年做了广东话的翻译，然后一八年，我们这个神经网络的这个技术，用在新闻领域上的翻译，就是甚至超过人。因为我当时看了个翻译，我就说我自己感觉我们，肯定是翻译不出来。结果真的发现翻译的非常好。然后这个翻译结果，有人去评测，主要就是让一些懂中文和英文两个的，他们一些做那个做翻译的专业人士，然后给他看这个翻译的质量好不好。打一个0～100的分，然后就是写一个翻结果，让另外一个人去打分，然后实际上就是使用系统的分那个比那个人的要高一。就是机器翻译的一些背景，就是这个世界上人就是一开始的时候是说同一种语言，然后他们项目合作也比较愉快，然后他们的一个盖了一个塔盖到天上去。越来越高，然后，上帝看见感觉真的很担心很生气，然后他就把这个人说的话不一样，然后放在世界的各个地方，让他们盖不了塔。这个世界上语言原本有7000多种，然后说普通话的人越多，中国人说的不一样，然后人就一直有一个梦想就是能有一个翻译器，比如说，比如中国人的话根那个说英语的说话。这个技术，在一些科幻片儿也体现了。
那什么是机器翻译呢，他认为是一种，就是编码解码的思想，其实现在包括机器翻译，包括原来SMT都是沿着这个思想来的一个编码解码器，其实我们想机器翻译的话，可能一个人想非常简单过，然后有个词典，然后来个翻译过去，这很明显有问题，比如说看到一个翻译 ，他是用机器或者用程序来把用语言翻译成另一种语言。这是机器翻译研究的一个路线，有人用机器语言构造了一个系统，然后，当时下起来掀起机器翻译的一个热潮。

在1984年就有人提出了基于样版一个翻译系统，这个大概是一种思想，然后在1989年提供具体的方法，据统计的话，人去或者专家写一些规则之类的，然后在01年一年就有工具，然后大家可以用这个工具实际上是对SM的发展起到了很大作用，然后在零四年提出把这个局势的发展阶段的，然后写一个系统，推动了那个SMT发展。一四年是神经网络比较火，然后一四年情况，应用神经网络的地方，这个开始就是用来解决机器翻译，然后后来用到了其他的应用任务上，比如说QA。

这个基于样本及统计地方就是一个非常大的变化，他把机器翻译也就是如何把一个语言这个问题转换成就是我估计某种语言句子的翻译的概念。把问题，那么它主要包括两部分，一部分是存储空间，就是说有可能会把过程的空间，然后还有一个就是基于那个模型来讲，意味着构成一个基于其他的就是必须要大量的送出去学习，，各大队然整个的SMT框架。这个就是线性模型，有一个参数。有一个递归的过程，然后基于这个结果进行规则出去，就是不一般，这个对其的约束出来，然后构成了这个翻译模型。大量的数据或者语言这个模型，就是基于这个去学，然后用这三个match，然后有一个开发集，也是去学这个特征的系统。

就这个解码算法简单介绍一下，方法有两种，一种BTG。
就是，比如说俄罗斯飞行员，然后我们刚才那个那个翻译片段内容，然后继续往上拼，这个意思这两个翻译套过来是，是这一块的前面，然后刚才说的那个对象模型打分，等级高的前20个或者30个带在网上进行那个组合。还有一种就是SMT，那个刚才说的模型的区别有着它允许这种翻译规则的存在。他是一个允许变量在中间这样的规则就更灵活一些，同一个同底往上的一个翻译过程，这大概就是一四年之前，机器翻译的最主要的技术，当时的技术的话也可以 ，就是人基本可以用，从一四年开始，神经翻译，本质上还是统计模型，就是说他也是试图去对那个建模，然后，但是他在SMT的区别是你的每个片段的那个翻译都是有你的方向，所有可能语言的拼起来。但是MTV可能上午讲的，每个词语进行预测的，库那个串儿那个空间就是他的空间，还有就是他不需要人问题，我会详细介绍。然后说一下就是机器翻译评测，我记得一个翻译家提出来，就是说你如何评价很多，三个伙伴有三个标准，当年严复哪样吸引他，然后就是对应到，就是说你发的那个结果，你要跟原来的那个绝对是意思一致的，你翻译出来，你是能看懂意思了，感觉一点不像正常人说话，然后自动评价，提出了更高的评价方法。自动评价的思想，就是说把那个比较就是越像当然翻译更好，总会比较现在比较重要的方法就是用MST类的话。这个bP是用来做是什么的，这个bP就是那个它也翻译不了程度，因为你想一个问题，翻译越短会越来越好。因为越短我犯的错误可能是小，你给一个很长的句子，一个字一个字评审，所以这错误率的概率比较高。

## 67

它是起到了一个model，他们够评价这个是否流畅，那个比如说，100还150，可能就完全不一样，也就是年关越大，你的那个句子，就是这个评价可能有一个问题，可能不是很一致，举个列子或者会有一个结果，比如说是这样，然后MPG文件，我们有八个，然后起码有四个gram precision，有七个，都会分享，所以就用他们的公式的话。这个就是无论什么就说，比如说一个小子说sl一样，连如果没有看下这个就是四个，但实际上就是这个两个字，所以这时候你要推算框框的话，你看那个最多的那个方案，应该是，就出现两次也是他的一个培训，是S这个值A，这个跟人的打分数相关，就是相关性的一个结果。也就是翻译的结果非常差的时候，机器是能看出来是时好时坏，当非常好的时候，几乎可能见人就这个意见不太一致。说道评测可能给提一下这个图灵测试，一开始提出来时播那个聊天的。有一个人，这个是机器，然后跟另一个人聊天。如果这个人分辨出来跟你聊的是人还是机器，那通过了。这是图灵测试的种思想。但是这是用来判断一个系统，它是否智能的一个标准，如果一个性能通过图灵测试，就给出一个问题，就是说假设一个屋子里，每个人看一个词典，写的所有中文，问他们只能做简单的那种，说出来的这个，他知道所有的答案，而且中国用了就很难，但是不是外人就感觉这个人肯定很NICE。

就这个问题提出来以后，就有一个区分，就是什么是真的人工智能什么强人工智能，现在对这个弱人工智能的定义基本上就是说，你如果你能如果一个机器能够表现的像，像那他就是一个强人工智能就说，那个机器你知道他是真正的有智能的，比如说他是真正的理解有问题，后来他真正的看懂的一个圆圆就是是什么样的，所以现在这个强人工智能，很难说现在的系统，它就不是强人工智能。那种完全可以认为是神经网络，机器他有自己的一些理解，但是当然理解可能根你的理解不一样，所以现在说这个比如机器翻译的翻译结果比人好了，是不是是真是那个强人工智能还是弱人工智能，这个很难说。
以前那些统计，下我们可以认为他是弱人工智能，我们知道他结果是怎么产生的，你不知道他怎么产生。当然，也不知道他是不是真的人工智能，我就重点讲一下就神经网络机器翻译，其实上午周老师大概讲了一些转化，最开始就是做一个机器翻译，然后我会简单的再回顾一下。

大概是七几年了，就提出来了，当然那个神经网络还不太好使，然后实际上类似，只不过它没有那种明显的语音输入输出，他用了两个，一个是通知，一个是中间。只用最后一个来影响一个词，然后这个信息，因为两个都是你每个时刻的一个输入。就是这个模型问题，他在那个转区的上，他的结果还是可以的，但是在长度上还很难，因为特别短的词，它大概意思，他可能就是上那些辛苦当然，当那个长度在20左右的时候还要变长的话，就会明显下降。你的信息很难保证他可以保存下来，所以就就有人说，我们能不能有方法吧，提出来的问题开始解决这个常非常的依赖的问题，怎么去建立一个链接，我们还能讲话，就说我要是能进，我能不能用方法知道这近他就从里面反映了，我不知道这词还有什么意思，也不知道这个词对应的的是哪个词，那我能有方法就是说我给我用所有的input data，然后我把这个加进去，就是加了一个新的项目，用这个项目来做一个深刻的一个输入。方面有哪些改变，轻微的改变就是以后当他是一个方向的。

## 68

就是有两个东西，multi head这儿需要停一下，就是说这儿有一个jk是p的向量长度。就是你这个jk越大，
你source时候看出来，当你距离越来越长的时候，我希望你算的那个概率越集中越好，他起这么一个作用。还有一个position  embedding，他是天然能够识别顺序的。他是没有位置信息的。就是说偶数位置一个值得话，这样设定有什么好处呢，就是说相隔K位置的那个词，他是有一定关系的。基于k之前的和k来算一下。这个关系实际上就是对相对位置坐了一个建模。这块到目前为止有什么问题吗？你可以吧位置信息显示的建模。

## 69

那个其实MT的基本知识，都已经介绍完了。比较前沿的一些东西。主要讲两个比较前沿的东西，一个是那个数据增强，MTV一些应用，还有一些就是把APP的艺术用到了语音处理，语音处理就是自动语言识别生成，任何声音转成文字和如何把文字转声音。数据增强，这个对讲三个，三个技术和数据增强是一个什么水，什么概念，就是说，对，现在的神经网络里边，有很多参数，动不动就好几百兆，上G的或者是十几G的，就是这么非常大的这么大的模型，你给弄相应的数据，你如果能训练出来，所以这个数据增强就很重要，，你可能会有几千几千万或者上亿的那个trainingdata，中文到日语的，那可能找不到，所以这属于非常重要。

这个先讲这种就是，就是曝光偏差的问题，其实用数据增强技术来解决这个问题就曝光，他是有什么造成的，就是我们在训练的时候，我的答案是知道的。所以就是怎么来解决这个问题，就是我们对这种翻译的这种建模，我们可以有一个假设，因为他的概率应该一样的，怎么生成的，两个方法生成的概率都比较高，所以我们就引入引入两种，就是说，到那个从头到尾来生成的，概念就是算两个分部的他俩是一样的一个方法掌握的，这个是刚才说的那个先指出了那个，就是说我既要最大化语言就是生成的概率，我又要让他从左到右的分布尽可能一致，然后去追这个，就会就会得到他的那个更新的公式。这个意思就是说，我从右到左的模型进行采样，自己去翻译，翻译完之后，我用这两个模型打分来看。
其实这一块儿，昨天讲过了一个强化学习，强化它实际上就是强化学习的一个思想，就是通过模型自己，自身踩出一个样板来，然后放一个WORD，然后用这个去更新模型，具体谁做都有人说这个整个的公式，第一种是这个这个是通过这种公式可以选择，另外一个自己出来，然后就发一个过去，然后再去练。然后，具体训练的时候，你可以基础数据，然后把项目先，然后就可以就可以继续，提高那个发热。这说这里什么时候停止，你可以开发然后能看一下，看来这个翻译质量怎么样，如果发现质量开始下降了，就停下，这些结果也有了一定的提高。

training的思路，就是说我们现在的翻译数据，都是双语，就是比如中英，那这这段是比较还是相对来说比较少，当然现在监管是有2000万或者几千万的数据，但是对单个数据相比，但是来说还是小很多，所以不会用单语的数据来给机器翻译质量，就是整个这块就是刚才说的那个，然后这块儿就是就最大化那个的概率这个东西怎么怎么怎么具体怎么算，就是这个概念，你可以通过这个你可以把它打开就是说，实际上是一个引用空间的。就是远远的引入这个引用空间。你就可以继续正常一点的推导和去学这个，这样的话就可以有两个翻译模型了，这两个翻译模型就可以基于EM公试进行迭代。
[20:0.019,21:0.021]  一直到他的性能下降为止，这是一个结果一些情况，就是这个，就是讲了前面我们能够看到的事情，一个区段的这个语言，我们有可以有四1种模型，一具体细节就先不说这个我们用的这个技术就是前面说的那个结果。 讲第三个方面的那个方法就是SMT来做无监督的机器翻译，其实上午周老师也说了就是翻译的话，我们有一个种子，这个CM可能有100个200个，就比如说，最好把它基于这个思念，还有大概的待遇远远的就，因为你是在各种语言的训练，两个其实要是不在同一空间里面，所以需要继续训练，一个盒子里面把这两个空间设计基本的思路可以在里面。
 基本思路就是大概就是这样，他有一个非常长的假设，就是不同语言，它的问题的空间是具有很强的一致性，又不是同一个神经网络，小孩整天想的事情和我们讲的可能确实不太一样，

## 70

这些这个这个问题，对于AMD来说的话，可能是一个很可能问题比较大，因为AMD它本身的记忆能力是非常强的。你如果你给他的数据包的话，他有能力把人记住了，比如你在发的时候他会特别按照错误的来进行翻译。那有没有一个什么解决办法就是用Noise就是我的数据进行过滤，FT的好处是，像前面说的，我们技术具有对出那个发片段，如果两个片段他共同出现的次数比较多，他的他的基本上对应关系比较强，这样的话就是SMT，你可以可以抽取一些，噪音比较少的一些翻译片段，就这个翻译就是做事相对比较好的一个SM点，就可以对那个韦德数据进行过滤，具体做的时候还是有这么一条就是SM这个系统，因为这个SM系统有一些非常好的翻译片段。它用这个翻译系统那个概率分布，具体的那样的话来说，假设这是我们要的那个店，然后一开始的时候我们可以挑选的空间几何里面一个根分布来救这个人根本就不MTV进行那个nopice一下。
这个是一个行动化的一个展示，就是一个具体的结果。提高比较大，对这个数据非常的其实，其实基于前面的三个方法，大家可以看到，比如数据非常本质上就是，无论是用自己模型还是用其他的模型生成的数据，然后生成这个伪数据之后，我用这种方法把这个数据进行过滤下，处理完之后那个数据了再去训练这个模型。对这个数据分析的一个基本思想，后来就是这个数，这个我的数据，怎么生成出来，你可能是用那个翻译模型，你可以用那个解码器生成的那个数据也可以用，比如说SM就是，生成的数据的方式不一样，但都是需要先生成你的数据，然后过滤，然后用我的数据，这个数据共享的技术。就AMT这个序列转换模型，除了来过机器翻译之外，他还是可以，基本上他都可以做任意的串转化的一个任务，做简单的介绍一下两种方法任务，一个是SHESSARASR，这个模型是LES提出来，就是关注这个模型，他跟前面MT的模型区别是，这一块儿串 ，大家有没有学过就是时域到频域的状态。
傅立叶变换， 就是你的声音，它是一个一个关系图吗，是以12为单位，你可以用傅里叶变化变成一个频率上的一些向量是吧。这就是那个体育场的，好像会输入或者输出的时候，经过一个CNN四层的来预处理，处理完之后，这个就是方向盘下面的这个这个code。
我们作了一些改动，就是把这个非常简单，就是，那我们就把RNN那个H取代，然后还有就是我们把那个里面一个CNA进行取代，因为音频声音变平稳个非常大的区别就是，就是十秒之后，他可能会变成1000个现场，各项他的程度非常长，非常长的话，他肯定有问题，所以我们就把他切换掉。
还加了刚才说的那个从坐到右的一个，是结果现在做到了二八，然后错误率，就是1000词里出现错误，现PTS是把文字转成音频。

因为那个文字转语音的时候，他的顺序是固定的，这个都是没有调序的。它的顺序是是就是那个里面它是它的基本上是线性形状的。因为我们可以预测一个比较符，它都是一个都是一个符号，但是你要做TS时，他是一个向量，所以需要这是这不一样的地方。

下面我们做一个改动，其实也比较简单，就是AIR差不多就是把HTM请替换成了CON，然后用MAC的情况，这个地方加一个coding，其实这个这块非常好使，不加的话就变差，其实现在的GPS生成的质量非常好，就是比如说你去听是听不出来机器声，还是人声。

中文的我没试过，因为中文的语调不是很好找，因为对数据比较重要的，也有就是微软的产品做的，你要是论文还是在大家都有的数据比较好，跟人家没法比。如果五年前那个导航的声音，你是明显听出来是有问题的，但现在的话， 语音好很多。TTS的时候会有些问题，有的时候可能会读两次，可以播放各种停一下，就比如说flash，对，你看后面再清楚就是她的问题，那个概率，他一直重复这儿。一种情况重复了好几次，所以连不连续这个就是这种一类模型，自己去学可能就会有问题，他必须一个一个和自己去学，我们需要一个就是这个完美，预测一个每个词，它大概需要用多少时间。

这样的话就没有任何的那种，没任何L产生立，就是很复杂的一些东西，比如说，这种很长的数字，甚至是URl，一下乱七八糟的东西都可以，他都可以读出来。我们大概还有半个小时的时间，大家可以随便，然后大家讨论一下，或者说你们想了解哪一块我可以给大家讲一下。
























