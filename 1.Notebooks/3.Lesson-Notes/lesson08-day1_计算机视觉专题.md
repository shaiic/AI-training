## 0.

图像处理的问题，这一条刚才我们提到了一些简单的成像原理吧，然后或者做一些简单的处理，就是说你比如说你给我一个图像，然后我把它做一些滤波做一些把我会谈在谈到这个滤波对吧，然后你最后输出一个可能向更像是一个对这个边缘显示这个物体边缘的这么一个图像，这个这个实际上是一个边缘检测器，他跟他操作两个人做完操做出完全超过以后，他最后会得到一个能够反映出边缘的一些一些图像，那么对这个图片做这个操作，有不同层次的，有的非常简单，这个这个叫我们intensity transformation，这个就是说是点对点的操作，你给我一个像素，一个词，我把这个词转换另一个词，比如说把黑的变成白的吧，这个非常简单的操作吧，这个点对点，所以它的输入一个值，输出给你另外一个词，然后这个怎么转变，看你的你的任务是啥吧，那比如说这个地方，这个这个样子叫做图像增强，或者伽马transformation，实际上刚才他刚才就这个表方程的表示，就刚才那个取现了这个取现，不是我把设计成这种POWER function，输入是一个R，然后起一个POWER，然后得到一个S吧，然后这个方程depends on R，所以说你可能输入两个相差不多的时候，我输出的时候可能相差很大，他的曲线把它拉开了吧，为什么他其他图像增强的这种功能，比如看，因为这边你看起来都白了吧，就有拍照的光线太强全白了个白了，因为所有的项目，他们吃都很接近你分不开，我们已经通过电话把他拉开了以后你就可以看得更清楚了黑的可能更黑白的更白，爸说，你这个图像就感觉他的这个对比度就大起来了，是吧，然后这个大概取不同的这个伽马值了，你可以调整，你可以看到有不同的这种效果是吧，这是非常简单的一个点，到点的操作，那复杂一点，稍微复杂一点的就叫取平均值了吧，这句什么意思，就是我对每一个像素点，我要输出一个值，那么我用了一个window，一个窗口，比如说3x3，然后在这个九个不同的位置，我把它平均加起来取个平均值吧，这也是经常操作，那么我们可以对图片作品做一个平均值，就是那个图像里面，它相邻的像素本来就是很接近大部分情况下他都是很接近的词，所以我做这种加权平均以后大概不会把这图片弄的太难看，要不然的话你随便哪个做权利平均要那就可能会影响很大，那这段为什么做平时的基本都造成对吧，这种点噪声，所以我们利用他这个图像这个平滑的这种特性，我们说好我就我知道，可能有点噪声，就比如说这个白的，其实都应该黑的，但是这边都一个白点，那我希望用利用周围的黑点，做一个加权平均以后把我白点把它降低一点，这样的话我就可以把这个白点至少把他的效果器去调点，所以这是一种这个平均值的一个操作，对吧，你可以看到3x3的时候，你可以得到一个这么一个图像，这个调这个白点稍微稍微弱了一点吧，或者用个五乘五的一个窗户取得平均值，然后放在中间这个位置上。

然后窗口，你可以移动滑屏滑移个位置你就对应这个位置取个平均值放在那再滑一下，这个，这是一个衡量图像质量的一个标准一个度量，越大越好，对吧，所以你看做完之后处理以后他至少根据这个度量，它是提高了20提高23吧，所以它确实起到了去噪声的作用，当然他也起到一些不好的作用，你可以看到这个人脸看上去有点模糊了吧，因为取平均以后那个边缘地方就可能会模糊掉，所以他有一个trade off，需要注意的，比如说新用户成本的话，可能就可能会太模糊了吧，所以这个在设计的时候，你可以自己去决定好，然后这个平均，我这个可能不太细说刚才说说平均值就是大家都是加权都是平均加起来对吧，都是相当于这个窗口一直都是一直都是1，那很多时间，因为我们可能会用一个叫高斯的这么一个形状就中间正中间那个词它对应的权重很大，离中间值远的地方，它变小了，所以这个也是很有道理的，因为本来我就是要算中这个词，它的值什么，那我当然最相信我自己的原来的词，那这个越远方，他可能也不像对吧，所以我就把它用权值小一点，这个高斯这个公式表示他的权重大小，你可以得到更好的效果就比如说你跟刚才比，如果刚才接了前面那个大二12，这里是24吧，所以用高斯的这种形状的去做平均，他可能可以得到更好的效果。

那你认为就是去去设计了到底是高斯还是其他的，什么样的滤波器，我们就叫滤波，这个是你实际用的时候你自己可以去设计就是，但这个你看都是人为的设计，大家觉得是高斯可能不错，把它形状可以调一调吧，你试一下，那我们在看一个另外一种跟他相反的一个操作，我们这个叫做sharpening linear filter，是吧，看起来更好看，清晰度更高点的，所以这个你也可以用另外的滤波去拉巴斯过去对吧，所以我们看它的，它的这个速度公司讲，但实际操作起来在数字这个理由那个数字信号处理，操作起来他不能像这个这么一个窗户，然后这些都是权重。

所以他就蛮想把中间那个词去减去周围四个词，他就我这个词跟我作为到底差多少，所以他关注的是什么是边缘，就有非常细节的一些东西，他关注那些东西，如果说是这个输入这个平面的话都是一样，飞度的话，他输出是零的对，如果是一个白板的话，然后乘以这个窗户以后，所以他对平面的地方被背景平面的地方试试输出是零，但是队友边缘地方，它就会输出大一点吧，所以他像是边缘检测器的有点这个味道，可以不同的的设计，所以你可以看着这个输入的话，然后你用滤波器滤一下，你会得到这么一个输出，这说你可以把它放到我把它放大看的清楚一点，它就变成这样了，对吧，所以看到这背景的地方，他就没有没有输出，但是对这个边缘这种地方，他的数据会大一点吧，所以他在提取边缘，哪里听说你在他加回到原来那个图像上，你就会看到一个更好看更清晰的图像，所以这个就叫图像增强，不是这个图像sharpening，如果你嫌你说我拍照片模糊了，或者是怎么样，你可以通过这种操作把它变得更好看一点，还有一种这也是常见大家拍照时我的手一抖拍下来是个很难模糊的照片，这时候你可以去去模糊，你可以有一些这个我不细讲他怎么做的，有很多不同的方法，你可以得到一个比较清晰的图像，当然这是比较理想情况下虽然没有挣到这么便宜的事吧。

那你也看到这些图像处理，如果你后面要去做图像识别的话，你到这张识别很难识别的是吧，所以你可能需要先做一个这种low LEVEL的这种图像处理以后，然后再去识别，你才可能够做的比较好一点，或者把这两个结合在一块儿就说处理和这个识别结合的一块儿，还有一些这个也是个例子，这个比如说叫inpainting就说你图片被折了被划了，你想把修补回来，所以这也是一个你可以做一个类似于滤波器的这种东西最后，这个还好，这还算没有太夸大。

比原来那个稍微好看一点吧，这个inpainting还是有些效果，然后刚才提到这些这种low level，然后他他其实也不管他不管输入的是啥，反正是像素一大堆像素一个阵列，我反正做一些移窗的处理后的输出也是个项列，计算机视觉可能更关注的是我们想知道以后到底是啥物体对吧，所以我们这种概念的东西，我们希望把它抽象把这种概念的东西抽象出来，所以我们可能就会更关注里头到底是物体是什么，它的边界在哪，有边界么，可能在判断物体是啥，然后了物体之间的关系，希望了解东西就是个也是抽象概念的东西，那往那儿走了，其实以前比较传统的很重要的一个操作就是这个边缘检测器。

先把边缘找出来，然后在这个基础上再去判断这是不是一个物体，那是另外一个问题，随便选一个非常基础的一个课题，在80年代也是个最热门的一个课题，那个时候大家都在设计什么样的边缘检测器能够把这个最好的边缘找出来，Canny Edge Detector基本上是可能是最好的设计一个最好的那个边缘检测器吧，所以他基本上的操作也是拿个图像我在这个方向把它当成随便哪一行出来，对吧，把它作为一个一维的一个信号，然后你在那边去做这个，这个有点类似于gradient，求导，这个一阶一阶求导在也是找这个差值了，找差值对吧，所以把这个边缘的值会大一点吧，所以可以在原来这个方向走。

代言的这个数的方案可以带走便去检测，把每个单位的检测，反而放回来拿第二列做一个做这个边检测器，然后放出来，所以看到这么一个然后你可以把这两个结合在一块，就要快那个这个比如说两个词加起来取平方，加一块儿也可以得到这么一个图像OK，然后你再去做，你可以得到一个这个就比较主要的边缘，你可以把它检测出来了，有的细节就忽略掉了，比如说帽子上的一些花纹的那些可能就你就不是你关注的东西，你关注的是一些最主要的这种边缘的东西吧，然后你也可以再进一步的把这个边缘细化，把他弄成一条线，实际上便可能就大概这条线理想上面一条线是吧。

你可以做一些细化的操作得到这种边缘，然后说你甚至可以把它近似成一条线了，对吧，一条都是一些直线这个然后进一个基础，你可以去做一些比如分类这个东西可能是一个杯子，因为有些线在那个地方吧，或者可以做些其他的一些物体的检测，所以传统上很以前可能都是先把边缘找出来，然后把它作为一个我们叫做特征值feature，再利用后面再设计一些分类器，什么东西，然后做一些分类的那种，然后这我就提一下这个大家可能也比较熟悉了，对吧，这个指纹的话头像你可能需要做一些指纹匹配，好吧，那么这个大家要需要这个指纹到底怎么匹配法里头类似的过程，你要想法找到你刚才说的特别直用这个边缘检测器，其他的更复杂的方法，把一些主要的特征找出来吧，然后找到它们各自的特征值吧，然后再利用这个特征，只看他们是很相近，后面我会给个例子，但大致概念，一般都是先找到FEATURE就或者一些就说description这个图像到底有哪些点可以来表达来表述这个图像，然后，你把它作为一个数据，然后另外突然有内似的数据，然后你看他们相似度多大，是一个比较底层这种low level的vision对吧，同样处理那么，其实有两个。

要的任务是吧一个，就是四点肯定是看那个东西有问题，我希望识别它这个后面那个打代码表示会讲一下，然后还有一个就是三维重建的，因为毕竟这个物理世界这个三维的，你拍下来图片这个二维的东西，你要理解这个场景，你最后最好能够把它还原成这个三维的这么一个一个场景，对吧，才能更好地理解他，所以这也是非常重要的一个任务，那么这里头可能有一些数学的东西，比如说这个几何的一些东西就成像吧，是怎么成像，会稍微再谈一下，所以可以然后，当然，如果有两个两个摄像头有点像人眼的这种，这叫立体的，然后你就可能可以能看到一些深度的信息给他重建或者去估计一些深度的一些信息，我会稍微谈一下对吧，怎么去估计这个深度的信息，然后说你有这种这个。

360度还这个布置了一大堆，这个摄像头，每个角度都拍，然后只有你可以把这个整个三维的场景给他重建出来，不同的角度，或者说你可能也有一种你这个摄像头或者是你的无人机在那里飞，所以拍的一系列的图像，然后，利用reconstruction这个技术，也可以重建出可以重建出这么一个三维的一个物体，谈谈这个怎么做一个重建我这里时间有限就给大家谈一些非常基本的一些概念非常基础的，那这要从我们成像这个过程来讲吧，我刚才前面稍微谈了一下图像怎么形成的，这个实际上也是一个最基本的一个model，这个MODEL，也就是说，在这个物体是这么通过这镜头对吧，成了上头去了，成一个项后，我们看到其中一个点，那么我们知道那个真正的问题，肯定是在这个点和这个这个孔之间这条线上，对他的限制这么进来，然后在那里成像，但是我们不知道他到底在哪，在这条线上他在这还是在更远的地方，这棵树长得稍微大一点，然后你也可以成同样的项目，所以这个成像过程，其实我们叫lossy，也就是说，你丢了信息从三维的物理空间到二维的图像丢了一些信息，信息上就是深度信息丢了你不知道他这个物体到底是在它的深度是啥的，它多远，所以这是个问题，你看，图像你也不知道这里头人到底离你摄像头多远，那你上次想恢复这些信息。

还有一个，大家可以知道，因为拍的时候是从那个角度拍对吧，所以，一定有物体被遮挡，比如说这个树后面还有一棵小树，可能就看不见了，所以三维的信息最后拍下的时候，上次你拍不了全部的信息，所以如果要整个真的把整个三维信息都重重建出来的话，那么你可能就像刚才前面提到那需要多个摄像头不同的角度对将来会不会漏掉一些信息，然后当然可以稍微讲一下这个深度，怎么去恢复对吧，假设说我们现在有两个摄像头就跟人眼一样有两个摄像头就是左摄像头，右摄像头，然后他们再拍一个物体对吧，我们刚刚提到假设我知道这个物体不感兴趣，他在我这个图像平面上，他在这里。

然后在另外一个摄像头里面是那个成像他在这个位置上，那我刚才前面提到，因为刚成像原理，所以我知道这个物理X一定是在我这个摄像头的那个针孔和这个图像这个点上，然后你把两条线往外走，他一定在这条线上，那你知不知道他到底在多远，他在整条线上，如果说你知道相对应的这一点在另一个图像上的这个位置，那你也知道这个真正的物体X，是在这这点这点连的这个连线上，你不知道他具体在哪个点，但你知道在这条连线上，然后这两天居然最后他是他会相交的，所以那个相交的点就是你物体真正的在三维空间的点，这个能理解吗，就说当图像有一个不确定性，你不知道具体多在这个现场哪一点，另外图案大多单只是这个图像也一样，也不知道在哪，但如果你有两副图像对吧，你都知道这两个点是对应的同一个点会共同点，那么你可以通过这家传统雷神锤是三角的，这个也可以计算出这点到底在这个三维空间的物理空间上，他的位置是多少，然后就你就研究这个深度信息了，对吧，你就知道它的S就不仅是这个它在这个平面上的信息你也知道这个点对应的实际的这个事情的这个清楚，就是对，那，那这个其实也是很简单的，也就是说这条线我们知道这条线实际上在这个摄像头里面，它会成像是吧，这个这是一个具体的一个为空间的。

他会丞相他为成像在这条线上这样说一定会过，刚才我说的这个点对应的一个点，因为这点这点会成像就这点也会成现在这个相对那个点上，所以这条线网的这个，这个计算机视觉里，我们叫BAY吧，然后，所以，所以根据这个，我们有不同的，不同的情况下，我们可以有不同的，任务不同的那个一些解决的一些方法，比如说Epiolar Lines，我这里刚说这个Epiolar Online大家提到对吧，要不然卖就是说在在第一个摄像头的原点和这条线他他线上他在另一个摄像机那个摄像头上的成像这条线我们就叫做Epipolar Line。

几个例子，就是说你比如说吧，这个是一个摄像头，第一个摄像头拍下来的他上面有几个点，比较特殊的点，那我们感兴趣说这几个点，他到底在另外一幅图像上另外一个摄像头上，他在哪，那么比如说这点他一定会在这个图像上一条线上，刚才前面还说了，他本来就是物理空间那条线，所以他一定会成像会存在另外一个视角上它变成一条线，那么这个线呢应该对应的是这个绿色这条线，因为他也过了这一点，比如我们也知道这一点对应的是这个位置吧，所以它的相对定位这条线上那这条线呢，反正这个这个，这个这个示意图了，他一定在一条线上同样对另外一个点，他的这条线也会对那个线，而且一定是过这一点，这个头这个地方，他用这个先要过这个相应相对头这个位置，然后有这种选这个好处，这个好处就在这样的就如果说你给我指定这个点。

 

 

 

## 1.

然后你说我不知道在另一个图像看他到底在哪，我通过刚才那种关系，如果说我知道那个摄像头他们的位置，她们的朝向三个可以计算出这条线到底在哪相对，几何空间上，已经定死了，如果我的那个摄像头的参数，朝向全是定死的，以后你给我一条线，你给了我这个点已经把定死一个，这条线在另外一个位置已经知道就是位置和朝向的这个这个图成像平面上这条线，我们可以算出来的，我知道在这条线上只是我不知道具体在哪，因为我本来要找他的匹配点吗，我知道这个匹配一定在这个图像上，而且一定要这条线上，这样就很方便了，因为我要找他匹配点的时候。

我就不用别人点上去找吧，找到最下面一点就行了，所以这是一个方面的应用就说知道这种关系跟我们提到这个关系的时候有这么一个信息，这个支持以后，就看你弄啥了，刚才我提到的是给我一个指定一个点，那我至少可以利用我刚这种支持我可以减少我的计算量，我不用再整个图像上去搜索我就在一条线上去搜索就行了，所以刚刚提到一种，第一种就是相应的两个点，那么你可以算出这个三维的坐标，第二个刚才也提到了就如果一个给定，然后你去找那个另外一个你是可以通过刚才那条线那个支持只在让现场去找就可以了。

所以可以减少计算量，如果两个摄像头的位置朝向我都不知道我就随便摆两个摄像头在那儿，告诉我这个图像里面，这个点到底她在三维空间位置在哪，这个就更难了吧，这个是说你有好多参数不知道，首先两个摄像头，他们的三维位置不知道，他们朝向不知道很多参数，所以这时候要去解他，你要怎么解，你可以告诉我，如果说你能告诉我两两对应的这些特征点他们是哪些，比如你告诉我SL对应的是XR，这只是不够我刚才太多了吧，我解不出来，那你认为我好多倍，这点对应这点。

给我足够对了，以后，我希望通过数据的话，我可以算出来的，我可以算出这些那个摄像头的位置，它的朝向，然后以及任何一个点，它在三维空间的位置是啥，所以这也是一些另外一个，你不知道他们参数是啥，那你要去做校正吧，你就必须告诉我，对应的是那些点你告诉我足够了，以后，我可以去把这个摄像头参数给算出来，那这个大概是可能非常基础的一个三维重建的，这样的一个一个非常重要的一个参数，你知道这些几个关系和实际上也是可以做后续的一些很多工作，刚前面提到你需要给我一些这种匹配的对应点那图片，大部分情况下你不会手去标对吧，太累了，也听不出希望计算机自己去找到找到哪些点，而且是好东西点，它真的是能够匹配上是非常，唯一的这种匹配，因为是哪一点，那可能有些不同的这种，选择性选择项都有可能，你希望找到这种最可靠的这种匹配点，然后基于他那就算跟他后面提到那些摄像头参数，所以这个我们叫feature point，你怎么找到其中一个很重要的就是这个DISTANCE就是你希望它是它是非常有利的，那这给一个例子。

他并不一定是个好的featuring point吧，因为另外有太多点了很像的你也不知道哪一点，所以所以并不是好东西，那么这个点可能是好东西，这点因为还毕竟还是比较有特点的一个点，那么在另外的图片上有可能找到可能唯一的一个跟他最像的一个点吧，所以这个就是属于这个FEATURE POINT Desire的问题，你怎么去设计最好的这种Feature Point，然后在这个基础上再做一些其他的一些操作，那么只有见我也觉得不太细谈这个具体的怎么去找featuring point，那么这边例子，可以想刚才那个背景那个平板的一定不会不是好的featuring point，如果是那个角度能更好，在角落里面更有特点，所以有时候就会找一些Corner point，角点那个它更具有代表性，更加有特那个自己的一些对比，所以这个的话就是这个detection控制它的输出，他给我图像，他就会说，这些点红色的这点这点事好点，比较可靠的吧，然后你也看到这个点也都还有特点的很多都是在这种比较extreme，因为这样他才有特比较有特性吧，然后你如果是给我两幅图像对不同的视角拍下来的你都找到这些匹配点了，然后你下一步就是说这他就找到这好的点，首先第一步找到好的点，那你下一步就说我这些到底哪一点哪一点是匹配的，你要去匹配，比如说你说这个好像不一样，对这点。

所以所以你首先第一步找到这个可能的这种好的这种feature point，第二步你如果给我两幅图像，去找他们匹配的两两对应的一些点，如果找到以后，然后下一步就我刚前面提到了你可以通过数学的方法去解，我那个摄像头它的参数是啥什么的吧，因为参数值有限的，你只要有足够的这种匹配点以后一定可以把那个参数可以找到比较好的参数，所以这个时候用这种立体成像原理去找这个三维的，这个恢复尚未这种信息，那还有一种大家可能也比较熟悉的这个方式，就是属于这种比较active的方法就是我不是用两个摄像头，用一个摄像头就行了，对吧，但是我的要求是要可控，所以Kinect它是靠这么一个一组这个这两个，一个是，算是三维的sensor吧，一个是摄像头，一个是我接收光，然后成像第二个，是我是发光的，是发射一些特殊的一些光发到物体上，然后反弹回来，对吧，然后被他成像，然后我利用这个光，我知道这是什么光发出来的对吧，然后，这个光经过这个物体反弹以后反射以后。

他会在可能取决于物体的形状远近，他成像不太一样吧，然后我利用这个成像这我就分析given我知道这个发光体我可以算出来到底这个物体是多远，就是我是控制发光，然后到我只用一个成像，这个sensor一个接收器一个成像器就行，另外种方案方式去得到这种三维的信息用这个，这个提一下这个第三个，前面提到这上次就是界面可能最基本最重要的问题，

Recognition，其实是有不同的，层次上的那肯定是也是不同难度的connection。

简单一点的是这种比较图像分类，它是在这个图像层次上，你跟我头像你告诉我的图像是由是啥有什么东西是吧，所以他不是非常细腻的，只是说整个图像里面告诉我一下，主要是什么东西，第二个叫物体的检测这个就要求更高一点要告诉我，他有猫，你告诉我这个猫在什么位置，所以他跟这个location有关系，就是localize物体到底在什么位置给他更难一点更难一点的就是这个叫分割，因为这个detection，其实经常我们要求只说一个框把它告诉我大致位置就行了，，要把我这个轮廓给找出来，更细，所以在bounding这个LEVEL，在像素的level。

他是背景还是在吃这个物体还是另外一个问题，他这个要求更高更难，这个可能导致后面会，在想进一步详细的介绍，这一套，我稍微再提一下，那神经网络应该你们的学，那基础上应该有一些对，是用这个神经网络去模拟一个输入输出的关系，对吧，你如果告诉我输入这个输这个我就可以去训练，把参数调出来，最后能够达到你刚才告诉我这个输入输出的关系是吧，那网络，当然最后大家可能也知道这个他真的用起来还是应该是在2012年的时候过年的时候，那个新城的这个这个大家肯定很熟悉，这个是自动的，不好意思，他们就做了一篇就把这个神经网络，用到图像识别这个任务上是在非常大的一个IMAGE NET，他可能也听说过这个词，因为这个这个大一百多兆的图像数据集上去做识别，然后大概是1000内的不同的物体就做这个分类，然后这个东西这个动物，其实，一个是这个图像数据很大，以前就没法做了是吧，没有做，所以后来有GPU的，去优化一下，还训练了一个礼拜把他训练出来了，然后参数很多，六千兆的这种参数里头，这样的参数，所以用GPU继续训练半天，然后以前的神经网络，你们可能前面介绍的可能都很窄的三层四层就完了，等到在做饭去了那这是他们八层七层，把他训练出来了，所以说效果非常好，那个一下就把以前的这个任务的这个性能一下提高了这个就让他10%几的，处理下要涨价到百分之七八的忘了吧，就已经下提高特别大，所以当时非常轰动，然后有了他以后，全民最多就是做deep learning就是deep learning可做现在的源够数量有足够有些情况，这个效率足够大量数据量有标注的数量只有标注的数据，这个大概给他介绍，因为有了它以后计算机视觉方面发展非常迅速，这个一大堆都出来了，然后这里可能只有大家介绍一下。就说这个CNN到底是到底在做啥，我们前面提到了我提到说提feature，其实我们是在人为设计高斯filter，都是人为设计的，其他我们不知道，真的好不好，他好不好，特别是对于特别难闻，不知道那是个什么网络它的好处，就是说我是用数据去训练好不好，数据说了算，所以说出来说好就好了，对吧，所以他通过数据去训练自动去学什么样的，这个filter那我刚这里显示的就是这个是在底层比较最底层，那个比较基础的那个他提出了他的feature就是参数，实际上表示的是他对这些pattern很敏感，他看到这些pattern数值就会大，每个神经元的数值就会大，所以她就这个filter，比如说你一个输入输出响应，我看到他的输出就会很很激动。

所以你可以看到它底层他就是对这些pattern感兴趣，那这个pattern是啥不就我们前面提到的edge detector，所以她也跟我们以前这个人为设计其实是一致的，我们学出来就很好，再往下看，后面低层次高层次上的时候，它对它敏感的pattern就变成这样子了，你看这其实已经是有有物体的概念在里头，如果输出有这么一个物体，那么他的神经元就会有个比较大的输出，而且这个它是完全自己学出来的，如果能够设计我们设计不是这个东西，这个太复杂了，能怎么知道时间长，这样对吧，所以你看他就完全用数据驱动的去自动学到这些feature，他为什么那么powerful？，就是他可以做一些我们没法做的事情，这个太teadious我们自己去抠这些，而且你想想是6千亿神经元的你不可能轻易自己去扣的话，因为它这个它是有输出，输入输出的对应关系都有光速是那种所以知道你调完以后训练好以后它到底性能多好，你有一个number告诉你的，所以通过这样的话可以学到对至少对特别是这个任务specific，你跟一个专注的我就知道这个来讲到底怎么样训练出来的，这个神经元他到底是多好，对吧，然后他应该长成什么样子，所以，所以确实有了它以后，它后面就就业发展的非常快，是吧，我可能最后几分钟稍微讲讲，那现在我们的状态是啥的，我们可做。

给一些depth的信息出来了吧，然后把这个放上对吧，IPHONE有一个所谓的初代现在IPHONEX上才有这个能力拍人脸识别对吧，他有active的sensor，这种SENSE会发光，他会这里有个一个，对他有个PROJECT，这里会发一些PATTERN反射回去，然后用另外一个CAMERA，我们看不见他发一些不可见光红外线光对吧，然后通过这个接收器接收出来，然后分析这个变形对吧，这个原来发出来的，不知道说啥，然后中间有关系，然后可以判断出人脸的或者物体的位置关系，它的深度关系。

然后我们也可以做一些这个三维的这个重建对给我一个物体，我这个拿东西，扫一扫就可以把它上面给找出来，就是反正你给我那么多，这个不同的或者像无人机在飞的话，你就可以把他这个应该是斗兽场斗兽场的这个三围的物体重现出来可以做到一定的程度，OCR这个我们也有很好的engine现在，应该是做的比GOOGLE好好一些吧，可以去识别数字，车牌，一些其它的一些check或者receipt，或者是一些报销单，可以自动去识别，这个大家很熟了，这个摄像机里都会有人脸自动检测器对吧，那做这个当然是为了聚焦了，有这种以后你就可以聚焦到那个人脸的位置上就不会模糊，后面那个可能更多一点，是物质的识别也是要做的很好的，然后人脸识别这个大家也听很多了，这个，这个可能应该是也许是计算机识别里面最有用的，现在最广泛的用起来的一个一个任务了，就可以识别人的一些性别，年龄，等等以及然后在这个基础上，你可以做些Filter，做一些滤镜，你也可以做一些Base PRODUCT SEARCH，下面我要找这种类似的椅子，然后网上可以给他类似的这种沙发，或者是等等的这个东西，这个不知道有没用过的这种微软，微软识花是吧，这个软件一些家庭有孩子，可能会用吧，就说你去植物园，看看花拍拍告诉你这个是什么花，对吧，然后也可以做的不错的，这个一些这个比较常见的一些花都可以识别得很好，然后稍微讲几句，可能现在还有些问题吧，现在三维成像还是很大的问题，特别是你这个视角没那么多的情况下会不要把重建的高质量不就是你拍中，你可能要从其他的没见过那个视角再去看他，你希望看还是看很舒服，对这些不太容易的事情，还有一个比较难的就这种吧，这个就说，你要你去做推看到的图像以后你再说这到底发生啥事了，那能给我们能可以做一些推理计算题做的推理，比如下一步要可能发生啥事这东西是不是掉下去，或者是吧，就这个就更难了，就是往高层走一步吧，然后这个也是对吧，这个版本看我们可能会下吧，就是想不起来了，然后最后大四角合理的这个实际的一些ACTION要怎么结合到一块也是另外一个，当然也有一些工作了对吧，你比如说拍的视觉有，然后，然后计划怎么走，对吧，这些都有些有些application，但是怎么样做最好对吧，然后视觉里可能会有一些可能犯的错误，怎么样能够让整个系统对这个错误会主办的吧，不要因为这个视觉上出错后面全错了吧，那些都还有很多事情可做，我想这里头给他12点多是从这个一些材料，或者是一些Course的一些materials，所以大家可能有机会可以直接去看有更多的一些一些信息吧，你们可以去看一看.

 

 

 

## 2.

今天我讲的内容就是在神经网络空间对几何形变进行建模，首先是一个背景的介绍，然后下来是三个在deep learning时代比较work的技术，最后是相关工作以及总结，这几个形变性建模个比较重要，也非常久远的任务，很久远的问题之前文君老师也介绍了，这样的STARTMAIN，他可能来源于PART deformation，是skill的变化或者或者叫做intro class variation就这样的同一类物体或者甚至同一个物体，它就呈现出来的效果上就会有非常大的区别，然后，就说，因为这个问题非常重要，所以说就有很多方法尝试去解决它，传统方法有这样可以分为两类，第一种方法说是在训练数据集中间，然后去，就是加入人工的去引入一些几何形变，然后，把这个增广之后的数据集，给一个给定的网络模型进行训练，然后希望我们模型能够很好的去利用这些增广之后的，这些deformation的training出现的一个example，通常来说就是对训练样本，比如说数据，这些字符，做transformation出来，第二个，就是说是用这个transformation environment的FEATURES这个就说是指望我的模型或者算法本身，transformation environment，就是不变性的特性，比如说SKILLSTRANSFERSAFETY是DP时代一个非常有名的一个特征提取算法，它的基本做法，就是说是基本做好只用鼠标就能做到，就是说是对于每一个点，他每个他想提取特征点，他会去估计一个他主要的一个方向及主要的一个SKILL之后，在这个给定的方向和skill里面把特征给提取出来，然后再把它给normalization到一个主方向和skill上面去，这样他指望说这样能够制作的两张图匹配的时候，它同样把对应的点的FEATURE能够克服掉它SKILL，还有就是就是尺度和旋转的变化，另外一个很有名的算法叫做DPM，这是deep learning时代之前，那个DEFAULTSTANDARD的物体检测算法，他的做法，就是提取特征，然后再过一个SVM分类器，然后其中再获个F后续或者特征的时候，它的提取特征的区域就是他的PART，他是可变型的，他是符合一个方案transformation，我们行就是符合一个仿射变换简单的，比如说这个自行车轮子把他给提起来了，他就会把这些PART就是把它给提起来都用到这些自行车轮子这样的区域去，然后，他的问题了，就是这个节目H的这样一个形变，然后，需要用Handcrafted的方式去设计environment feature，还有算法，就比如说他们的geometry，比如说我们都是假设他是一般都通常假设用非常简单的一种变化，一般都是仿射变换这样一种非常简单的变化，然后，就说这样我先规定，它是某种特殊的一个很简单的变换的一个FAMILY，一个集合之后吧，对比如说就是用六个参数就可以刻画的一个仿射变换，在这里面之后在一个非常简单变换之后，然后我们再去估计说这六个参数究竟长什么样子，所以就说是他对稍微复杂一点的形变，比如说是非钢铁形变等等，就说你不要需要用多少个自由度来刻画的。

[然后再加入我们进入到了CNN的一个时代，CNN是一个时代的就说是普通的CNN就是对这种大的几何形变就普通SIM模块对接大结局里面，他们是无能为力的，这是因为通常CNN的这个，他的一些基本的构成模块和单元，他们对几何形变的建模能力是有限的，比如说普通的convolution，他说在]input的feature MAP上用一个固定的三乘三的一个REGULAR的KERNEL对她对她再在上面做Sliding window，然后，你可以产生OUTPUT FEATURE这样做sliding window的整个在扫一遍之后就能到output的feature map，就是convolution最基本的操作，然后，它在不同位置扫的时候，它的convolution kernel都是长得一模一样的都是三乘三的这样的一个kernel, 所以它这个就说是无法比如说当你把很多层的convolutional stack起来之后，他就说他在任何物体上面，他看见的input的，看见的区域，他都是固定不变的，他并不能够很好的对几何形状进行一个建模，然后还有一个重要的operator叫做R pulling，是在detection里面一个很重要的一个模块，他就说你在全图做完feature之后，然后大家非常慢，这个时候你用R pulling在这张大的FEATURE MAP上你对于每个vision proposal对每个候选区域，然后，你就把他的feature给抽取出来，比如说用一个三乘三的一个degree把他抽出来，然后再用一个非常浅的一个网络比较小的网络，然后对每个区域判断究竟是人还是背景，大家怎么回事儿，然后，也是一个三乘三的一个或者七成七的degree，这个时候，她对几何形变也是很好建模，比如说这个格子是一个人的手，这个格子对于人的肩膀，那你到时候在神经网络后面神经网络看见的时候，他的appearance的变化对这两个人来说，它的变化造成非常大的，所以他在需要他要学好觉得很好的一个非线性的，一个表达或者这样的一个变换，它能够把这种geometry的这样一种transformation引入的appearance的change，能够把它给表达出来，OK，然后所以这个，首先第一天工作的叫做SPECIALTRANSFERLETTERBOX，这个是来自牛津的实验室的他们一个工作日是2015的文章，这是deep learning时代第一篇很好的对transformation进行建模的工作，这个工作了，我先看一下网络结构，最左边，这是一个input的image，他先过一些网络层见得到一个U，这个U是在这张，就是他可能存在比较复杂的几何形变的image上面产生一个U这个时候，我中间指望通过某种变换，然后得到一个normalized的一个非常MAP V，我们希望他是就是说他的物体，比如说不同的字符，1234同一个二，比如说他的，都是规规划到一个固定的位置上面去看他怎么做到的，他就是这么多，他是在加了一个CBD的一个档期，旁路吧，对CB这个CB他是当前的一个network，他要apply network。

通常就是一个非常简单的变化，比如说仿佛变化发生变化的参数CL就是有参数来刻画它会下也可以看到，这个时候，在做规定违反这个东西，就根据这个SIM卡，然后就会对这个在上面做sampling，然后就是相当于sampling位置的值，然后填到里面，然后得到的非常的非常简单，好吧，那他具体怎么做，这个就是他的具体的那个中间的操作，这个就是一个仿射变换的最简单的表达，XYIT代表的是那个TARGET就是output featureMAP上面的一组坐标，他们的变化，通过六个参数来刻画旋转，然后，请倾斜，然后SKILL的变化等等，反正是相对比较简单的一些几何形变，比如就像这个酒这样这样转一下这种它是可以表达的，好吧，OK，那接下来这就是到了一个最关键的一件事情，就是说是，我们有了这个变换之后，我们怎么就是说从input featuring map U上面的值，把他采样的output featuring map V的做法，是他通过做事，保利练三步的那种方式阿双线性插值，所以它的发展的一个旨在把红框的值它会从阴部的CPU上面，他跟他最最近的四个值通过一个线性插值的方法把她给擦出来，对，这就是一个线性插值的一个计算，这是它对应的一个公式，好吧，然后这个地方有意思的是说我们试试用神经网络训练的时候是需要有线信号的，这种怎么去做这个了，因为那个网络的训练， 会对U产生导数，这是非常直白的，但同时这个式子里面，它是能够能够对这两个坐标才能产生导数的，就是说这个数学基本的微积分求导就能够求出来，而这个导数，那就能够就说这个V上面的导数他即会通过这个记录传到一部分马路上面，然后，另外他会可以通过bilinear sampling传到这边的localization的网络这边，这样就能够通过这个实验目标，新的训练也drive就说我们应该怎么去赚这个INPUT的HTC能让他最利于识别，然后这个是他的一个效果就比如说我们对于字符识别的时候，它就能够就把，比如他能够做SKILL吗， OK看起来反正还是非常有意思，并且住，他就说他这个并没有ground truth就说我并没有标说这个5应该就这么这个样子，六应该这样子，它都是根据最后的训练的这个任务就说我这个分类的时候我就把这件事情给做对了，那我怎么能够更好的把这个分类来做对，通过这个训练信号来drive他长什么样子，另外一方面我的localization的网络应该把这个字符究竟做一个什么样的一个旋转，所以说，这就是他是并不是需要额外的监督信号来监督这个deformation，不然的话那样就太复杂，也就是太累了对然后，但是他的问题，就是说他还是对他是对整张featuring map，做一个变换，然后，他很复杂，是因为他在不能够用的很好。然后，highlight就说我们能够对复杂的能够有效地在concertize对这些进行建模，那我们也并不需要额外的监督信号，然后，它能够对复杂的几何形变，然后再复杂的，他们的convolution的任务上能够取得一个很好的一个效果，然后，它的核心思想就是这样的就说之前的transformation network，他work的对象或者说他变化的对象是INPUT的featuring MAP，然后，他work的对象或者变化的对象是这个sampling的这个克隆就是APPLY，featuring MAP上面的这个靠谱，对它做一个变化让她不再是一个规矩的3x3的一个形状，而是说每个sample位置的点，它是可以动的他动起来之后，对很复杂的几何形变进行一个建模，然后我先给他看一下这个动起来的效果的例子。

[就比如说这个A图是STANDARD 的convolution，对吧，我们出来的效果，当然很多层stack起来，他看见的东西最上面一个点开通下长下面这个样子，然后就说是对于这个上面这个点，他们就说我能够改变我自己采用的位置，然后下面滴每个采样点，然后他对应再往下做convolution的时候，他的采样点的位置，他也可以自己去动那这样的话，他对于不同空间位置，他根据他的INPUT的东西的不同，他可以自己去调他究竟想看见啥，这样就可以对非常复杂的几何形变进行一个有效的一个建模，不知道我说明白了吗，对，然后下面是实际的一些让的设计例子，在相对比较简单的VC的图上面，然后这个是对这个绿色点对于最上层上面一个点，红色点的是下层这这些散开的这些采样点。

当在背景上的时候，他会把自己的视野，阔得非常开，这样去看看自己究竟有没有MISS掉什么重要的foreground就是这个问题，detection的任务，我们的foreground，比如说是车和人，那这个时候当他把在一个小屋地上的时候他，他会把自己的sampling的点都集中在这个物体上，大地上也会在其中的物体上，将来更好地帮助自己去做一个识别大家对这每幅图都是想说明这样的效果，而且我们看他是怎么实现的具体实现的，就是说我们先看看数学公式吧，convolution的公式，是说给另一个位置毗邻的output 的featuring MAP上的值Yp0，他是input featuring MAP上的位置，XP0再加上一个PM，这是第N个采样点，它对应的偏移量之前在convolution里面，这个偏移量，它是固定的，比如说-1,0,1这样的，然后WPM的对应DN个材料。

 他的可以学的网络位置长什么样，对吧，卷积就长这个样子，然后，然后就说它的好处，就是说在对然后接下来就是说当刚刚已经训练的时候也是一样的就说你这个是施加在的output featuring map，上面这个YP零上面滴这个梯度他也会反弹，算传统继续翻动前面低音谱的非常慢，它也会对这边的OFFSET打法片式剃度的去发展，这不就是学这个OFFSET，所以，他的特点就是他跟PLANE的普通的靠谱性，它具有同样的阴部的OUTPUT

然后，这个是DEFAULTRP，然后对应的，刚才我说的那个操作这个他也就这些效果会好一些，类似于PART这样一些东西，这样才能更好的对，的变化以及这个Spital传送空间位置的变化进行一个接O，然后，就说那边普通的convolution比，他有有一家他取得了很大的一个进步，然后，它的引入的额外的计算量和参数量是非常小的，这样讲下后面的工作叫做DEFAULTCOMPLEXVITO，然后，就说LS就说我们对deformation的建模，取得更好的理解，然后把增强了他对几何形变建模的能力，然后，接下来就是说是我们为了更好的利用这个网络建模能力，我们，来增强网络的建模能力，我们的visualization，我们的方式，是通过一个叫做 spatial support就是说我们是直接画他的采样点的位置，究竟采了什么地方，我们通过这种方式来对他进行分析的，然后，这种分析方式再稍微复杂一点的图片上面，比如说QQ图片上面就会发现他长得并不是这个样子就说他并不是会集中在小屋里上货集中在集中在带物体的foreground上，而是说他好像散非常开，再一次破CONTEXT量的这个时候再将这篇文章发表出来后引起了一些问题和争议，大家觉得我们是在在利用context，然后就做了。

我们就说是借鉴的在在期间大家发表了一些其它的一些工作就说你网络实际你关心的区域，就拿来识别的区域是啥，这个不止跟你跟你的采样的位置有关他也跟你这个做网络的weight有关就说是不指定财务的关系，，就是在普通convolution里面，就是你最后实际的网络看的有用的区域，它只是整个你理论上你看的区域的一小部分而已对，然后我们，就是，对这个工作来说启发然后所以我们对他的visualization做一些新的一些分析，然后这个地方，然后，这个地方我就比如说去看第一个图吧，他在这三种是三种不同的方式，第一个图的这些所有的点就是其实就是我们上一次我们看看我们看见那些所有的采样点的位置，但是，这样的我们是用不同的颜色进行区分，这个时候就越红的点就是意味着他对我detection的结果影响是更越大的点，然后越蓝的点，就是他对最后结果影响是越小的的可以看到了，虽然说整体的点的位置到散布是散布的非常开的，但是对这个结果影响很大的点都是集中在物体的foreground的上面的，所以就是说我们的这个vision它最核心的点就是证明的网络确实是在，对几何形变形建模儿不是说是利用周围更多的CONTEXT是上下文信息来进行一个识别的，然后最后的一个观察，就是说是普通的convolution还是能对几何形变进行建模，因为可以学习网络的weight，所以说就算他她是这样一个矩阵的形式，但是他有些地方还是会红一点，有些地方还是会蓝一点，所以它是会能够对不同大小的物体，然后做到一个自适应的一个变化，然后，你若deformed convolution还要IP之后，网络对几何形面建模的能力，相当程度上增强，但还是有缺陷，然后，所以我们进一步的就要增强，地方COMPLEX，所以说他们能够更好地对这个几何形变进行一个建模，增强的方式，就是说在第一个非常就非常简单，非常简单粗暴，就在之前在descending one的时候，我们只在一个网络中间相对比较靠近OUTPUT的某些层apply了deformed convolution，然后，这个地方，在这篇文章里面，这个时候，他对几何形变的建模就有了进一步的一个增强，然后第二点，就是说是我们又赋予它一个额外的一个自由度就说是之前的赋予它自动就得来PK看这个看看这个公式好下面的不用管，delta PK，就是说让他去动他的OFFSET就究竟看得区域，那个sampling location在哪，现在我们家里有外资比较delta MK的就说是我们究竟看不看第K个sampling区域的这样的一个信息，它的取值范围，是一个零到一之间的一个实数，一个scalar，如果MK等于一，就退化成普通的convolution，如果当MK等于零，那这个时候，他就说我引入了记录信息，这sample位置信息，我就不看了，对，就这个就可以赋予网络更多的调整它的sampling location的这样一个自由度，就可以直接干掉某些区域。

之前的这两个方法都是为了增强就是网络对deformation的一个建模的能力。

 

 

## 3.

它具有这样的能力，然后接下来这个方法，是说是我们希望能够用好这个能力了，我们得需要适当的训练信号在网络训练的时候需要用恰当的一个训练信号来drive他来更好的利用他的能力之后，他的能力，现在不能够很好地被利用。

那我们是怎么做的，我们是叫做了RC mimicking这样的一个一个方式，这个地方我顺便我把不同的detection方法跟大家稍微讲一下，然后先讲一下吧。

这边是这个是传统的叫做RCN的这种方式，这边这个路是传统的RCN的方式来来做detection.

这个最早叫RCN的方式做object detection，他的方式，是对于这种候选框，他直接从图片中间把它的候选框把它给抠出来抠出来之后，然后把它resize到24乘24这样一个固定的尺度，然后之后，他就把整个网络apply到上面去，就整个图像识别网络apply在这些image的PAD上，然后得到一个结果，然后说最后说，说这个地方究竟是人还是还是背景就是传统的LCD的方式，然后这output说是人还是背景。

Fast RCN来做识别的方式，由名字可见他特点就是会快很多，因为这里需要把这些整个网络apply在整个图像上非常多次，然后这边，就说是我其实有很多计算量，我可以share在整个图像上面算的，不用重复算非常多遍，他的做法，就是说是对全图做一个convolution的操作，然后得到全图的一个featuring map,上面来了，然后我抽出一件Proposal，然后做R pulling，然后对每个output的feature再apply一个vision PROPOSAL每个候选框单独做的一些操作，但在这非常轻量级了，就能得到，最后识别的一个结果，好吧，然后，现在通常主流的是后面的方式。

Fast RCN，那这种，我们做的方式，就说我们在训练的时候，我们可以，利用之前的RCN的这种做法来更好地监督他feature的训练让他集中在物体上面，因为之前，RCN做法他是直接把这个框给抠出来，这时候它采用的feature是完全对于这些前景区域上的feature，FASTRCN因为他是在全图上面做convolution feature说他非常有可能不是集中在这个前景的物体上，但是，这个时候我们施加了一个lose，我们强迫RCN这边的feature跟fast这边做出来的feature他们的feature长得要像一个cos的路子还是加在路飞长的像，这个时候，我们希望他可以强迫这边的deformed convolution他的offset是什么就是说进行恰当的调整，最后，把他的目光把它的这个，attention聚焦在物体的前景上面，好吧，这个就是这个KEY Idea。

最后，就是看到一个视觉上的效果，视觉上的效果，如果你没有施加这个loose，他可能会，看着要么太多，要么太少了，而当你施加这个loose之后能更好的FOCUS在物体上面，然后最后是实验结果。

实验结果，我就饿很快的讲一下，然后这个是普通的靠谱性，如果考虑的结果，比如杯赛，我们也开始在QQ上32.1在商业版的DCN里面她能够做到38，然后在第二版里面，当它增强了这个FACEBOOK上涨到41.7。

然后当你再进一步施加这个loose就是那个，叫做就是那个FISHME的loose涨到43.1就是最后一个实验结论，最后，我们在讲下一些相关的工作，有对几何形变进行建模的非常老的一堆的工作，然后还有deep learning时代，其他的一些工作，接下来，还有一个相关的，就是说对关系进行建模，以及对attention进行建模，这些东西非常相关的一些工作，最后，还有一些就是手工的去调网络他看到这个空间区域的一些工作最后，还有就是说是NETWORK MIMIKING AND JUSTIN的这些工作都是相关的，可以看看最后结论是普通convolution并不能够很好的对 Spatial transformation进行建模，然后还有很多OPEN QUESTION就说怎么更好的去，怎么更好的去分解不同的geometry上transformation不同的factor还有很多其他的，最后非常感谢大家。

 

 

 

## 4.

当然了，实际上是并没那么简单，有两个很大的问题，第一点，是计算量，比如说在图像上面，我们其实现在网络已经做的非常简洁了吧，就已经基本都变成一种全卷积的模式，往往都意味着说能够解的计算量都已经全部share了，在整个image上，但是，就说你把这种方法很快的放在VIDEO上面，可能很多情况下，在计算量很厉害的情况下还是有点不太够，然后另外一点，就是是accuracy,往往是因为每个appearance变差了，然后所以，accuracy也会变差，比如可能来自于motion blur，就是通常在图像中间这样比较少。

还有一种叫做PART inclusion就说一部分要include还有一些是如何POS就说，VIDEO里面会有一些动物突然做奇怪动作会出一些很奇怪的POSE，没见过，然后这是我们的一些的方法，然后就是说。

尝试利用VIDEO里面最核心的概念就是FLOAT或者是motion的概念来尝试去解决，刚才说这些说到了就从image到VIDEO的几个核心的问题，核心思想，就是说是就是在VIDEO里面图像的内容的变化，他是slow的，是慢的，因为他慢了，就说他就是人不会突然凭空消失的出现，凭空消失，又出现，不要让你看着就是一个鬼片，对吧那所以的话，就说，你同一个物体在相邻帧之间玩玩只是动了一下，动一下这件事情，就可以用叫做flow field来刻画。

他在CONFERENCE里面究竟DELTA X，Delta Y究竟有多大，就能把空间上的斜度时间维度的信息给利用起来就是视频最相对于图像最关键的就是有时间维度的信息，然后，这个时间上面的变化，它是可以用motion用一个optimal flow来进行了刻画的，然后，首先我们第一个工作就是对他进行一个加速可以快很多。

第二个是说对他进行一个性能上的一个增强解决性能的问题，最后一个工作，是，把两者结合起来，既能又做到又快又好，然后首先第一工作叫Sparse-Feature Propagation，然后，这个是我们说完话看一下现在的图像识别的网络架构是怎么样的是给定一张input的图片，这时我们会用一个全卷积网络把图片的feature把他给抽出来，然后能够得到无形的一个convolutional 的FEATURE MAP。

他是不同TASK 之间share的，然后，他计算说他一般比较深比较大，然后计算起来算量非常大，在这后面，然后接一些head的，

他这个是Task specific的网络，然后它是非常浅的，然后，也计算起来非常快，然后如果我们是在每个frame上做convolution，他在整个pipeline就说，把整个图像识别网络直接搬到每一帧上面去，先过Nfeat，再过Ntask ，那这个地方我们就说我们想利用图片中间的这个S就是说他变化缓慢，这个特点来看，能不能有机会把它计算量给省下并且把精度给提高，例子，这个是一个key flame，这个是经过一个比较深，比较笨重的网络之后得到的convolutional feature map，然后把中间的某些filter response，就是把某些抽出来，他可以看到的就是把某些shallow刻画，比如可能是对车会比较敏感，某些对人会比较敏感，当前

就是说车敏感人敏感可以看这个shave，然后，他们只是挪了个位置，这个时候，如果我们是能够用一个FLOW FIELD就是用一种方法就是能够估计到之间的对应关系把这个FLOW FIELD给估计出来。然后，就是说拿这个FIOW FIELD就说他存了每个像素delta x, delta y，就是这个对应关系就可以从key film里面这个东西吧，变到这个current里面去，然后这个是work之后的FEATURE，非常可以看到他们直接从current feature抽出来里面出相比，两个是可以长得非常像的，那就是我们的，Key Idea说我们可以用这个work之后的feature来替代current feature或者是跟current FEATURE综合起来，提供互补的信息，这样也许能够让她做的又快又好，然后我们先看快的部分。

然后，就说是我们是这么做的就说我们在新的一帧我们只在key film上，我们用这个非常笨重的这个网络，我们只跟key frame做一个estimation

算对应关系的FIELD，然后，但这个队就是一个，就是我们这个时候可以得到一个delta y，这个时候我就可以把CONFIRM上得到这个feature map work到这个current frame后。

我们照样能够得到Conterframe的segmentation的一个结果，然后在训练的时候，那我们就是说我们可以sample，然后，这个时候他们的gradient，然后，要注意了，就是说这个Flow刻画的是这个KEY FRAME跟current frame的对应关系，这个东西倒是没有ground truth的，大家不会很辛苦的一帧一帧去标，这个时候我们暂时。

他最后的recognition TASK跟上午讲的一样，然后转了用这是一个bilinear duplication就在work的时候，就可以导致我的梯度是可以往下传向这个FEATURE extraction的网络也可以传向这边这个对应关系的这个FLOW的一个网络虽然也并不需要额外的需要监督信号来做这样一件事情，然后这个时候，那我们就在这个current film上，就只有这个FLOAT网络还有TASKCITY网络，而且服务网络计算量，他可以这个Nfeat的这个网络计算量会小非常。

所以我们就能够得到一个相当程度的一个加速OK，然后，这个是，computational的visual大概就说可以简化成feature extration的网络跟这个flow的网络，他们的计算量的一个比直，而通常情况下了这个网络这个适用不同的FLOATFLOAT的网络可以看到了通常情况下，就说非创新的网络是比flow的网络汇总非常非常多的，那就意味着我们用非网络能够得到一个很好的一个加速比。

这是我们的实验，我们在两个就是非常主流的VIDEO  recognition数据集上做实验，通常的比如说有这样两个很经典，semantic segmentation和object detection，然后，在C scapegoat上做实验。在visual detection上做实验，然后就有key frame的duration，然后就说我们每隔五帧取，这是基本的一些实验的一个SETTING然后这是一个对比这个，就是一个Oracle的baseline就说我们在每一帧上面，我们apply per image的recognition的网络，这样一个性能，然后这样一个run time，然后我们的方法，就说是他的加速，能够比如加速的四倍，它这上面加速五倍，每隔几帧的情况下，然后这个时候，它的性能会往下掉一点点，因为我们近似过来的feature会比原始的差一些，比如73.9到73.1造成71人，一到七十九七十九点六对大概是这样的，就是我们要证明我们比其它好比如我们可以用，已知已经train好的这种flow field的方法。

然后，来做这样work的操作相比，我们这种，find tone的时候会去处的方法比较它们的性能会掉非常非常多，主要是因为这个FLOW要apply在feature MAP层面低，所以，他跟那个pre-train好的这种flow的方式。

们是在pixel的层面上去估计，这样我们其实估计的是那种deep network的FEATURE层面的motion，他们俩是不太一样的，所以说我们必须要做翻动才能取得更好的效果，然后，最后就说是JOINTOMTOM的春景，这个是非常重要的对大概关键想说的话就是这样一些话，OK，然后接下来我们可以画tradeoff的图，横坐标是他的run time就是究竟有多快，纵坐标的是accuracy，然后，这个曲线是我们调这个key film的duration。

每十帧每二十帧，他越长的话，那我的性能自然会越掉越多，因为你的key frame跟当前费用之间可能差异会越来越大，但是，同时你的速度的加速，会越来越好。

这个东西就你自己去取去了，这个怎么去取这个flow function不同曲线代表不同取法，比如不同的网络标志的一零一五十，然后还有不同的FLOW网络对就说我们可以用最cheap的网络可以做最好，然后，

发现说越有效的这种feature性能也越好，这个是一个效果图。

接下来还有一个叫做是Multi-frame Feature Propagation是一条线上的工作，这个时候我们就要打的问题了。是来自于motion blur, video detection, part occlusion还有rare poses，这个要解决的办法，其实大家的思路都是说是我们要把相邻帧的信息给借过来，比如识别这个系这个可能人都识别不出来这玩意儿是啥对吧，那你人看到都说我前两天看见这样是个蜥蜴。跳下来了这个东西是一个东西，所以说大家的思路都是说要在相邻帧之间借信息把信息给aggregate，那之前的很多方法是在box level在最后就是你，就是已经之前，比如我在这张我抵到了。

这个是我把他帮你Bounding box，然后，然后，做这样一个识别他的问题，这类方法，然后需要很多technic，然后在之前的比赛中间用了非常多，然后我们的是一种更好的就说。

我们work的对象feature还是飞车还是在feature层面进行一个操作，在判断之前在做出框之前，我们就已经做了一个APPLICATION对，所以就是抢救的更早。我们尝试去enhance这个FEATURE，它是一种PRINCIPAL的clean的方法，然后之前study的非常少，然后这个是最核心的做法，我们举个例子，比如说这个帧上面第七层上面我们并不知道这个黑猫是什么，对吧，就说识别不出来，因为motion blur，所以说本来对于这个黑猫的这个FEATURESBOX

这个时候feature response是足够好的足够清晰的，它识别出来，这是猫我们怎么办，我们这个时候，这几个猫并不align，他的位置是变了的，所以我这样估计出来他们之间的correspond，然后flow把他给work过来，t-10,t+10帧都过来，做aggregate之后，我们在当前站我们就可以得到一个A还算比较清晰的model一个响应这时候，最后我们在PSP网络在上面再处理一下，我们就做点出来，这是一个猫儿不是检检测不出来了，好吧，就是最核心的想法，那接下来我就要衡量它的这个性能怎么样，这时候说相当于是我们定义的一个指标。

这个经典设计上的该做的划分分为慢的中等运动的快的这个分部，然后这个，就就是这个一个APPLICATION的一个STUDY，那这个是一个SINGLEFORM的杯baseline就是我们在每一帧上面用这个object detector效果，最后，我们的方法的效果，就是在这当这两行都是我的方法减掉一些东西，大家可以不用看，反正我就直接讲结果就说反包东西都用上是后面就是说刚才讲这些，然后性能能够有大概比如说有overall三个点的提升，那这个时候，在慢的东西上面提升非常小，但在快的东西上面看到这个提升是非常大的，这个方法的缺点，就是说会比single baseline要慢不少就是说他要aggregate不同帧上的信息。

然后，就是说他跟之前的那种，像传达信息说是跟之前的那种基于Box层面滴POST PROCESS的方法，他们是complementary, 他们在feature层面增强。

在box层面，把不同的BOX给aggregate起来性能还能有一个提高，然后就说，一七年的的比赛，然后当时我们没有去参加比赛做觉得精力不太够，这是这个Field里面最权威。

比赛然后最后比赛结果出来之后，很高兴地发现第一名的方法就直接accept我们的paper就说他们用我们的方法然后去打赢了这个比赛。

然后，就把这两个技术相当就相当于这种技术面分析它的优缺点，然后能不能结合起来，然后，在第一个sparse feature propagation引入key frame这样一个概念，然后，他们引入了这个TEMPERATURE aggregation的概念，然后，我们把feature quality增强了，然后用aggregation方式，然后数据量给给增强了，计算量也增加了这两个都是他们的缺点，但是我发现他们其实是互补的，然后就是其实说起来他们就是两个principle，一个是motion estimation，

那我们怎么做，都是说对之前的两个方法的一个一个总结，然后，这个黄色框，意思是key frame，然后蓝的框是蓝KEY frame，然后我们就是说，C开始就是我们在做的方法，然后C1到C2到C3是逐渐的变得越来越复杂，然后呢C1的就是我们最简单就说是我们这儿就把它变成了一个叫Recursive的aggregation for key frame就是key frame，我们还是就是跳着取，这是我们在key frame上做featuring aggregation，第二点，对于中间的，比如九帧图，这个时候，我们部分的去update featur，就是在某些空间区域，如果发现它的这个，别的地方又新出来了一个人对吧，有新出来的车，这是我们就在这一小区域重新算一下他的FEATURE其他大部分去的feature，我们都是用key frame把他给work过来就就可以了。

另外还有一个技术，之前C1C2说是每个时针取个key frame，那我们其实可以根据就说这个图像内容变化的大小而动态的决定，什么时候去取开始一个新的key frame，这个是一个例子，然后这个地方，意思是，其实本质上就是我们要重算feature的区域就说我们在其中里面就有百分之多少的区域，我们需要去重新算他的feature，如果超过一定的比例，比如20%的图像，我们需要重新去算feature而不说从key frame appropriate过来我们就重新开一个新的key frame的这开了个新的key frame这个时候下面车的姿态变化都不太大，她这时候我们就从这个key frame work到这个就好了，而在其中的部分区域，我们就是他网络觉得识别不了的，然后在这个时候他车身姿态变化非常剧烈，他这个时候他又处理不了这事我们又重新开一个新的key frame开之后，他就大部分的又可以重新不用再算了，，然后这个细节，这个里面没有讲，因为公式有点多，要看的话去读paper，然后，这个就是一个图，这边的就是第一篇PAPER就是我们能够这个这个地方的是就是single的baseline就在每一帧上面我apply一个BASELINE这个地方，这是第二篇PAPER，就说算的越来越多，但是性能会往上涨这个，是我们的方法就是不同的技术apply上去之后，会越提越高。

 

 

 

## 5．

然后接下来，就是说我们怎么把这个技术进一步推广到VIDEO MOBILE上就是说是，然后就说我们这些网络就说还是for GPU，然后，在非常轻量级，针对于手机上的网络还我们还没有设计好，然后，我们并且也不知道刚才我们说的这两个关键的Principle在非常少，计算量的时候，是不是work,开会的时候别challenge我的所以说我们回来做PAPER回答他这个问题，然后这个就是其实也要回答结果就这些，都是之前就是想办法在SINGLE FRAME上把这网络做的非常轻量级的一堆的方法，他们的SPEED Accuracy tradeoff，然后我们基于其中的一种方法，这时候把我们这个技术给apply上去。

就是利用这种针尖的信息，那这个时候，然后就是，最后我们总结一下，我们要explore motion for video的可能性，我们做feature learning instead of heuristics。

 我今天这个讲座的题目，叫做人和物体的跟踪，首先我们讲一下就是这个跟踪这件事情就是其实区分一个真正的计算机视觉的从业者和非从业者可能有一个标准就是你问他你觉得跟踪是一个很难还是很简单的问题，一般来说这个从业者都会觉得它很难，然后非从业者，就觉得他很简单，这是为什么，就是说视觉追踪这件事情，其实它是人类最基本的视觉能力之一，就看在座的可能，也许有了这个家里已经有小朋友了小孩子了，我们知道就是小孩子他在出生大概两三个月的时候，我们就会给他的那个这个婴儿床上面就买一些那种可以动的挂件，一按那个按钮，那上面那些小动物就开始转起来了，然后拨一点莫扎特的音乐之类的东西，然后小朋友其实是在出生两个月左右就具有了一种叫做故事的能力，就是在两个月左右，他就能够盯着他自己头顶上那个小玩具，转的时候，就可以随她移动，这个是一个一个人类非常自然的一个能力，那么这个能力当然是很重要，在我们的生活当中很重要，它对计算机来说也是非常重要的，它也是一个计算机视觉里面的经典问题，因为我们通过对这个物体的检测识别和进一步的跟踪，我们就可以对这个目标的行为进行一个更加，就是进行一些行为分析，其实也就是更加语义上的一些分析和理解，所以我们说这个视觉物体跟踪，是视频分析的基础模块，也是人机交互智能监控，这些应用的技术基石，但是这里，就是有一个但是，，但是就是视觉物体跟踪，对于计算机而言是一个很难的问题，所以就是今天的讲座，可能就不会有作业，没有什么，但如果如果只有一个，就回去就希望大家明白就是视觉物体跟踪很难。

为什么拿我来我来举两个例子给大家看看能让大家就是可能管中窥豹就会为什么会这样讲，在左边这个视频里面就是吧，看到有一副手套，这个这个男的这个手里，他拿了一只，比如说我要跟踪这个男士手里拿的这个手套，那么我们看一下这段视频是什么样子，好，这个时间点这个时间点发生的时间点就是就刚才他这个手套叠到另外一只手套上把它拿下来时间点是现在所有的，目标跟踪算法都会失败的地方，因为今天早上那个其实曾老师也简单的提到过，就是其实现，在计算机，他还没有非常深层的一种推理，这种能力，所以对于我们来讲，我们知道就说他把这个手套放下去的时候，他是肯定是叠在上面滴那张很快的动作把它拿起来，他拿的，也是上面的手套，所以我们要跟踪的这个手套目前在这张里面是在这个人的手里，但是计算机不知道，计算机当时他很快拿下的时候，他的那个，如果是跑各种算法的话，那跟踪框会留在这个凳子上的手套上，他走不了，这是一个问题，这是一个例子，然后右边这个例子，可能就是在这么小的分辨率下，对人也稍微有点难度了，但是我们就是比如说看了两遍，应该还是能知道就是要跟踪的物体是这这位男士右手下面有一只白色的兔子有点对人，有点难，但是其实我如果把这个视频放出来，大家可以看一下，大致能知道吧，他大概知道的对吧，但是现在几乎所有的我不用手几乎所有的计算机视觉里面滴目标跟踪算法一定会fail。

没有一个能跟他住他对这个里面的，我就已经找好，那么接下来就跟大家讲点技术的事情，就是，其实跟踪这个问题，如果我们根据他的这个应用这个场景去分类的话，我们因为很多可能我知道在座的是做一些比较应用类的东西，那么我们去根据应用场景去分一下类就是，首先，这场用的肯定是double摄像头，只是一些独立的摄像头，然后他获取的一些视频流，我们对他进行单独处理，那么这种，其实有很多的，很多的应用，包括我们的这个监控可肯定我们也是从单录的这个视频的处理开始的，还有一些家庭拍摄的一些家庭就是，娱乐的一些视频或者是其他一些电视节目，那么这些，其实都是一些单录的一些视频，然后双目的视频流，他的有一个很重要的场景就是机器人很多机器人，就是他装了两个摄像头，就是在相对于人眼的这样的两个位置，那么，他获得的视频流，其实两个左眼一个右眼一个，那对于我们跟踪有什么带来一些什么样的影响，后面会讲到还有一个就是多摄像头，象我们这个目前我们所在的这个园区未注意到就是园区里面也布置了很多的这个摄像头，那么在这些摄像头里面，就是有有多个摄像头，比如说20多个甚至上百个这样的摄像头里面如果要去追踪一个人，从这个人进入园区开始，他在园区里面随便活动到出园区为止，按道理来讲，我们是通过对于多个视频流的分析是能够知道他在园区里面里的一个完整的这样一个轨迹的这些，都是我们就是目标跟踪的一些应用场景，那么在技术分类上面，其实我们就在研究上，我们是分的很清楚的单目标或多目标，单目标，他一般我们考虑的是任意的物体或者类别，不管是动物也好，有生命的没声没生命的或者是人车这些常用的物体也好，它可以是任意的类别，然后再多目标的这个跟踪里面，通常我们会固定某一个物体类别，而目前最受关注的可能就是人和车就是，应用会最多的，那么这个其实就是我今天讲的两部分，就是跟题目其实对应的，但是我顺序稍微调换一下，就是我会先讲一下就是单目标的跟踪，普通的一般物体的跟踪，然后。

 

 

 

## 6.

在这个第二部分我会讲一下，专门是针对人的一些跟踪，那么在专门针对人的跟踪里面，就不光有单目标，有多目标，那么先开始今天第一部分，单目标物体的跟踪，今天的这个讲座里面，因为大家都不是说这个专门做这个研究的，所以，我可能不会讲的太深，我会尽量的就是说从这种直觉的这种角度跟大家讲一讲就是我们为什么要这样做，这个从直觉上都能理解的一些技术路线，然后，我可能会给大家一些关键词就是有了这些关键词以后，以后如果大家看到这些类似的这些关键词以后你会大概知道，这个就是目标跟踪里的一些关键词，那么在需要应用的时候那只要打开一个搜索引擎去搜一下，或者就是开GitHub我去输入相关的这个关键词，我可能就能download的这个code，就就可以去去弄一个掉包，简单的就可以做一个掉包侠，然后如果，如果说这个今天听的比较仔细的，也许到时候还能当一个调参侠，先讲一下这个名词，就是叫visual object tracking，直译过来，就是视觉目标跟踪，简称，叫VOT，但是，就在我们研究，这样就是这个VOT一般都是只单目标跟踪的，就比如这件事情一般是指当目标的他的问题的定义是什么，就是在一段视频里面只有第一帧我会给出这个目标物体的边框，就像我这两个例子里面左边的这个歌手，他给了这样一个筐，是怎么回事儿，就是他不是给了整个人他给了人了三分之二，所以其实就是他希望你跟踪的，就是在这个视频过程当中，是人的这个这个上半身，这个是说在第一帧里面，它会给出这个目标物体的边框，然后他的要求，就是说在接下来的每一帧里面你都能够给出这个目标物体的这个边框，右边这个例子，是一个篮球比赛，这也是比较难，就是因为在一个篮球比赛里面有五个人是穿相同的队服的，所以，而且这他们的运动，都很快，所以，再比如说人在交叉遮挡的时候，会带来一定的这个挑战，再讲技术之前，我先大概讲一下就是这个，给大家列一下就是这个，在衡量这方面，因为我们现在，一般网上都会，叫做benchmark 的dataset，有了这些标准数据集，他给我们提供一些训练和测试的一些就是比较标准的东西，让我们每做一个方法，我们就可以在这些标准数据集上去进行训练和测试，最后，这个结果，就可以进行相应的一些比较，那么在这个，VOT这个领域，比较有名的这个数据集我都列在左边，以前做的比较多的，是OTB和VOT这两个数据集然后VOT其实现在一直还都有比赛是每年都会跟ECCV，就是它会有个比赛，然后有兴趣的，都可以去参加这个竞赛，然后跟别人去比比结果，然后在这两年他各自都是有就是把来个视频量并不是很大，所以这两年就慢慢的就出现了一个，出现两个大数据集，因为原来小数据集有点趋向于饱和了分别就是叫做TrackingNet和LaSOT这两个数据集是这两年才刚刚出来的，那么这些数据集上，我怎么衡量他，那么这个衡量标准,我右边列的一个叫做VOC就非常这个可能会常见的她的他其实是一个简称area undercurved,回头，我再讲某一个方法的就是我们其中有一个方法的这个性能的时候我再给大家细的讲就这个AOC应该到底是怎么样算另外就是AOC，其实关于他的跟踪的成功率的一个指标，然后，还有他跟踪的这个准确度PRECISION一个指标，然后在VOT里面，有一个比较单独的指标EAO，组合了一个鲁棒性和一个准确性的这样一个指标，所以这些，是我们在VOT里面经常用的一些指标，那么看一下VOT这个技术发展的这个大事记，这里面其实，我画了四个大框，我觉得，就是在VOT的这个技术路线就是一路发展下来四个比较重要的这个里程碑式的这个工作，那么可以看到就是前面三个他们，他们这个出现的年代了，其实间隔还稍微拉的长一些，就是在九四年的时候有一个算法叫做KLT算法，这个算法，其实就是红火了其实最早可能在九四年之前就有，然后这个算法就是红火了可能近20年在我就是最早，读硕士的时候我就做过这个跟踪相关的那个工作，那个时候就是还是就是KLT一统天下的时候，然后一直到了2010年才出现了一个叫做相关滤波这样一个方法，这个方法，大概红火了，有五六年，这样的时间，接着，这个深度学习就出来了，深度学习一旦出来了以后，就有一部分人会在原来的相关滤波的方法上面就是再去加一些，深度的特征，就是在原本的这个correlation这个框架下面再去做，还有一些人就会去另辟蹊径去搞一些纯深度学习的一些方法，那中间，也有一些其它的一些潮流，我就没有列出来，因为我现在列了这两个，是我觉得比较主流的，目前比较主流，而且未来也会变得更加主流的这个方法，这个我我接下来就会一点一点的来给大家做一个介绍，那么先看一下这个KLT的话，可能就是做跟踪的人在可能在过去过去20年间，凡是计算机视觉毕业的这个专业毕业的硕士或者博士生，那肯定都是会知道这件事情，这个三个字母，没啥意义，就是三个人，这个字母的首字母的缩写，它的历史，是这样子的，就是在八一年的时候，八一年就有两个人就是LUCAS和K这两个人应该都是CMU的，它们俩，就是搞了一个算法，这个算法，其实是什么，是为了做双目视觉，他搞一个算法就在做双目视觉的时候，我们左眼和右眼会拍摄到就是左眼图像和右眼图像，那左眼图像和右眼图像，他们有一个什么样的对应关系，这个问题，当时叫就是因为这个image registration的problem，图片里面这一个小点对应于右眼图片里面到底哪个点，这个对应关系所以他们就搞了这样一个算法就是他们俩搞一个算法，然后，九一年的时候，这个卡内地，就连和另外一个人，托马斯他们俩一起，去搞了一个就是原始的这个，就是检测和跟踪的这个算法，那为什么我刚才那个图里面列了一个九四年，就是九四年的时候，他们又有一篇比较重要的PAPER，我估计这个引用率也可能好几千了，这个PAPER，就是定义了一件什么事情，叫做GOOD FEATURE To TRACK。

那今天早上其实曾老师也也曾经讲过，就给两张图片到底左边这个点跟右边那个点是有关系的，那我把它应用到比如说一个视频流里面就是前一帧和后一帧，哪两个点是是这个一模一样的点，那么九四年这个工作，就是说如果我要跟另一个物体的话我就首先在这个物体里面去找到底什么样的特征点是一个比较容易对应的特征点，那其实我们根据人自己的这个感觉来讲，就是说他这个比较容易对应的点可能就是一些角点，就是有一个角或者是有一个边，这些点可能会是比较容易对应的，所以我们来看一下这个例子，这个例子，就是比如说我一开始第一帧给了这个框要跟踪物体是这个人的人脸，就是左边这个图里面黄色的框框出来的这个部分首先绿色的一些小加号，就是九四年的那个PAPER提出来的，我怎么样去找一个比较好的FEATURE，这就是所谓的GOOD feature，我们可以看到他在就是这个人的发际线还有眼睛，嘴巴，鼻角这些位置就是这些，他的梯度变化比较大的这些部分吧，或者是这个边缘比较多的这个部分，他就会采更多的这些点，然后这些点，他一个一个地，去跟右边的这个新的这个图片去做比对，到底哪个点对应于哪个点，然后我们可以看到在右边这个图片里，白色的这个点对上的那些比左边绿点要少有些对上，有些没有对上对鱼背上的那些点，比如说100个点什么，对上了80个点，那么我依然，就可以大致的判断，这个人脸到了一个什么样的位置，他可能会有一些角度的旋转，对吧，然后，它尺寸可能会变得小一点，这个就是最早的一个KLT算法就听上其实非常符合直觉，所以也是备用很多年就一直到现在，就是据我了解，微软的有些产品里面，可能对于这个跟踪要求不高的依然在用这个算法，他的确非常好用，那么接下来就是2010年的这个所谓的这个叫做相关滤波这个算法，就是不知道大家对滤波器有什么了解，因为今天早上其实曾老师也讲了一些，这个关于滤波器的这个事情，其实这个想法就是很简单，就是说所谓的Filter其实就可以把它理解成一个模板，那么进来了第一帧的图，我已经刚才已经讲了就是跟踪的，这个任务其实就在第一帧里面我给你把这个物体框出来化出来以后，我就生成一个模板吗，看看这里有点小就是这样，这个东西其实就是当前生成的模板。

他可能有点糊，但是，他会把一些主要的一些轮廓把它就是提取出来了，意思就是我这个地方是有些轮廓的，然后这个模板，到下一帧我就跟下一帧去比，去匹配，等于是向华创一样的这样的是匹配，匹配以后响应最高的那个地方就是就会得到一个所谓的响应图或者是热力图在这个热力图里面这个白色的点就代表响应最高，黑色的点就代表这个没有响应，没有MATCH上，所以这个相关滤波的这个方法其实什么就在第一帧的时候我根据第一帧的这个图片我生成一个Filter，这个Filter，其实就是个模板，然后到了第二帧的时候我拿这个模板去匹配，匹配完以后，得到了一个Heatmap这个heatmap上面响应最高的点就是我这个物体新的位置，然后有了这个物体新的位置以后那第二天这个物体肯定这个形态有些变化吗，我就用这个形态再去更新我的模板，所以根据这样一步一步的这个更新下来，比如说这个100帧过去了，我这个模板，就已经是综合了前面100针这个物体所有可能的这个形态，所以这个模板可能就会越来越变得比较有通用性，那，如果拿一个人来举例子的话，比如说第一帧这个人是站着吧，那他只认识站着的人，然后第二章，这个人慢慢的弯下腰，他这个基本上还是能匹配上，同时我知道这个人是可以弯下腰，可能到十几人，这个人慢慢蹲下去了，我就知道这个物体是可以蹲下来的，所以根据就是等于是说我边跟踪边去学这个人的形态，而且学到的这个东西，其实都把它融合在一个叫做所谓的filter，这个Filter，跟图片所做的那个操作，我们叫做相关操作亏损就操作，所以这个方法就叫做相关滤波的方法简称，就是一个CF的方法，那么最早的这个基于CF这个TRACK，是叫做MOSSE是一零年提出来的，那么为什么他比较有名，就以前，其实这个滤波器已经提出很多年，为什么他比较有名，这个原因，就是首先他用第一帧就可以做一个滤波器的初始化，这是以前不太能做到的，以前的就是我要为了训练一个滤波器我至少可能需要个十来帧，我才能训练出一个滤波器，而这个就是他设计的一个方法就是只要第一帧就可以对这个滤波器进行一个初始化，那么另外一件事情，就是说这个相关这个操作，他可以在傅里叶，就是在屏幕上面去进行操作，它的好处，就是说我把一个卷积的操作变成了一个乘法操作，所以这样子，就大大提高他的效率这个滤波器在CPU上面可以做到669个秒，也就是说我们如果处理一个30帧每秒的一个普通的这个视频流一个并不是非常高端的一个CPU就可以同时处理20多路，所以这个是当时他出来以后就是引起比较大轰动的，这样的一个一个原因，那我这个，再给大家稍微稍微讲一点点，就是我这里展示的这个三个图，第一个图，其实是一个图像，第二个图，就是所谓的滤波器，然后第三个图，就是它的那个响应的热力图，所以每次就是第一个用第二个那个滤波器跟第一个图去做一个匹配，匹配到的结果就是，其实滤波的那个公式里的那个G就是第三个图就是那个热力图然后在这个方法里面，就是跟踪和滤波器的这个训练，它是迭代进行的，每一帧，我用原先的滤波器进行一次跟踪，跟踪的结果也要拿来去迭代我的这个滤波器一步一步地上去迭代，然后就是因为我们可以把这个操作滤波器的这个训练的也放到傅里叶domain，所以这个迭代，也可以很快的完成，所以他就是整体的都很快，然后接下来，我这个只是给大家列名字，就是有兴趣，你就记着几个名字就好了，在这个最早的这个相关滤波的这个方法提出以后大家，就是对他就做了各种各样的这个改进阿一个改进，这个所谓的KCF，就是一个和华的，然后第二个是一个SR的DCF就是叫做SPECIALLY Regualized的这个效果也是非常好，然后第三个DSST，是一个进行这个尺寸能够进行更加准，准确的这个尺寸估计的一个基于这个CF的一个方法，那么这些东西，就是在2010年一直到这个2016年这个这个期间就是有很多，可能会有20多个，大家觉的上来名字的方法，慢慢的这个发展起来了，接着，就是，深度学习出来了，其实之前有一些这个想法是说我完全就是像做那个目标检测一样去做这个跟踪用一些这个深度网络去做一些跟踪那些方法，红火过一阵就是有那么一两个比较有名的代表作的方法，但是我觉得，因为那些方法，它的缺点就是太慢，基本上可能只能做到在GPU上只能做到一帧每秒，所以大家想像一下就是一个30帧每秒的视频刘如果我去拿GPU去处理的话，我要30块GPU做到实时所以没这个实用性就是太差了，所以这个我觉得不太会是主流对我我的理解，就是跟踪算法，你最低要求你得处理视频流你的做法实时对吧，要不然可能我们也就没有什么太大的用处了，那么这个CMSFC的这个框架，我觉得是一个就是非常伟大的在VOC里面一个非常伟大的这样一个框架，这个框架，其实你如果去看这个图的话，其实跟那个相关滤波是一脉相承的就它其实也是有一个相关操作，在我这个图里面，上面那个Z就是模板就有点相当于我之前的一个Filter，下面这个X，其实就是一个所谓的叫做search region就是我搜索的区域，这个search region其实很好定义就比如说我上一帧这个物体在这个坐标XY这个坐标位置的话，那下一帧我就把这个物体所在的这个区域，稍微扩大一点通常就是扩大两倍，然后这个就是我的搜索区域，这个其实就是基于我们对于视频的一个比较基本的假设，就是一个物体，在视频里面相邻帧之间，他不会移动太快，通常他不会说这个乾坤大挪移对吧，一开始在左边一会到右边去，所以，就是我们可以把我们的，搜索区域局限在上一争这个物体出现了这个这个周围然后，中间这个紫色的这个话就发的这个地方，你画的很小，其实他是一个稍微庞大一点的一个深度网络，这个网络，他干的一件事情就是去提取其特征提出来的特征上面这个六乘六乘128，这128，是一个CHANNEL维度的就是它有多少通道，这个不用管它我们看一下六乘六六乘六，是在空间尺度上的就是原本一个127乘以一二七的。

乘以三，就是那个RGB，就是早上讲过的那个三个色彩空间，然后127x127，是一个空间的分辨率，变成了一个六乘六的一个特征的这样一个小特征图，然后底下那个search region，从255乘以255，这样的一个尺寸，把他提到了一个22乘以22的这样一个小尺寸，接着就是做相关滤波，这里给大家做一个简单的计算题，就是上面滤波器是一个六乘六的小框，下面这个搜索区域是一个22乘以22的框，那我如果做华创这个一一去做这个相关的话，最后得出来的，这个热力图是什么尺寸的，这个尺寸就是实际乘以17就是22减六加一。

简单的算数，就是最后我们会得到这样一个17乘以17的这样一个热力图，那么在这个热力图上我划了两个点，这个红色的点，其实就对应了，在搜索区域里面就是这个这个大红色的，这样的一个位置，蓝色点，就对于搜索区域里面淡蓝色这个筐这样的一个位置，所以我们在最后的这个热力图里面，比如说红色那个点的位置是响应最高的，那我就判断就是这个物体Z这个物体就移动到了，这个搜索区域内淡粉色的这个位置，所以这个就是这个方法的一个比较直觉上的一个很简单的一个想法是不是就跟我们刚才讲的这个，滤波器相关滤波器，其实很像的那这里面这个，深度学习，它起的作用是什么，深度学习，就是这个紫色的的这个框他做了这个特征的提取，然后，深度学习，为什么这个深度的特征会比较好，他比较强大的一个地方，就是就是说它省去了一个步骤，就刚才我们再讲相关滤波的时候，我讲过就是每一针，我跟踪完了以后我要对这个滤波器，做一个更新，做一个迭代，而在这个CF的框架里面，它完全拿掉了迭代这个步骤什么意思，就是我只拿第一帧第一帧的图片提取出来的这个特征，我就能给后面所有的用了，就像我刚才讲的一个例子，如果第一帧这个人是站立的，我从站立的这个人提取的特征，我们一直给后面的帧用哪怕这个人最后蹲下来甚至躺下来，按道理来讲这个深度特征应该都能够有足够的能力，能够把它检测出来能够把它表达出来，能够把他这个模版匹配上，所以这个，是这个框架能够做的非常快的，这个原因是因为他不需要再去做这个迭代的这个模版的就是模板的更新，所以他就是虽然需要GPU，但是在GPU上，他可以做到86帧每秒，所以是超过的，我对这个框架，因为它很重要，然后其实现在有很多他的这个代码，如果有兴趣的话可以去下载一下去试一试那么，我大概讲一下就是他训练的时候是怎么训练，一个是训练，一个是，测试inference训练的时候，她用了一些视频的数据库，比如说一imageNet的VID和bounding BOX，这两个，是视频的数据库就是，的确是这个物体在一帧一帧都是有变化的后面两个，是，试图向数据库，因为真的DIRECTION的dataset和COCO，他们都是图像数据库，但是我们通过一些图像增强的方法也能给他用来做这个，视觉目标跟踪的这个训练，在我们做这个信息提取的时候特征提取的时候，我们用的这个，这个主网络其实是一个非常简单的，然后用了这个LOSS了就是老师这个老师可能别的课里面也曾经讲过这里面就不细说了，我给出了一些三个例子，就是他在做训练的时候，他的这个输入图像都是成对给的，我刚才图里面讲的上面那个，就是我的模板127x127,下面这个，就是我的这个搜索区域是255x255，然后，他在训练的时候，他就把这个目标，都放在这个search region的这个中央所以，我要训练我所期望的结果就是所谓的ground truth的。

 

 

## 7.

结果是右边我显示的就是这个输出的热力图在这个热力图里面我们刚才说了，是一个17乘以17的一张图，他只有最中心，我标黑的这13个位置，希望他是完全匹配上的而边上的这些，都希望它的值是零，所以这样我通过一对这个数据的这个训练，我只要输入一对这个，这个图像的图像块我就可以，其实我相当于训练17乘以17个结果，其中有13个一剩下的全都是零，这是我的训练，然后，这个是他在做inference的时候就真的来了一个视频，我现在要进行跟踪了，那我怎么搞，我就从第一帧里面我先因为已经有了这个物体的目标物体的这个框吗，我就在这个目标物体框的周围先截下来，一个图截下来，一个叫做patch的一个一个图,然后把这个图片，把他这个从新叫Resize就是重新把它尺寸的变化成127乘以127，然后接着在之后来了每一帧我都根据上一帧物体的位置，以那个上一帧物体的位置为中心去切一个255乘以255的这样一个图片其实是放大两倍，然后从新缩放到155乘255这个是这样一个概念，我在测试的时候，因为我不知道这个尺寸变了，因为我在视频里面，有时候你，这个物体，不管是说因为拍视频的时候物体离你的镜头更近，还是说我们有一些zoom in的这个操作物体，这个尺寸可能会有一些变化的，所以，在这个网络在进行测试真正进行跟踪的时候，他会测试三个不同的尺寸就是会把我这个搜索区域，一就是代表就是原原始大小，大于一，1.375这个反正是后来搜出来的一个超参了就是会把它稍微放大一点点，或者把它缩小一点点，然后再去比较，所以其实在进行比较完了以后，它其实不是得到一张热力图他会得到三张热力图分别是三个尺寸的热力图，那么在这个三个尺寸的热力图上我去找到响应最高的那个点，那我就不仅确定了目标的位置，而且我还确定了这个目标的尺寸的变化，他到底是变大了变小了，还是没有变，那么底下有一个叫做cos window的，这个其实是一个就是一个后处理的东西，他也是基于一个什么假设了就说这个物体的在相邻帧之间的这个变化不应该太大，所以，如果说我有两个非常相似的物体，一个物体，出现在，前一帧我要跟踪物体的那个位置，还有一个就出现了边上，那肯定是说就是原始位置中心位置的这个物体是我要跟踪的那个物体的可能性会更大一些，所以，他这个其实是一个后处理，其实就是说倾向于去找在原来这个位置不不怎么动的那个目标，但是，在这个SiamFC的框架里面，他们那个尺寸是没有变化的，所以我用个非常简单的一个例子，来讲一讲就是这个我们在跟踪里面遇到一些CHALLENGE怎么解决了就比如说我们就是在标题那个上面，比如说是我的目标物体，我要跟踪的是一朵云，那这个云，他可能是后面他有个太阳画的不太好的，应该能理解这个人的视觉还是很厉害的就是后面有太阳，但其实我工作的目标是这朵云，然后当这个云，就是稍微缩小了一点，比如说我镜头拉远了这个云稍缩小一点，这个叫做skill change就尺寸变化的时候，其实通过一个非常笨的方法就是我，我测试三种不同的尺寸，其实我基本上是解决了但是，如果这个云一开始是个白云，后来变成一朵乌云，就是，虽然它还是一本云，但是它变成乌云了，而且开始下雨了，这个时候，它的颜色可能发生了变化或者，他的这个形态也稍微发生变化，这个原本的框架，尽管他希望解决但是事实证明，他解决的并不好，后面一个我换了一个叫做orientation change就是这个角度变了，对吧，就是，方向感方向变了，那么我们知道其实计算机是很傻的，如果是他方向变了，以后你傻傻的用变了一个方向的去跟原来那个，直立的那个模板去匹配的话，其实你是匹配不上的他那个分数会变低的，所以这种情况就即使是这个关系完全没变，我只是旋转了30度，那这个跟踪器有可能也会fail了。

所以这个orientation change也是没有解决的，还有一个，就是我最后讲了就是说好这个云，他不光是大小的尺寸变了，而且长宽比变了，接着，她前面又漂亮又飘来了一朵云，不管上面没有写字，对他有遮挡，然后，你发现原本的太阳，是在她左边的现在在右边，周围的情况也都变了，这个时候，反正这个原本就是SiamFC的框架都解决不了那根据这些CHALLENGE，其实底下我写了这这几个关键词，都是我们组就是这两年做了一些工作分别去试图去解决这些问题，那接下来我就去讲一下就是我们这几个是怎么样去解决它的这个问题的,第一个，讲了这个叫做SAM。

我先讲一个这个例子，这个例子,在这个例子里面最左边那张图就是我跟他是这个人的人脸，然后绿色的框，就是真正的这个人的人脸后面那几帧，就是这个视频到后面到后面这个几帧，红色的框就是我刚才讲的的结果，你们可以看到这个人明显他那个脑袋在那儿，但是他从左侧脸变成右侧脸的时候，跟不上了，然后跟到了书架上的一个位置，所以也就说对于原本的框架来说，我们看右边儿是一个示意图就是这个书架红色的筐跟原本这个模板的这个距离他觉得是更小的，而这个人这个脑袋这个方向一遍，他觉得这个距离更大了，其实你看就知道为什么，因为那个人，他连左的时候，他那个黑色的头发都是在那个右上角吗，黑乎乎的，然后其实那个书架也是黑乎乎的，所以这这个很简单，就把这个计算机给欺骗了，所以，这说明什么，就是不太知道这个语意信息如果说我知道我跟踪的是一个人的脑，那我肯定就不会偏到书架上去，那暑假肯定不是一个脑袋吗，对吧，所以我们就发现了就是原本的的框架在他的训练方式下他有点太看重一些我们叫做low-level的feature所谓的low level，就比如说这个像素的灰度值，比如说他的那个边缘信息，这些他没有一些比较强的语义信息，还有第二件事情，就是你看，我们要跟踪的，这是一个百米比赛奥运会，我们绿色的筐，是博尔特。那么我们把这个框的框稍微放大一点，大家可以看到试试这个框这个就是我们当时的一个一个要跟踪的东西，结果SiamFC，他跟踪成啥了呢,然后他还他还是停在这一开始也没人理解不了我的人都跑了，你为什么还留在那地方，但是其实你要从计算机的角度你去看一下你去看一下这两张图，你就会发现，其实他很相似，对吧，因为左右两侧他有这个黄色的很显著的一个标志牌，所以对计算机视觉来讲，这个中间这个人好像他觉得不是很重要，所以他就在这就是犯了一个错误，所以这两个错误，就是总结起来，其实他都是说在还没有什么语音信息，我给了你这样一个图，我都已经跟你讲了我要跟踪问题肯定是在中间，但是去看边上的东西，所以根据这个，我们就设计了一个叫做双录的双胞胎网络，我刚才忘记解释这个词这个词是不是很常见，那为什么叫双胞胎，就是对于我的上面滴这个他给的这个目标我用的这个网络和对于底下搜索区域所使用的这个网络网络参数是完全一样的就用一模一样的网络去提取特征，这个很好理解吗，用一样的网络去提取特征，这样的特征才有可比性吗，那么以前，其实就是这个蓝色的这这个线蓝色的框和蓝色的这个虚线，他去做这个网络的训练，还有测试，他得到的这个右边我指的这个就是他得到了一个热力图那现在，你们可以看到就是在这个热力图里面其实他响应最高的不光是在中间就两边他其实也有很高的响应，而我们现在的就真增加了一个旁路，这个旁路，其实是你讲小trick也行，就是我们用了一个在，在image net上面pre-train的一个网络就是他原本那个网络它是为什么做训练的，他是为了图像分类问题进行训练的，所以你们想一下就是针对图像分类问题进行训练的，这个网络一个人做到的那个特征，总是会比较相似的，因为他们都会要最后要分类成人，所以这个网络是以在这样一个任务下训练，它就会训练的适合这个任务，对吧，那么在这样的任务下训练一个穿黄衣服的人和一个黄色的墩子，他们在特征空间上肯定是不相似的，所以我们就利用了这一点，所以，我们就用image net上面pre-train过的这个网络，我们就把它固定下来，就用那个网络去提取一些特征提取的特征以后，它所产生的热力图他其实就能很好的去区分到底是一个人还是一个柱子，然后再把这两个热力图进行一个综合那就能够得到一个正确的结果，在这里面，就是还用了一点叫做attention这个机制了，我这个可能就不细讲了吧，他的这个主要的意思就是说我想要就是有些信道，他们在周围就这个图片周围响应很高，虽然他值很高，但是他响应高的地方，不是我的目标所在的区域，那对于这些CHANNEL来说，我就应该给他一个比较低的一个权重，其实这个这个网络的想法就是这样，他就是给定一个CHINA，然后我生成这个CHANNEL的权重最终就是会如果是这。

图片中间的这个响应很高，那么这个权重的相应的就会比较高，这样我就让我这个网络能够更加的专注于中间的这个区域好现在就讲一下这刚才讲了个什么事AUC，就在这个图里面，AUC就是我，这个中括号里面的这个值这是什么意思，这个图叫做一个SUCCESS RATE这个横坐标就是我，我这个成功率某个视频的这个成功率，然后这个中坐标，就是、、就是他的这个比例，然后我每一个TRACKER我都可以在这个上面画出一条线，接着，你可以看到就是这两边的这个坐标轴都是零到一吗，所以整个这个方块的这个面积，是一而我这个这根线左下方的这个区域的面积就可以算出一个值。

然后这个只能就是所谓的AOC，那么我们可以看到就是这个AOC当然是1，是最高是最好最好，我们这条线是这样子，那就所有的就是跟踪对的，但一般来说，这个曲线都会都会是这样的，这样的一个形状，然后右边，是一个经度的一个尺寸横坐标，就不再是成功率了横坐标，其实就是说我中心偏差的像素值就是我认为的目标所在的位置和真正目标所在的位置，他们两个中心点的像素值，然后纵坐标，一样的是他的一些是他的比例，所以这个是另外一个指标就是精确度的这样一个指标，那么看一下这个结果就是，着重有点小，等一下会有一个视频的DEMO，在我三个工作讲完的时候会有一个视频的DEMO，但是我要稍微提一下就是这里是一个滑雪的像是一个像高台滑雪的这样一个视频。

一个人从右边出来，然后，他在这个空中进行了一些翻腾，然后翻腾，这个角度，是发生变化的，当时在我们这个工作提出之前，几乎其他的所有的TRACKER，都会失败，然后这里，是一个像摩托车特技一样，这个摩托车在这个像U型池，这样里面它飞起来非常空中的进行一个旋转，那一带有这个旋转以后，就是之前的一些方法就肯定就不行了，因为他模版匹配的时候，他也搞不清楚这个方向一匹配匹配不上就就丢掉了，而我们，因为就是获得了更多的语意信息就是对我们来讲那个上半只给出的这个特征永远都是说，是一个骑摩托车的人，那么这样的话，他的这个匹配，就会做得更好，接下来讲一下就是关于orientation问题，先看一个例子，这个是一个在培养皿当中的蚂蚁。

这个视频，就是说这个这个蚂蚁，一开始的最左边就是他第一帧就是给出的那个要跟踪的这个物体的这个第一帧这个蚂蚁，在培养皿里面爬他这个角度，当然就会就会随便变了，所以原来的那个方法，他其实是不会进行这个角度旋转的，所以，她一直是用第一帧的那个角度去做匹配，所以就是即使是这么简单的一个CASE他最后可能也会跟丢在根据之前，他那个框也是画的就是不太合适，所以我们就提出了一个，就是可以适应他这个方向变化的，这样的一个方法简单看一下这个图，就是我们会做一个角度的匹配，这个就是我的目标，目标的图像，接下来，再下一站里面首先就是这个是直接切下来的样子，她有一定角度的旋转，所以现在，原来里面他要测试三个尺寸对吧，然后现在如果我们想要测试测试多个角度的话，它的难点其实在于，就是比如说我要测试三个尺寸又要测试三个角度，那么他的总共的可能性其实三乘三，再减一，就是我要我要测试很多这个可能性就是我尺寸又变了角度又变了，那我们这个方案，就是说在每一帧跟踪的时候尺寸和角度，我只变其中之一，中间这个是尺寸没有变，一代表尺寸没有变，零代表我的角度没有变，右边这个，右边两个图，都是尺寸没变角度，分别是正八分之派和负八分之派，左边的这两个，这两个图像块，是角度没有变，但只是这个尺寸进行了一些放大和缩小，然后这五个图一起进这个双胞胎网络，然后做相关滤波得到了五张热力图，其实你们如果仔细看这个热力图的话，你们就会看到就是这个右下角这个热力图里面最高点，其他响应是会最大的就它最亮绿色的会亮，所以，就是通过这样一次性的进五张图出五张五张这个HEAT MAP，我们就可以判断说，原来这个尺寸是没有什么变化，但是角度，变了负的八分之派，所以我们这个方案，是能够很好地跟着这个物体的这个角度变化在动，看下这个结果吧，就是说Siam-FC当时的AOC的指标，是0.607，我们有时候用百分比是60.7，我们的第一个工作Siam是把他提了很大的提到了67.7，然后就是又进一步的把他到68.6，这个都是在当时提出的时候都是在，学术界应该是最好的，这个方法，我们可以看到用了这个角度的预测以后，一个是中间这个蚂蚁培养皿里面蚂蚁我们这个就可以更很好了，然后在这个下面这个这个视频里面就这个人这个方向乱转的这个视频里面，其实我们基本上是达到了我们原来的目的，看下这张图吧，就是这张绿色的是标注的结果，红色的是我们跑出来的结果，虽然说我们红色并没有完全契合这个蓝色完全跑出来，这个就是这个绿色完全跑出来这个ground truth结果，但是我们对这个他达到效果还是很满意的，为什么，你可以看一下就是我们框出来这个红色框，其实他是非常贴合人的这个角度的就是在我们第一帧里面其实就是说他的上半身是这个人蓝衣服，然后在我们这里跟踪出来的框里面也是他上半截是这个蓝色的衣服，但是因为Siam-FC这个框架他的，它的外接框的尺寸长宽比不能变化，所以就导致了我们，就是这里框了一部分背景区域，导致这个效果就看上去就不是那么好了，如果要让我们能够框到这个真正的绿色的就是这个groundtruth这个区域，我们必须要让他的这个长宽比应该能变，所以，这个就是之后所提出的框架就是其实是有一个带region PROPOSAL的这样的一个结构。

那我们做了一个工作，就是在这种结构下面去做的，一个工作叫做SPM TRACKER看一下这两个比较难的，这个例子，其实我们现在就TRACKER比较骄傲的说，我们已经做得比较好了，就是第一个是撑杆跳高，撑杆跳高的，其实他这个变化也是很大,人飞起来，然后第二个，就更夸张了，第二个是变形金刚，这应该是一个机器人，后来变成了一辆车，我们偶尔能跟上我必须要说就是有些时候参数调的好的时候他能跟上参数调不好的时候还是跟不上，因为他语义的确是变化的有点大，后面我给出了一些就是其中的一些截屏吧，就是在一开始这个算撑竿跳高这个是细长的他跳到空中的这个头朝下脚朝上，然后这个框，其实是一个接近正方形的框到了最后一帧，他落下来的时候这块又变成了一个扁的框，所以我如果是用原来那个框架用一样的长宽比的尺寸去匡的话肯定是框不好的，然后变形金刚也是这样一个类似的这样一个问题，所以我就希望这个网络有一个什么能力，就是我不仅要追到这个物体，我还能够非常好地把这个物体的外接框给他标注出来生成出来，然后另外一件事情，就是说帮我这个帮我这个视频有非常多的同类问题的时候，我怎么样能够正确地区分哪个是我要跟的像尤其是第一个美式美式足球是橄榄球的这个这个图片里面大家戴头盔都是一样的，到底哪个是我要跟踪的，原来的方法可能会fail，所以总结一下，然后下面那个跑步的也是这样，因为他同一个国家的人穿的是相同的运动服，那稍微总结一下那就是说其实我们跟踪，就是两个两个非常重要的一个要求，第一个要求就是我这个物体，只要还是我这个物体，不管他怎么变，形变也好，光线变化也好被遮挡也好，穿了马甲也好，脱了马夹也好，我都能够把他认出来这个就是我的要求，第二个要求就是我的区分度的要求就是不管周围的人多么像目标周围的东西多么向这个目标我还能够区分出这个目标还是这个目标，所以其实我们仔细想一想就是其实这两个要求其实是有点对立的吧就说第一个要求，我让你不管怎么变，我都能还把你这个认回来，那就说明我这个可能我这个模板要搞得比较宽松吗，对吧，随便一种都行，但是第二个要求又很严格，你稍微变一点，有一点细微差别，我都要区分出来，所以这个其实这两个要求，是有点有点对立的，那么为了解决这两个比较有点对立这个事情，所以我们就提出了我们在这个网络里面我就分两步走，这个我画中间这个这个框图，这个有点太复杂了，我们看看右上角那个就是一个简化版，就是我图像进来以后，我经过一个卷积神经网络我先做一个比较粗略的比较，粗略比较的结果是什么，我们开发这个大图里的这个例子，比如说我要找的是博尔特，他首先是一个人对吧，那么在这个进出率比较的时候，我就能我把。

 

 

## 8.

这些人都给他捞出来，这些都是跟我是同一类的物体，我都把捞出来不光包出来，而且我把他的准确的拼边匡都匡出来，接着这三个筐，我在进入到第二步就是叫做精细比较精细，比较的时候，我就只专注细节，这个时候我就知道，中间这个是我的目标，周围这两个，不是为了干这件事情，那我们要怎么做，就是训练的时候，尤其是在这个粗略比较的，这个部分这个训练，我们要耍一点花样，这个花样是什么，比如说我原来训练的时候，我要跟踪一批斑马，这个斑马是就是头比较靠前的那那匹白马，那在以前的训练过程当中这个斑马也就是绿色的这个区域是作为证样本的就这个的确是我要找的东西，但其实对计算机来他怎么知道这个不是你要找的办法，你蹭的给两两张图，他其实是不知道的，所以在我们这个方案里面，就是在第一部粗略比较的时候，我们把这个绿色区域和红色区域都当作这样的，因为他都是斑马都当做证样本儿在第二个比较的时候，，所以这个就是体现了在就是粗略粗略比较和精细比较这两个阶段，我们要使用不同的训练方式，这样的就可以在物体变化很大的时候，这个撑竿跳和变形金刚，这是两种以前的这个方案他的响应都比较小了，而我们这个方案，还能形成一个比较大的响应物体形变比较大的时候能够形成一个比较大的响应，那我们要讲了那如果说周围有相似物体，我们来看一看，以前的这些方案他都是只给一个分数的，对于每个物体他只给一个分数，比如说我要跟踪老师这个阿黑裤子的这个人，那以前的方案，比如说黑户这个人得了一分边上，这个白衣服的人，得了0.85分，他的问题在于什么，就是比如说这黑裤子的人正好经过一个垃圾桶，然后被遮挡一点，他这个分数，比如说降到0.8的时候好，那这个白衣服的人，0.85的分数更高，可能就会误把它当作一个目标了，而我们最右边这个是我们这个方案我们对于每一个框会有两个分数，一个分数，是粗比较的分数，我们可以看到，就是不管是黑裤子还是白衣服，他们两个粗略比较的分数都挺高的，一个是1.01个0.96都挺高的，但是你再去看精细比较的分数，我们就把它差距拉得很大了，精细比较的时候，黑裤子的人是098而把衣服的只有0.05所以这个时候即使黑衣服的人稍微瘦了一点遮挡，或者说他。

[3:0.002,4:0.004]  稍微有一些形变他这个分数，比如说，变成了0.8依然还是比白衣服的人要高很多，所以这样的保证，我们能够留在这个白衣服的人，这个身上，接下来就给大家，看一段视频吧，就是我们的这个效果，这是一个迅速跳动的视频，我先讲一下就是在这个里面白色那个框叫ground truth，这都是人标的，然后大红色的框是我们的结果，而其他的颜色都是都是其他的一些方法那我先回到最早的，因为这个，因为它这个动作比较快，它飘到跳过去了，所以很多方法就框就留在原地了，就完全没跟住，然后这个就是我刚才讲的那个摩托车这个跟上的这个框红色的框，绿色的筐的都是我们的方法，白色框是那个人标的这个是撑杆跳撑杆跳很多，你看他就呆在原地了，所以就几乎是只有我的方法能够把它跟住这是一个就是滑雪的其他月尺寸的变化，又有旋转变化，这个是电影里的这个人的打斗的动作又很暗，这是一个跳水，你看这跳的时候就已经好多框就没了，就完全跟不上了，对这个我们还是能够跟住的，所以这个是刚才讲的就是关于这个，单目标跟踪接下来，就是还有大概15分钟的时间，我会讲一下就是关于人类这个跟踪那在人类跟踪里面，其实就是稍微有点杂，因为，在这个学术界可能会映射到这样四个找相关的就是学术的问题，那我就是会跟大家稍微过一下就我不会去讲特别主要的这个细节的问题，就是有四个非常相关的学术问题，第一个问题叫multiple object tracking，简称MOT叫做多目标跟踪这个问题的定义是什么，其实他很有意思，他是说在一个视频里面每一帧我都已经有了检测结果，所以他这个所谓的跟踪，跟我们之前那个不一样，之前那个只有第一帧给了一个框后面那些我完全没给那个那个框架你自己确定的，而这个多目标跟踪，他是假设我有一个很好的检测器能够把我感兴趣的物体都已经检测出来了，他要干的事情只不过是说比如说我第一帧框出了三个框，第二帧框图五个框，我把它对应起来第一帧的三个框分别对应第二帧里面五个框。

所以他干这件事情在这里面有一个非常简单的一个经典算法就是五五年的一个算法叫做匈牙利算法，他干的事情就是一个对应的关系，就是根据框和框之间的这个距离我来判断就是我到底哪个框对应到哪个框，我的这个所谓的COST会更小，比如说这个我举的这个例子，其实他最早的来源，比如说有三个工人，然后，我有三个任务，这三个工人分别做这三个任务，他会收费不同，那我要怎么样分配可以使得我花最少的钱能够把这三件事情都干了，那其实跟我的这个association这个问题其实是本质上是一样的吗，就是我怎么样能够让第一帧的三个框跟第二张的，比如说三个或者五个框进行对应使得框框之间的这个总的距离是最小的，这是一个匈牙利算法，然后之后，还有一点稍微复杂一点的就是基于这个马可夫的这种决策过程，也有也有这样的一些算法，这个我就不具体讲了就大概给大家了解一下就关于这个MOT，也是有比赛的，这个比赛，一五年开始，然后接着中间停办了几年到一九年，又开始办了，在今年这个六月份就专门有一个这个我PUP是这个MOT CHALLENGE，那么这个多目标跟踪了我先前提过一嘴，就是一般来说，我们做多目标跟踪的时候都是会专注于某一类物体，而最常见的，就是人和车，所以现在一般大家做多目标更多的是人和车这样的物体对这是多目标跟踪里的就是他的那些就是数据集里面那个视频流可以看到就是说，这里面的每一个问题对于我们的目标跟踪来讲，相对还算是简单的，如果像那种滑雪，摩托车，那种那种很难的，这样的视频流，然后第二件事情就讲一下这个关于双目的视频跟踪，人的跟踪这个，其实主要的应用场景就是那些机器人，他的一些服务机器人或者怎么样，他有两个是两个摄像头阿两个摄像头跟单个摄像头最主要的区别是什么，就是说我可以得到一个深度的信息就是根据左右两张图去做一个匹配，我可以得到得到一个深度图，所以就说你可以想象成什么，就是我的input，除了以前的视频流是RGB这样三个色度的这个通道之外，我还有一个深度的通道，所以呢之前就有一个这样的工作就是说我怎么样去利用这个深度信息，他提出了就是比如说我还是用这个深度网络，一个模型，就是把这个RGB和这个SD就就是立体的深度放在里面，只用RGB或者是RGB和深度信息分别用不同的网络，然后去做这个跟踪最后那个结果，就是把它就一股脑堆在一起，这个效果肯定是最好的，他肯定是会比你不用深度信息要好的多，当然了这个深度信息，还有一些其他的用法，比如说我就在那个检测器里面我去用这个深度信息，因为我刚才讲了吗，我在这个MOT的这个任务里面，我一般是会假设有一个检测器的每一帧我是可以做一些可以做detection，可以在detector里面去用深度信息或者，是我在跟踪的时候，比如说在三维里面去做这个跟踪去利用这个深度信息。也可以，是比如说，发生一些遮挡，那么我怎么样从车遮挡里面恢复这个深度信息也是会起到一定的作用的，所以其实双目的这个跟踪的，你可以，你可以理解为就是用现有的单目标跟踪和多目标跟踪，然后再多加一个深度的CHANNEL就能把它做的相对比较好，所以这个领域，其实这几年来看并不是一个非常活跃的研究领域，他们也是有这个Bench MARK的就有一个这种双目的产品的版权吗，所以你们可以看到阿在我给的这些图里面其实他是有深度信息的，第三个，有点相关的这个东西，叫做人的姿态检测预测和跟踪就是对于人这种目标来说，我觉得就是跟其他普通目标来讲,最大的差别应该就在于人是有结构信息的,人是有各种各样的关节点的，然后观结点和观结点之间，应该是符合一定的限制条件的，比如说人的这个上臂和前臂之间，他可能是有一定比例的，这个比例他尺寸不会变太大就比如说你比如说我做了一个关节检测以后你如果发现你的前臂是你上臂长度的两倍，那这个肯定是不可能的，对吧，所以说他有自己的一些约束条件，那么通过对于人的这个姿态关节点的这个检测的话，那么可能我能够对人这种物体起阿起到一个更好的这样一个跟踪，先讲一下就是第一步人的这个姿态的这个估计他们其实是有两种技术路线吧，一样的图上面那个三张，是一种所谓TOP down的技术路线，什么意思。

我就在这个图片里面先做一个物体的检测把这个人的物体的这个框检测出来，然后把每一个框拿出来去看，这个框里面是一个人，那他各个关节应该在哪里就给出了这两个人的这个关节点下面三张图，是一个所谓的BOTTOM UP就是由自底向上的，它的主要差别就在于第二张图，他不是去做这个人的检测，而是他直接去检测关键那个关键点检测关节点以后在第三步，他干的事，把那些关节点，去聚类就是哪些关节点是属于同一个人的把它组合起来，所以这个就是关节检，一般是有两种这样不同的这个技术路线，所以这个大概就了解一下就好，，那么为什么要讲这个关节，就是这个关节的检测他也是任务就是这个，是给了一个benchmark叫做poseTrack这样一个benchmark，它的特点，就是说他是多人的这个视频里面有很多个人，另外他的这个动作相对来说是比较快的，而且，经常会有一些遮挡和一些那个尺度的这个变化，下面这两张图就是从这个POSETRACK上面截下来的图，那个绿色区域就是我们不关心的区域中间的那些有点的，这都是那个就是他们标注的真正的这个人的关结点这个是关于就是姿态检测，如果大家，想要做更多的了解的话就是我就列了一些关键词和一些link然后需要的话就是到时候去GitHub上去搜，就会收到相应的这个项目和PAPER阿很多，可以就down下来，可以试一下。

最后一个稍微有点相关的，叫做person re-identification就是叫做行人重识别这个，一般适用于有多个摄像头的这种像我们这种园区他的意思就是说我在这个园区里面布了很多摄像头，这个人可能是会从一个摄像头的镜头里面会移动到另外一个摄像头镜头里面，那我怎么样在不同的摄像头不同的是视频流里面，我把这个相同的人给他对应上那我们知道就是不同的摄像头，他可能这个比如说白平衡就调的不一样，所以在前一个视频里面，你看上去是比如说是一个蓝白裙子到第二个可能你看上去就像一个金白裙子，所以，所以这个是给大家带来了一些挑战其他，我们先看一下这个应用场景就是这个这个问题的这个定义，就是在我的这个视频这个raw video里面，比如说这是其中的一个角度，就是一个VIDEO，然后在这个里面，我去做一些这个行人的检测检测出一个一个框，然后这个所谓的gallery，其实是比如说我别的摄像头，在做检测以后都有一个一个切下来的框，那我这里找出来这个框就要去更gallery的图像去做匹配，然后看看，到底是哪一个人这样，就把不同的摄像头里面的这个视频流就可以穿起来，讲一下这个挑战吗就是刚才讲的对比一下，这里面，我给了八组这个例子，第一组的这个例子是说，我第一部不是用检测器去检测吗，检测区域的检测框，他有可能检测的不准，比如说像这个图他他检测框包含了很大一部分的这个背景，这个人的脑袋本来是应该是出现在画面滴但现在是出现在中间，所以你如果他们俩就是很直白的这样去进行匹配的话，可能是匹配不上的，另外一个，就是他的动作可能就不太一样了，或者是角度不太一样，像这个行人她们俩是同一个人，这个，还在自行车下面推着车在走，这个已经骑上去了，那你要知道他是同一个人，然后还有，就是有一些遮挡的情况，那最后就说，其实这个人和这个人不是同一个人，但是你去看她上半身都是白色的T恤，所以看上去可能会非常相似，所以怎么样能够做到一个非常好的一个行人的重识别，行人重识别这件事情也是一个有名的任务，所以，它会有一些benchmark的数据集，这是一个简单的列表，讲一下这里面滴这个长的方法，我们时间不多，简单过一下，这个是一个很直白的这个方法就我有这样一个框，那我要比较的时候，我就把这个光切成六块，哪一块儿，去提这个特征，那这个淡蓝色最上面这一块，我提了，就是可能就是头的特征这块，可能就是胸的这块可能是腰的，这可能大腿小腿脚或者是鞋子，然后，就是一个一个去比较头和头比，然后鞋和鞋比，然后接着再把这个总的比较的结果，去综合一下，这很直白的，但是这个刚才那个方法，可能就没有办法去很好的去，因为他只是在空间上去切，所以如果一开始检测框就不准或者这个人的这个形态发生改变，以后，那我可能就这个匹配就匹配不到了，这里讲这个工作，是我们组另外一个研究员，今年的一个有工作也是在发表的这个就是说利用所谓的dense pose就是我在看到这个人的时候，我对这个人姿态进行估计，所以我知道每一块相当于我一个标准人的什么部位，然后把这个标准人的这个部位的这个，PICTURE把它把它有点像踏下来，所以这个可能是右脸，这个是左脸，这个这两个是这个是手然后，有些部位可能是背面的那个部位，现在看不见那他可能就是黑的，然后最后，所以这个就会变得更加的精细，所以，就是会得到一个更好的这个效果，请大家就是讲这些，最后，就是简单总结一下，就是目标跟踪，其实他还是面临很多挑战，比如说她这个外观的这个变化，这个变化可能是由于光照可能是由于姿态或者是由于这种杂乱的背景引起的这种外观的变化，然后在人的跟踪里面，其实，大部分来说，人的跟踪你基于以前的这个单目标跟踪以及一些就是这个，association的方法都能够得到很好的解决，但是，其实在人的跟踪里面，就现在很常用的应用里面，他更大的挑战，其实是人和人之间的一些相互的这个作用或者是一些遮挡，然后因为在现实生活当中，这个人一般都成群出现的么，所以在一些非常拥挤的这种区域里面这个人的跟踪，会面临一些比较大的一些挑战，那我们认为，就是从技术上来讲检测和跟踪应该是一个一起考虑的，这样的一个任务，因为这个也是比较符合我们人类自己本身的这样的一些常识或者符合我们自己的一些直观上的这种这种想象那最后，还是就是总结一下就是我开头讲的就是跟踪其实是一个很难的问题，所以你记住这一点。

大家一讲，你这个是视觉专家了就知道这个更多更难的一个问题，那么还有很多的这个研究需要去做，那我今天就是讲到这里谢谢大家。

 

 

## 9.

今天讲一点不一样的东西，今天讲的是对抗深层网络，简称GAN，也就是前三个首字母缩写，首先我们来讲一下什么是生成模型，今天我们见的都叫discriminative model, 这叫判别模型，输入的话就是一张图片，输出的话一般就是框，或者是类别是这样一个问题，这样我们叫做判别模型，这次我要讲的东西要反过来，假设我知道一个类别，我能得到一张图吗？我们在判别模型里面，我们输入图片是猫，我们把问题反过来，假设我想得到一只猫，我们让计算机生成一张猫的图片，问题就是我们如何去让计算机生成一个图片，如果你想生成一张图片，试试GAN吧，对于GAN来说我们分成两个部分，第一个是生成网络，输入一般就是一些你想要的label，我们有另外一个D网络，这个网络就是判断你生成的图片，到底是不是一个好的图片，这就好比我们婴儿学说话的过程，指引把话说的更清楚的过程。我们这个网络也是这个过程，下面的网络我们不断生成的图片，然后尝试去欺骗D网络，去判断是真是的图片还是假的图片，最后的效果就是D网络分辨不出来。在我们的世界中不是所有图片都是有意义的，在一些很特殊的点这儿是有意义的，只有在蓝色稀疏的点上才有意义，那我们怎么知道哪些点是有意义的？我们用网络估计出有意义的概率分布，知道哪些概率最大，就去生成概率比较大的那些点，然后这就是GAN的一个目标函数，我们不必理解，意义其实就是通过动画来表现出来，蓝色我们假设在世界中真实图像的分布，我认为0周围的这些数是有意义的，生成网络就是我不断尝试到底那些点是有意义的，右边是整个损失函数的值，当分布完全重合的时候，损失函数的值是最低的，然后通过不断的优化损失函数就会使生成的图像分布和真实的图像分布基本完全一致。GAN是一个非常新的topic，大概2014年才出来第一篇文章，才过了5年现在有500多篇文章出来了。因为GAN有非常多的应用，最简单应用是生成人脸的图，生成不同人脸的图像。还有一些更加神奇的例子，比如我给一个Mask图像，告诉你我哪个地方需要生成车，哪个地方需要生成树，就给了语义的这样一个图，会自动画一幅图像出来，这是之前完全不可想象的任务，我们都能把他做出来。还有一个应用就是我画了一个简单的线稿，我就可以生成一个人脸出来，不仅仅是简单的上色。另外一个就是图像的超分辨率，比如输入是模糊的低分辨率的图片，通过我们的GAN就能直接生成一个高清的图片，真是的图像就代表高清图像，我的输入就是低清图像，网络会逼迫他从低清图像转成高清图像。所以一切都是网络自动去学习的，怎么把低清转到高清，告诉他什么是高清图像就好。另外可以做一些很fancy的东西，比如去编辑人的不同属性，比如改变头发的颜色，改变表情，甚至改变性别。 GAN可以做到人类做不到的事情。

更有意思的是除了简单的转换，我们还能做更复杂的转换，比如说从这种油画转化成真实的照片，从照片转成油画，然后我们能发现不同的画家可能还不一样，我们可以转成像莫奈一样的画风，或者梵高的画风。我就可以利用所有梵高的画，那就是我真实的数据分布，然后就自动转成梵高的画出来了。

今天的讲座主要讲三个GAN的算法，第一个是做人脸的生成，给一张人脸做不同的图像，第二个我们一般做video的上色，比如给一个黑白的老电影，我们自动给老电影上色。第三个就是做一些人脸的编辑，比如把头发变长，类似于美图秀秀但是全自动的功能。 首先我们讲第一个人脸生成，我们的目标主要有两个，第一个叫做open set，就是说我只有你一张照片，我要生成你其他的照片，并且不存在我的数据库中，我只在网上得到一张你的照片，然后可以生成各种各样你不同的姿势，表情。第二个identity preserving更好理解就是说我生成的这个照片看起来得像你，得是你这个人，哪怕是你的熟人去辨认是不是你，一样的。这个东西有什么应用呢？第一个就是人脸的编辑，第二个就是说做人脸识别训练的时候我们需要很多人大量的数据，有些人的照片只有一张，我们可以生成很多照片，然后用来做训练。还有做识别的时候，我们知道侧脸比正脸更难识别，所以就可以做所谓人脸转正的效果，然后用正脸去做识别，这样的话就可以提高识别的精度，这也是一些比较有意义的应用吧。

那我们是怎么办到这一点的，这就涉及到核心的想法，一个人能分成两个不同部分组成，第一个就是身份信息，第二个就是属性信息，叫做attributes，然后对于这样的照片我们认为身份信息是赫本，属性信息就是微笑，正脸，光照强度，这些属性。但其实身份属性不是那么容易理解。我们就让网络自己去学什么是身份，什么是属性。讲算法之前先看一下结果吧，左边就是身份的信息进来，右边就是生成的结果。 另外就可以变表情，比如输入笑和不笑的图，然后我们就能改变。

这就是我们整体算法的结构，整个算法非常非常简单，首先有两个网络，一个提取身份信息，一个提取属性信息，我们就把身份信息和属性信息给第三个网络，这个网络生成了照片，为什么网络可以做到这一点，我们怎么做到去训练这一点，这是整个GAN的关键，怎么去设计损失函数，让GAN达到想要达到的目的，提取身份信息其实很好理解，对于一个人不同身份信息向量应该是一样的，做人脸识别的问题，比如门口的闸机，你只要注册之后，以后每次进来之后都能给你开门。

 

 

 

## 10.

网络去提取这样一个能力的，这样的一个信息能力的，这样的一个特征，实体这个人只要是只要是你发的这个特征，向量就是一样的，这样的话就可以每次就可以去匹配，然后就是可以找到就是哪个是你的话就是你的距离就会尽可能的相机可能接近，所以这个地方，我们用的一个人脸识别的这样的一个特征，就是说我们这样的网络就是提取了电视特征，就是使同一个人的他的这个特征向量，尽可能是一样的，然后不同的话，大家不一样，然后我们就这样一个特征的话就代表我们的身份信息，所以说这个身份信息其实就是网络学的他都会海量的一个他为什么会选择这个我们搜集搜集了海量的数据，然后就是我们对于每个涉及很多人的数据，然后每个人收集的大量的这个人不不同的数据，然后让我自己去自己去找就是这个人到底什么东西在更新，所以说，所以说这样的话，比如说你这个人一直戴着眼镜拍照一直戴眼镜就认为眼镜这个身份信息，然后其实我们发现一个网络来说，什么事是那些个人意义的就是眉毛的形状观察会发现其实眉毛形状是最能表示这样的一个人的身份的或者那个人的眉毛形状，其实差别最大的反而嘴，鼻子的形状，其实大家都很接近，这种形状其实是最最能代表这样的一个一个身份信息的，所以其实网络也他也太自然而然的去抓住了这样的一个一个特性，然后这是身份信息怎么提取的提取和分类信息，我们怎么提取属性信息提属性，这个设计比较就稍微就有点复杂了，其实我们这个地方做了一个很有意思的一个假设就是我们认为对于图来说，除掉身份信息之后，剩下的所有信息都是属性信息，我们怎么去提这些信息，就是我们首先就是收集了各种以这个人各种各样不同的照片，因为我们知道这对这个人的各种各样不同的身份信息是一样的，然后我们要让一个网络去强制让他去重构恢复就是去恢复他自己的照片，因为这个地方我们输入的是身份信息和属性信息，然后身份信息，对这照片都是一样的，那剩下的那个他，如果这个网的话就要恢复这个照片的话，那么他必然需要提取那些这照片说他不一样的东西，所以说这个照片那么。

他就肯定那就是我们认为就是属性信息，那我们也不知道他到底是不是真的是属于但是我们觉得他应该是属性信息，然后这个地方，我们也要做一个示意图，我们就把这个属性信息的这个属性的向量能把他打到这个空间中，然后就看对于同样的一个这个，这个属性向量，然后就换他的这个身份身份，这个向量让他看到会生成什么样的图，然后发现确实就是如果你固定住那个属性向量的话，甚至这个图的话他大概的都是一个角度，光照，这种是不是做背景的颜色什么的，他都几乎是一样的，这就是我们所有证明我们的属相确实也提到了一些属性，然后这个地方就展示一下我们这个算法的一个结果吧，然后就是左边这一列，就是我们的用来提取就是我们表示这个是测身份信息，所以我们从这照片提取的一个身份向量，然后从这个上面的照片的话就提出这个提示属性向量，然后中间这一大块，就是我们生成图像，然后我们就会发现这是对于同一列的话就是代表是不同的人，他们的属性是一样的，然后，对于同一行的话，就代表是这个人，然后不同的属性，然后我们就发现我们这个记录只可以生成各种各样的这种不同的这种照片的话看起来比较有意思，然后比较有意思是我们发现口红这个事情他是他身份信息就是你看这个人他那个口红比较偏红的就是嘴比较偏红，然后对这个下面这个范冰冰她那个她用的是这个，他下面那个颜色也会比较偏淡，比较偏淡一点，没有上面那个红颜色的东西是属于身份信息，这个属性的，H的属性我们不用人，我们把它换一下会怎么样，换一些比较很奇怪的一些图像会怎么样比我们训练，没见过这个比我们换成黑人，你会发现在他其实这个地方他是认为黑的东西，它只是一种光照的变化，他并不认为这是一个ID，这是一个看起来像像在暗光下拍照的一个范冰冰。

但是如果对不是这种人脸的照片的话，其实我们算法不太work，这个也可以理解为我们的训练数据里面并没有见过这种属性，网络靠蒙他也不知道你到底想提啥，然后这个里面就是我们刚才说的那个就是人脸转正的一个结果就是，第一列的话就是我们的一个输入照片，然后第二列的话就是我们输出的一个结果，然后我们发现就是，这一列就是我们就是基于就是，后面就是基于一些三的方法旋转过来的，就是把这个脸贴到一个三D模型上面把人脸转过来，但是对这个3D方法的话，就是对不可见部分的话，他其实他是不不错，补充不出来的，然后，但是对我们这个算法的话，就比如说这种看不见的部分，就像这个这个耳朵，我们可以通过这个跟你猜出来，猜出耳朵长什么样子，然后其实这个因为这个也是很有意思的一个点就是一个网络来说，它自然就猜出这样的一个方向，然后我们同时，也可以保持好比较保持好这个原图的灯光照，左边这个是川普的女儿伊万卡，然后右边的是我们生成的结果，大家可能也看到范冰冰，然后那个就是通过这个算法，我们其实只有一张范冰冰的照片，我们就可以生成这样的一个结果出来，好像我说好了我们我们不是邪恶科学家，我看到这个例子的话，可能会有一些不好的联想，但是我们其实，我们研究这个算法的目的是可以证明我们算法是可以做到这个事儿，第二个其实我们更重要的是我们希望以后我们能够检测出右边那个视频是经过编辑的，是假的一个视频，然后从而来保护大家的隐私，我们不是邪恶科学家，但是这个其实这个算法，还是会或多或少，其实为什么能够检测这样一个这是为什么知道能够显示右边是假的，其实大家其实通过这个例子，我们可以看出右边那个地方还是有些不太真实的地方，特别是一些有遮挡地方，比如像头发这个附近就是他那个一旦被知道的话，其实这个地方我们是很难知道哪个地方有没有被遮挡，然后，所以生成这个地方其实有点重合，有重影又想保留原来的这样一个遮挡的这样的一个属性。就说这两个优化目标会打架，然后就会在这个地方会生成这样一些那种看起来不太真实这种这种结果，还有在这边这个头发这边也是会有一些这种这样的一个东西，然后第二个，就是他有看起来比较假就是他的表情做的没有原始视频到位就是比如像这种这个法令纹的就是鼻子这两边的这种就是这个纹路，它就是你原来的视频的话，做的会比较比较深，但是我们因为想保留这种属性的话，其实大概保留住，但是他没有保留那么的百分百的就是感觉这个人戴了一个面具一样的，然后那个就会显得不是那么真，所以通过这些这些细微的差异，我们就可以能够检测出就是这样一个事情是假的，是一个这种就是经过编辑的这样一个视频，但是这是因为我们知道这个插件的，所以我们也希望改进我们的算法能够让我们生成更加逼真的这样一个能力，就是一个矛盾的这样一个过程，就是我们希望我们能造出更好的矛也希望我们能够造出更好的顿，然后通过这个东西也可以知道就是我们一些现有的这些一个，比如说像活体检测的这样一个算法，其实不是很安全的，这个大家用过支付宝就是什么身份能够匹配，就是当你有支付宝需要开通什么功能匹配的功能的时候，往往是比如说拿到手机首先判断是不是你把他一般让你比对了一下他为什么动一下其实就是当时他这个目的是怕你用一个人的照片去欺骗那个支付宝，但是这一带有这个技术的话。

他需要能够踏实需要客户的硬件，它那个需要那个红外的这样的一个收发的，这样的硬件才能检测深度，就是需要就是更更强力的一些这种活力检测方案，才能够做到，比如像什么双目，这种才能够更好地做到，就是这个是一个意思的应用吧，然后这个例子我们讲到这儿，然后下面我们看我们第二个工作叫那个视频的上色，什么是视频上色，其实这个工作也很容易理解，其实就是这个视频里面向大家展示这样的例子就是对于一个黑白的一个视频的话，然后我们希望能够自动的给这个颜色这个视频中的每一个点，然后填上颜色，然后这个工作的话，其实早期来说是一般需要人来完成的就是一般是那种美术艺术家，然后他们逐帧去画，但是一般的话，他们更多的是用在这种照片这种上色，就比如当年开国大典，当年四九年的时候开国大典上拍了张照片，然后那个照片原始照片是黑白照片，然后后来大家问了一下，想把这个照片更好看，然后就找了很多种艺术家，然后去修那个群聊，那个照片，然后把那个照片变成彩色照片，然后的话，但是对照片可以这么干，但是对视频怎么办，你这个视频一秒钟30帧，你不能让那个画家没美中不足的计划，然后之前的话有很多工作就是说我只画第一帧就是那个画家就是去画第一帧画完之后，后面，然后那个算法自动去生成这样一个彩色的这样的一个视频，但这样的问题是你怎么去保证这样一个连续性的，比如说你第一周这个穿的是个黄衣服，第二个是两个人，一个是黄色，一个蓝衣服，然后这个你怎么能够保存这个两个人一直是就是衣服颜色不要拖发生突变，这个事情是怎么保证，这是那个视频上色里面滴一个主要的困难就是我们这个任务里面的三个困难，第一个就是说第一个就是要配合空，这是我希望我上课的时候，我知道我用的控制哪块区域上大概上个什么颜色上去，你不能说，就是我这个衣服，我想让它变成黄色，也可以想到变成蓝色也可以因为很多时候彩色到黑白的过程是没有没有什么黑白的彩色这个是有歧义的，因为某些也是丢了，比如说你一个一个红色的衣服和蓝色的衣服，你同意红色和蓝色头道黑白的照片，他也是他黑白是一样的黑白照片，你看的是一样的，所以反的，回来的时候你是有各种各样的选择的，然后就希望用户能够控制这个东西是我会直接说这块区域是红色还是蓝色，然后变相的控制，这个过程中尽可能简单一些，然后第二个问题就是我希望能够保持一个这种常识的一个相关性，这是在生成很长的一个视频的时候，我都能够保证颜色不要不要乱跳，不要不要跳跃，然后第三个，就是我希望的就是能够尽可能快，然后，就是我们那个我们提出一个解决方案吧，这个第一个我们是怎么能够控制这样一个大家都越来越会怎么去控制这个颜色生成，其实我们这个做做一个叫一个战斗的方法，就是说你会给我个例子就是我不要你完全完整的上这个图像的，这样的一个颜色，但是你给我一个跟这个照片类似的照片，然后，然后就跟你这个类似的照片，去处理，现在想上色这样的照片，然后第二个就是后面我们遇到了再说吧，这就是我们那个就是整个算法那样的一个例子吧，就是说首先就是我们有一个输入的一个黑白的一个视频，然后第二个说你用的你给我一个参考的照片，然后这个照片里，你可以跟这个视频没什么关系，可以是不是这个视频的一个照片，但是你得跟这个视频大概有一点类似就是比如说这个视频里面有个人，你给我的这个例子里面最好有一个人，然后我们就会把这样的一个照片里面这样的颜色，希望说这个人穿一个红色的衣服，然后背景大概是一个蓝色的背景就是我们算法能够自动找到这样的一些颜色的区域，然后把这些颜色跟他填过去，然后设置一个这种彩色的一个视频，那我们算法是怎么能够做到这么很神奇的事儿。

其实那个我们上回分成两个部分，第一个做相关性预测的找相关性，，我们首先去寻找这个图像中，语义上对应的一个点就是这个地方的，其实我们也是好像就是用的两个网络用的网络去提这两个图像的，这样的一个特征，然后，我们两两去匹配上面这个图中的每个位置特征和下面这个图中的每个位置特征，然后找到这个下面这个图中那个和上面那个图就是最接近的那一个特征，然后这样的话我们下面这样的图，这个颜色，把它拷贝到上面的图，然后就生成一个这个拷贝的这样的一个就是把那个颜色就是把语义接近的颜色跟他拷贝的上面去好像首先生成一个初步的结果就是，比如说那个比如说这个地方有草的话，用网络提取的特征，我们认为他和下面的这样的一个草应该是比较接近的，房子应该是比较远的，所以的话我们就直接去比较这个上下两个图的特征，自然就把下面那个草跟上面的草对应起来了，然后把下面那个草的颜色，就贴到上面的这个草上去，然后就得到这样一个初步的结果，其实他是或多或少会有一些问题，第一，就是说你都用交界地方，你可能就是特别的话就比较模糊，其实判断不太好，这到底这个是什么地方，这个他这个地方颜色其实过渡过来的，就比如这个地方草地的话，它的它下面是绿的，但是他这个房子黄的，他交界地方有点偏黄，这个在我们这个地方叫颜色的泄漏的问题可能会也会有这样的蔓延开的这样一个效果那种然后，第二个就是说如果你这个上面这个图像有一个东西像没有这个地方，它这个颜色的话，其他就是其实很随机的颜色，他说你不知道他就随便填个颜色去了，反正有时会产生一些不太真实的一个结果，所以我们现在需要第二步来去把这个颜色给修正，怎么修正，这就用了GAN，其实很好理解，我们知道一个真正的一个彩色照片是什么样的。

所以我们就只要去判断这两我们把这两个当作输入，第一步，生成一个彩色照片，然后和一相似度就是你图像的特征的像素大小，然后把这两个输入到网络里面，然后那个图然后我们只要用GAN去判断这样的一个图是不是一个真实的彩色照片，然后就会自动的去调整那个最后的结果使他尽可能地去接近一个真实照片，因为那如何我们怎么去解决这种常识相关性的问题，其实这个问题也其实也很容易，我们就是在每次就是这个X代表你的每一帧，我们把他那个输出的结果，这个上面的输出结果，我们把他那个把他当做下一帧的输入就可以了。然后在的话就是做成一个这种递归的一个形式，然后让他能够能够抓住常识相关性，这个地方就是我们用的一些一些损失函数吧，这个地方就是很简单的介绍一下，这是我们怎么能够保证生成图片尽可能好，其实我们除了那个刚才说那个GAN的那个LOSS以外，再说我们还有一些其他的一些损失函数，比如第一个地方，我们用的那个语义相关性的LOSS教就是我们判断这个你这个是什么彩色照片，这个和这个黑白照片，他那个对他那个的是尽可能相近的第二个就是说，再就是你和你输入的这个，输入这个参考图像，就是你的挪动一个地方，他们的尽可能向东西。

然后，第二个就是我们加了一个光叫smooth loss，就说我们其实也是很好的，我们希望你对于相邻的区域的颜色的尽可能是一样的，就是保证那个图像的平滑，不要说你在一个区域的颜色发生跳变，然后第三个是这个TEMPORAL，第四个就是我们就是刚说的加入我们刚才说的GAN的事，你生成真的一个彩色照片就是这么一个LOSS，最后LOSS的话，就是说在当我们那个如果你的reference就是个参考图像，我们训练的时候，我们可以把一个彩色的一个视频把他转成黑白的，这样的话我们就可以知道这个黑白的视频，他甚至说应该是个什么样的视频，我们在的话就可以有一个很直接的一个pixel。

 

 

 

 

## 11.

在一个比较的这样一个loss，然后这个参考图像，我们第一帧就是彩色的样子，但是它那个好像就是我们可以恢复原始视频，我们继续看结果吧，就是我们的一些实验的一些设置吧，就是我们在这个CASE 0里面的CPU上，然后我们的图像大小的话，300多*200多的图像，然后我们的能够跑就是大概0.6秒钟没每一帧，然后这是我们的一个一个结果跟其实跟其他方法的比较结果，第一点的话就是我们那个输入的黑白的照片，第二就是我们参考的一个彩色的图像，然后最后一列，最后这个就是我们的一个结果，然后就可以看出就是比如说我们可以自然知道比如说这个想穿个蓝色的衣服，然后这个下面这个也是穿个蓝色的衣服，但是这个有个很有意思，比如这个帽子，其实这个人的话带这个蓝色的帽子，但是因为这个蓝色的帽子，如果转成黑白的话，它其实是一个纯黑色的就是一个黑色的样子就是灰色的样子和这个白色有点不太一致，是我们这个帽子都匹配到了后面那个星条旗上，因为其实我们那个匹配只是长相相似的快，但是并没有一个明确的语义信息，所以他只是他匹配的那个白色的加上一个红边的这样的一个帽子，然后这个地方这个例子就是展示的就是给不一样的这样的参考图片我们我们会是一个什么样的结果，这个地方，这个图是一个黑白照片，然后这个是我们是展示了四种不一样的参考图片，然后周边的这个四幅，就是我们的那个生成的结果是会发现就是给不一样的这个参考图片的时候，它就可以生成这样的一个结果，这是代表了一个用户可控的，这样的一个效果就是你给个红色衣服，我就想问问红色的一个白色的东西可以生成一个灰色的，然后蓝色的话就是蓝色的，为什么给个深蓝的衣服是浅蓝色的，对，因为那个我们这个是保证这个生成的图像那个黑白图像都得跟这个图像是一致的，如果对就是保持一个灰度的是一样的。

这个其实有点偏红，然后这个有点偏黄，对可以有可能匹配到后面每次这个确实过去了，就真的就是这样的问题就是为什么这个黑的时候，我们这边不能是黑色的，因为其实你想如果这个头发是黑色的话题转黑白是什么颜色就是黑色的，但是我们给了这个，这个，这个，这个这个头像他这个头发是灰色的，所以我们为了保护了保证你这个这个，你甚至要转账很黑的和你的输入是一样的，所以说我们不可能完成你是你，如果你说这个地方不是黑色的，我们是不可能生出黑色东西出来的一些。

这一块儿其实没有看仔细看这个是头发其实其实也应该很难匹配的，所以头发然后一个一个光剑，那个文成县社会黑社会黑背景，这个黑头发黑背景，但是这是唱歌参考图片对这也有可能是对的，因为其实我们匹配其他也没有仔细管这个语音信息，我们就只是跟他的局部的这样一个形状去去判断，然后这是另外一个例子吧，就是一个风景的照片，然后就是，当给不也是说给四个不一样的，这样的一个参考图像的时候，然后让它生成一个不一样的这样的一个结果出来。但是这个其实这个例子有一个问题就是你看这个地方草地，就是如果你给了reference就是这个参考图像没有草地怎么办，他说他首先强劲匹配的一个蓝色，但是后面就是这个GAN，其实他也没有没有能力把他干好，当错开远的时候GAN也没有办法，的有点绿吗，那这个蓝绿色这个这个地方，他希望他能够瘦一点的往下面感觉偏远，因为定位这个你看这个其实这个照片的天空也很怪，如果天空是一个那种很亮的蓝色的话，其实他根本不会说有很黑的这种对这个生活的实际的颜色都会真的不一样，真的不会长这样的几乎不会这样对摄像你，他有可能是一个乌云密布的一个天，这是拍的黑白照片的时候，这是我们跟其他方法的数值比较，然后，反正就是我们好就是了，这个视频的结果的，然后这是输入然后这是参考的一个照片，然后再说我们算法这个上面三个是其他算法结果。

然后下面我们再看一些那种真实的老电影。

还有什么疑问，那我们再继续我们的下一步去人群，我们有做过，但是我们要放到这个PPT里面去全身，应该也挺也挺好的，因为我们也做过就是有人没人都都可以做吗，反正只是去匹配就完了，算了匹配就完了，所以说那个其实并不在乎你这个里面是半身还是全身。

只要你的参考图片和你的输入图像都有全身的就可以找到对应匹配一堆的匹配了就把颜色贴过来就完了，我们下来讲了一个，叫MASK-GUIDED PORTRAIT EDITING的破解，ID是这个任务的话，我们做的这个事情是这样的，就是说首先我们比如说我这个我输入，就是一个人脸和它对应的一个MASK，就是我们有一个语义的标注信息，就说我们首先告诉你哪个地方是头发，脸，眉毛，当然大家不用管这个东西怎么得到的，反正我现在有了我们有了，这玩意作为一个输入，然后我们要做的就是比如说我们可以改变这个人的这个面部的皮肤就跟他加个胡子，然后，第二个是我们通过编辑这样一个MASK来给来编辑这个图像，比如说这个人原来嘴是闭上的，我们把它张开，张开之后我们还可以自动跟他把里面的牙舌头给他补出来，然后就是比如说这眼睛就把它把它闭上，因为你知道就是比如说如果你在原来里面是原来的比如你想如果只有背景照片的话，其实是比较复杂，比如你要把刘海去掉的话，你还通过一些比较那种比较强的这种特效技术去修改这样一个图像就是才能把这个头发给去掉，现在我们比较简单，我们只要去改这样一个MARKU知道我觉得那个地方原来头发把脸然后至于变化什么样你网络你那个算法自己给给就行了，这样的话就可以大大的减少，这样的一个能供的，这样的一个干预过程，然后。

还可以说换这种眼影，换这个，这种嘴唇颜色换头发颜色是不是其他各种探险换装的这样的一个一个效果，这个地方，我们都是通过一个视频来看看我们能做的是吧，就是第一个就是我们那个通过这个编辑这样的一个MASK区域，我们去去深圳顺不要脸，比如说我们该张嘴的话，只能把牙喝成这样，所以看不出来，然后把那个通过托修改这个头发都放的跟他把刘海给它把它去掉，第二个是我们可以就是给个另外的原因就是我想让这个我想到这个眼镜像另外一个人的眼影，还会自动把你这个照片这个那个眼影换成你的一个这个目标的，这样的一个一个一个眼影，然后并且尽可能保持一个。

然后比如说换了口红的颜色，你可以把别人的口红把它transfer过来之后体验店，因为GAN他是整体的和谐，他因为他可能直接贴过去他不认为不太合理，觉得稍微稍微变了一点其他地方不仅是变嘴，是整体看起来尽可能尽可能和谐，然后这个变头发颜色，后面是大概简单的讲下这个是怎么做的吧，就是因为这个算法略显复杂被框图吓到，然后，其实那个地方就是这个算法就讲这个分成三个部分，第一个部分就是局部的编码，第二个是前进的变换，第三个部分是背景变化，首先我们看局部编码，其实就是输入一个人脸和你这个对应的这个MASK，我们把每一个component的先把抠出来，然后抠出来之后，我就铜锅一个网络去提取每个component这样的一个外貌特征就是提取这样的口红，这种皮肤也是这样的一个特征，然后其他课程之后，我就给一个他的这样的一个一个这样的一个MARK的我就把这些特征都贴到你想要的这个MASK这个位置上。

然后就说你就把上面那个外貌特征贴到下面的对应位置上，贴完之后，我就通过一个网络去生成一个这种一个新的这样的照片，这样的话就是他的这个几何结构就是就会这你给的这个MARK一样，反正外貌就和上面这个一样，然后这样的话，那是因为这个照片的只有前景只有两核这个东西，他没有背景，所以看得不是很真，我们下面还有一个背景层的网络就是我们给个图，然后通过这样的MARK，把那前景扣掉，只留背景，然后再通过这个网络，把这样一个前景的这样的照片把它融合起来，然后就生成一个看起来真的一个照片，然后后面也是有一个GAN，判断这个照片是不是下一个真的人脸照片。

整个算法流程就是就是这个样子，然后这个地方就是那个我们那些结果吧，这个例子就是展示就是我通过这样的一个MASK，我可以生成就是就是给个MARK，你给我把它变成各种各样生成不一样的人脸，你在保证你人脸生成出来能够符合这样的一个MASK就可以了，这样就是代表就是说我们可以生成就是各种各样不一样的人脸，然后，都符合这样的一个几何结构，然后右边这个就是最左边这个是groundtruth，是真实照片，然后右边是我生成的一些照片，这照片看起来效果还可以稍微可能有些问题就是，比如说这种，这种光照，就是那种其实看起来不是那么的真。

你比如说这个图片是那个脸上来看，应该感觉像是一个右边，一个就是有一个右边一个光一个侧光就是感受就行，这边连亮都不亮，但从身体上来看有没有这样一个的话肯定会感觉不是那么真实，但是，但是我们结果也是足够好了，然后这边就是一些其他的例子吧，就比如说咱俩把这个人的头发变得更长一点，你这种你这个任务，你刚才在PHOTOSHOP里面，如果你想做的话，你在这个图像改你你还把这个图像先头发可能先把它复制下来，然后再怎么把它那个一点点的画，现在我就只用这个MARK上面我加一块区域，然后他自然头发就变长了而且看起来很真实。

比如想把额头变高一点，显得像个秃顶的效果，我觉得把这个帽子把它推上去，可以自动把他变成秃顶的效果，还这个眉毛的形状就是把原来是个平的眉毛把它变成像这样一个八字，这种八字才知道是什么，然后这边的例子的话就是展示的是一个TRANSFER的效果，这边是输入一个人脸，然后我想，比如说我想让这个人的下巴看起来像这这个胡子看见这种其实就是我们把这个这个东西把它放在刚才说的那个就是局部的编码器里面，然后其他的还是用这个，那个图像就变成就变成这样子，然后克洪把他劝过来其实也是也是就变这样了，后面眼影的头发其实也是。

也是那似的，一个一个结果，行那个刚才的工作做完讲完了，然后就是现在想说，虽然干刚才有了给大家展示了各种各样朋友意思的东西，但其实GAN，他也不是一个很完美的一个东西，它是有多方面的问题，第一个问题的话就是这个盖子问他非常难收敛，他不像我们之前去的那种，比如说那种深度学习网络它一个明确的一个目标函数，然后你只要去比较训练下去，他就会没创意越来越低，然后就会收敛的概念，GAN不会，他会那个可能会不断的震荡，不太稳定，第二个问题是你没法评价生成结果的好坏，因为对于一个GAN生成的图像，其实你并不知道你想要什么结果，所以你不你没有groundtruth，刚才是我比如说两个人.脸其他。

但是你并不是我想要的那么长什么样子，所以无法评价你这个这个结果的好坏，所以你们现在有一些基于统计的方法，算一种统计量，但是这个没有量就是那个很明确的量化指标，其实，第二个就是还有更严重的问题就是GAN这东西很难保持一个几何结构，其实这个其实这个网络这个问题其实有那个神经网络的一个卷积产生的，大家知道卷积其实是一个位移不变的，就比如说你一个卷积一个滑动窗口，他其实一个滑动窗口的卷积，他算完之后挪一段位职之后，其实响应是一样的，就是把整个图像挪一个区域，响应也是挪一个区域，那个位置不对。

泡沫网络通过一个输入，然后告诉这个图片真还是不真，所以说，假如我把这个图片的一些某些块把他挪一下，其实你这个D网络可能还是很难判断就判断一个真实图像，你把其中一块磨一下那个地方还是告诉这是还是真实图像，但是这样的话对于图像的一些，比如说有很强的几何结构东西的话，他很可能是他完全变了，是不是就是拟人看其实已经不是这样的网络来看，还是真实图像，这就是GAN一个很大的问题，其实这也是近些年这个GAN这个不太方便，就是要尽可能怎么去保结构保持你生成这样一个结构就是就是现在一直在一个研究的一个论文问题，其实我现在也没找到一个很好的解决方案，然后行然后今天这个是我讲的所有内容。

有什么问题，训练是不是基本上都是多少的每一部分多单，其实现在大家发展方向是尽可能做到端到端的训练就是你保证你的输入到输出都是可导的就是说，比如说刚才那个像我们这两个人脸输出一个结果，然后这个其实这个整个网络每一步都是可导的我们只要把这个数这个概念，后来整个去优化整个网络，然后就可以了，困难的一个训练的问题，现在有什么解决方案，就是说在看loss确实看不出。

GAN之所以不收敛的一个很大的因素就是他那个他，那个对抗的LOSS吗，其实我们先要跟他那个之前最开始那个，这个问题其实问题非常专业，其实就是这个这样的loss function我们知道传给loss的话，一般只有一个minimum就结束了，但是GAN的话，其实是minimum和MAX就是他的两个网络，一个网络是在最小化这样一个目标函数一个网络最大化这个目标函数，其实说在这是这个地方的产生在这个纳什均衡问题就是说你到底在什么点上，这样的一个网络当他想要变的时候，都会产生他不想要的结果，这些是一个非常严重的纳什均衡问题，然后现在一些改进方案就是说，第一个就是尽可能让这个D能够连续这个你的这个分类网记者连续。

之前那个什么W GAN他们那个提出的方式说因为网络的话，它有一个很强的非线性就是你的输变一点的话，你的输出可以变化特别大，他这是我们想要的结果就是我们尽可能保证你的网络输入变一点的时候输出也是变一点，然后整个那个梯度的尽可能的平滑，这样使优化尽可能稳定，然后第二个是我们不要使用log这样的一个函数就是像之前的用户特征匹配，这就是那个什么交叉熵，不用交叉熵就不用做就是把它换成那个什么特征，具体要这种东西，出现梯度消失的这样的一个问题。

 

 

## 12.

其实跟那个就是刚说跟刚才罗冲讲的Personal ID那样的一个问题就是，行人从识别就是他，他的目标就是说我得识别中不同的性格，不同的人的pose下的是同一个人，其实我们这个概念第一个对问题就是我生成不同破处的人，其实整个过程就是他的一个历过程，对是的那个其实GAN是,如果物体的形变越复杂，它那个结构就越差，但是现在有些算法在尽可能提高这样的结构，比如说加入了一些姿态的估计的一些帮助，然后加入一些文体的鲜艳在里面，然后尽可能提高这样的生存精度。