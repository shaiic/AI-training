## 32.

今天主要是上午咱们讲重要的，那个分支的，大家可以回去看我这把特别重要的概念给大家讲了这样能保证这个知识的完整性，不会遗漏一些重点，然后分类是，主要是前面说的是都是线性的吗，所以到了这一层，其实从第四章开始就是一个双层神经网络第四步第五步也是方程，只不过第四步完成的是拟合任务，第五步完成的一个分类任务，所以我们先会用一个二分类来学习一下，然后再用一个多分类来学习一下，这个具体的分类，然后最后再用一个三层的时间网络完成一个那个MINUTES手写数字识别的问题。

先说那个异或问题吧，因为这个太著名了，因为在挺早，六九年的时候就有一本书，那人写了一个Perception感知器，其实现在我们讲到一层两层还都是一个感知器的扩展，不过他，后来从感知器发展成为神经网络，所以现在很多还比MLP这三个字母就是多层感知机的意思，这个人证明了这个，单层的网络是没法做异或功能的，什么意思，就是说要想分屏幕上分开这个点，比如说分开，直接进这个，这个时候屏幕想分开这四个点就是红的两个红的和两个蓝的你，无论如何画一条直线都是都是无法做到的，所以必须用一个，因为那会儿就是六几年七几年还没有那个双层感知机也没有那个那个BP算法，所以就没有人能研究出这个问题了，所以从那开始，那个感知机，这个技术就被搁置了，因此神经网络才后来才后来就是三四十年以后才发展起来的，否则应该早就会被发展，因为这个被搁置了很多人，当时觉得那个人是老大，他说什么话都应该是对的，所以很多投资什么的，在那个单层感知机上就就没有了，所以就会造成这种事儿，我们看为什么要用，先我们先简单的证明一下这个异或问题的不可能性，单层感知记忆问题不可能性。

异或不用说了，就是001什么的，就这些，神经元就是这样的两个输入，然后一个B，然后他的前项工作就是这个东西，一会儿在黑板上给大家给大家给她写一遍，直接看那个他不容易理解，然后看公式1，这个没问题吧，就是从这块来的X1，X2，X1乘以W1然后加上B就是这个公式，然后这个一定要有一个分类，是要有一个logistic函数二分类函数，这个以前讲过，在那个单层线性的讲话应该写logistic不是sigmoid，logistic就是这这俩是一个东西，只不过在不同的场合用不同的说法，然后A再等于logistic这个Z，如果是大于0.5小于0.5，就证明是正类负类，那个样本样本不是这样的，这个把第一个样本带进去S1=0，S2=0，然后得到这个y是什么，是这个标签Y，如果需要A等于Y的话从logistic函数就需要Z小于零，A等于y就是这个标签，A是那最后的结果，应该A应该等于Y，说是正类或者负类才对。

所以从内容上需要Z小于0，A是零，所以这Z一定是小于零的明白这个是吧，因为这个函数不是这样的，如果这个是0.5，如果A要是负类的话，现在是A是不知道哪，A还不知道如果A要是一个负类的话要求小于0.5，A小于0.5条件是，是这个是这个但是零玩这什么，然后要求的Z小于0，所以把这个公式给他小于零，让他等于小于零就是这样，然后再往后看，然后X1，X2等于零，所以那个就剩个b了，所以B小于零，这从第一个这个条件推出来的推出来的是要求B小于零，然后从第二个条件一样，往里带，要求Z大于零，因为这是一这是一所以要求这个是正类，要求在这边，所以Z大于0，这边Z小于0，然后最大的两个把这个带进来什么，这个零点。

然后变成了W2加B大于零，意味着S是零，所以就剩个WX21C大不了加B大于零，所以在这儿写成是W2加B大于零，第三个也是要求W1加B大于零，因为那个S二是零现在W一加B大于零，然后再看第四个，最关键的还是，X1等于一的时候，X2等于一Y等于零就是这个条件，代进去需要A等于Y的话要求W1加W2加b小于零W，那我们把这个公式两边都加B，W1加B小于零，我们来看这个，这个像是大于0的。然后再看这个这项是大于零的，所以加起来怎么都不能都不能小于零是吧，所以这就最简单的办法证明那个异或问题在单层感知机上是不可解的就是这么简单的证明一下就好了，然后这个非线性可能就是这个东西是怎么我以前那个上电路的应该学过，但可能没有这么复杂就说用一个逻辑或门逻辑与非门，与非门是可以搭出一个在再加一个逻辑与门是可以搭出一个异或来的，这就不仔细说了，大家可以照这个自己看一下就可以。

用这玩意儿整个搭能搭出一个能搭出一个异或，这个就是基本的数字电路可以完成的事，所以，从这块我们就可以得到一个启示，就是一个双层的感知力应该能完成异或这个问题，这是第一层数层完这输出成一个双层的感激是应该能完成异或这个问题，看异或问题你能想这个非线性的话，就是说在这会儿画一条线，他就能把那个红色的点给他弄在这个绿色的线左边的，蓝色点在他的在他的右上方，这就算一个非线性的一个问题，看看具体的是怎么实现的。

这是双层感知机基本的一个结构图，两个神经元，输入层是两个神经元，然后把这个隐层是两个神经元，输出层总是一个，因为是二分类，所以它出来是一个大于0一个小于0，所以就是一个神经元就够了，输入层是三个神经元四个应该都OK的都是一个神经元不行，这个异或完全可以用，比如说234567，用多少个都可以完成这个事儿，所以两个神经元是最基本的这个充分必要条件，所以就用两个神经元，这也好解释的，看那个，基本的最终特征值这个X1，X2是两个特征值，横着写一个一行两列的，然后W1是权重矩阵W1就是这个。

这一部分是一个四个元素的权重矩阵，这上面这个一表示第一层就是左上角那个一表示第一层下面一是位置是行列的位置，所以看这个W12就是第一行第二列，那在这里边儿W12就是就是这个W12，所以从第一个向量到第二个神经元，这个就是W12，以此类推，其他的序号都是都是顺着，两个神经元所两个B是一个12，2个东西隐层这两个那个所以他做两做两层的一个计算，先得到Z再得到A，这个A就是那个这块的激活函数，这个A这儿有个激活函数。

她会得到得到一个一行两列的Z再和一行两列的，这也是从一个神经元从第一个时间难道到到处走的第一个警方从第二个成员到输出层的第一个神经元，所以1121，现在这个序号都是顺的一般的二分类网络，可以这样就刚才说我中间多少个都没关系，往左往右多少都没关系，但是要求中间的这个数目至少大于左边的这个数目就说，他的隐藏的神经元的数量要大于那个输入层特征的数量特征值的数量，然后要经过一个，这儿会有一个激活涵数完之后过一个二分类函数，然后用二分类交叉熵，这个东西得得记清楚了，这一个标准的就是如果是那个拟合的话，拟合的话用的是MSE，如果是二分类的话用的是logistic，加上Cross entropy二分类，然后如果是多分类的话使用那个softmax加上cross entropy多分类就是这这个组和这种，这块没有分类函数，因为它是一个拟合的关系一共这个拟合，然后二分类三分类，一共就这么三种就别搞混了。

前向计算是双层的，所以前面都见过都见过这个WX加B，然后出国最再让过激话函数，然后A在当地二层WA加B出个Z二，用了这个logistic一个二分类函数出一个A2现在就这个概率是零到一之间的一个概率值大于一般认为大于0.5的就属于正类小于0.5的属于负类，然后再经过二分类交叉熵，输入一个loss，这个loss并不是每次都要计算的，因为在迭代过程中，每次都算loss就是太浪费时间了，因为loss那个值算起来比前面那个要麻烦一点，所以要每隔一段时间才算一次Loss。这开开始的时候总想不清楚，自己用用笔去推一下，以后就熟悉这种写法的就不用再去仔细地写每一个东西就特烦，但开始的时候，真想不清楚这些我们一定要自己推一遍，然后我这也分开写了从Z1到A1到A2，实际上一个矩阵就这东西，A1等于Sigmoid一个大Z1，第二层跟那个没啥区别，只不过它是一个单输出，刚才那个还是一个一行两列的双输出，这个是一个单值的输出，它是一个概率，所以就是个单值的输出，分类函数这俩一个写法一样的他都是一个东西，所以跟这个没什么区别，如果在这个logistic矩阵计算的时候，他这如果进来是矩阵输出也是矩阵，为了避免一些麻烦，我们一般的都用都用矩阵的方式就那logistic上的不用第一种方式，第一种方式是分解开来，给大家看的清楚一般都用矩阵的方式去做这样速度比较快，然后损失函数的二分类交叉熵就是这个样子，y,1-y，然后A,1-A，我这块少了一个括号应该在这块儿有一个在这边有个中括号，反向传播我们上回讲过，我不具体推公式，但是我把这个过程跟大家说一下就从这边看来，其实大家都理解一层的时候，就没有这个时候就是就是从这儿到这儿，然后多了一个激活函数，先看这边就是一个单层的网络，我们先不管这个这条线，单层网络在前面学过一个线性分类的时候，关键就是在每层网络的时候都需要，不管你多少层了，delta就是那个误差的反向传播的矩阵，传过来，这样做，所以要先得到这个的，值得到delta值。

所以在这个地方要想得到，你看要想解W和B到Z的反向的时候，你好像求到这个方向，这个反向要求A到Z的反向，如果没有后面的话就是这个前面是完整一个单层的网络，多出这个来就把这条线引出来把后边再看成一个单层的网络就OK了，所以这个一点儿都不难，无论多少层网络都是这么计算的，所以这个delta_Z在里边那个的就相当于这个地方也有人把它看成是这个地方就是，激活函数前面的delta还是激活函数后面，的我一般愿意看成那个激活函数前面的，很多教材会用后边那个，这个要注意他具体怎么写，那个计划是放在什么地方，你这把它看成delta也可以对这个完直接就能取这个把这个看成一种就是跨界函数看成一种过度反应传播的一个过渡一堆公式，让他就在那个求反向的时候，看那个W2矩阵是什么样子的，是一个两行一列的，所以求W2反向的时候，你要把这个标量的矩阵是没法求导的或者。所以要把他那个分到向量里边就给他变成一个两行一列的东西，这样就可以求导就是就是，本来这个这个向量吗，所以把这向量拆开，然后把那个一模一样的放进来，这就是一个标量对向量的求导，对每一个在拆的时候，长得样子是A11长得什么样的，是这个样子是个横的，在这儿，是一个竖着的，所以我把它写成A1T，A的转置就A1本来是一个横着的吗，A1A2。

A1T就是这么来的，如果对标量求导的话是得不到这个结果的，所以是对向量或矩阵求导才能得到A1T这个结果，所以在这个公式里面，是把那个拆开了loss一个个带进去就做得到这个WT是能看这个WRD也是，W2本来是长什么模样，W2是一个竖着的一个也是一个两行一列的，然后我们看这块那个W2，所以有横着的T，T都给他变就是就是，本来达不到横的把他T，W2本来竖着吗，给他一个那个TRANSPORT以后就变成横的了，自己回去一定要推一下，咱们上课不推那个推没得意义，自己要，理解，然后这个东西是比如说是一个那个element Y的一个点乘， A1本来是这个模样的吗，然后你这个A1A2，然后这边是1-A1,1-A2，这俩要乘的话，我就用点乘这种东西，然后最后整个的一个dloss对dZ1。

 

 

 

## 33.

变成了这个公式，这个反向也是理解神经网络怎么work的一个比较基础的东西，就是说你这个东西，你懂和不懂，还是有一点区别的，要是亲手推一下对以后所有的反向你都不需要再推了，懂，不懂还是有点区别的，哪怕用数值的方法去推一下对自己也是也是有帮助的，这样理解就是比较深刻，以后不会说脑子里一团糊涂，他反向到底怎么做，你亲手推一下就知道他怎么做的，只要推了一两次之后，你就再也不用推了，这就是相当于一个理解的基础上的一种记忆就是这样对学习东西是有好处，而且这些公式保证都没错的，不要网上有好多或者写的特别复杂，或者是用特牛逼的一些符号，列一下，然后就什么都看不懂的可以大家没事儿可以找网上找一下，就是所谓神经网络的反向传播四大公式的推导，这个跟刚才那个一模一样，我们看看这这个什么意思，这大概说一下，这个是激活函数，这个东西是sigma，然后这个就是刚才刚刚说的激活函数A到Z输出完就就这个过程，这个过程是这个Z的上一层的这个字了，这是上一层的误差，然后在这世上的上层的一个误差，还得到一个本层的，这好像应该是X+1吧，我仔细看我在看的是不需要那个X+1，他这个先得到一个本层的delta，就刚才说的这个玩意儿得到一个本层的一个delta，这个delta是已经没有。

我们画细点就是说这有个激活函数，再进来，然后，这个叫delta，是已经没有激活函数了。，经被反向过了，这儿有个WT，WT，刚才我说过那个那个W转置怎么来的，得到下一层的一个delta，然后根据这个delta再去解B，W看到现在，这就基本看不懂，他这个东西这都要什么意思，在这一点都看不出来，其实很难理解这个东西，你要把那个分解成一个个小分类。

然后就比较抽象的讲一下，这个实验，刚才有那个神经网络结构图了，已经没问题了，这个实践起来很容易，这个大数据的话就是往里写八个数据就好了，上面是上面那个八个东西是组合下面那东西四个是Y，然后别把你变成四行两列和四行一列，然后这里边，由于我们这个用了这个对他说的这个东西。

还要跟大家说一下，前几天在北京的时候，把所有的代码都整理一下先跟大家说一下那个新代码是一定要UPDATE看一下那个新代码是那个命名机制是什么，因为带版本2，有利于大家那个一步一步的学习，比如说，看刚才那个是。

左边都带版本号版本号这个二一的意思就是，2.1就在这儿会有working2.1让他为每次都说跟那个2.0，比如说改良东西，这样就是，其实要是对PYTHON不是特熟悉的你用这玩意一步一步的也能也能熟悉PYTHON，PYTHON的一些从简单到复杂的搭建一个工程的过程，是说明这个，比如说这个WB值是1.0，1.0就一个最初版一张自主版，这种版本号递增，这样跟大家说这个然后看咱这里边引用都是比如neural net，所以这个命名规范化都统一了一下，这样就比较容易看这个看看那个这个我会在再把他弄醒，因为现在比如说datareader想起这个现在其实那个叫DATA2.0，这个是，因为这个是这个DATA是只有只有这么四个简单的，四行，所以我们要遵循这个DATAREADER积累的一些规则把没有归一化的这个XTrain归一化，就这点区别就把他的没有的规划以后的东西都都换成一个，因为他也他做规划归一化都一样，肯定就是这四个数，这是为了继承这个DATAREADER里的一堆方法，你就不用，所有都照顾到了，因为他有时候是要需要读那个Xtrain raw那个数据，你只要把ReadData这个给OVERIDE掉就就OK。 然后把test和train搞成一个，因为这四个数据，没法儿住再分开了，这个其实没多大意义，这个就把那个validation，因为那个我们现在用两层的神经网络后面所有的那些代码都需要做validation的，每走几步就做一次验证，所以把这个validation也设置成XTrain，这个是完全是为了不改更多的neural net那个类的那些逻辑，而把这个数据上做一个小小的一个手脚这个不影响正常使用就行，但实际上我们真正的数据是不能这么做的，真正的数据，一定要分开的，

1000个的话就是100个是validation，900个是训练样本100个是验证样本数量的一定是一定是分开的，而这类的都是混在一起的就四个数据吗，就是为了那个说明一下，为什么有一个测试函数，这个做完之后就你在这个其实要二分类的话，这个大于0.5你在这个地方她也叫大于0.5，在这个地方也叫小于0.5，在这个地方有一个量的概念，我们就想把这个分类给他做的稍微极致一点儿就是让他的误差精度误差的这个精度要求是小于1e-2，本来这个零一是一，然后你算出了神经网络直他肯定不是一吗，他肯定是一个什么0.9，0.8这种数，所以我们要求是，比如说0.98达到这个数才算他们满足精度要求，让大家更深地理解这个一个神经网络的精度一个什么迭代次数，就这些东西，就是网络一定出来的不是一个数学解析解，不是01这种东西一定是个概率，然后这个概率要用为了尽量的靠近真实值，所以我们要提提出一个精度的概念，否则你训练那么两三次估计就OK了，但是现在这个叫训练这个比如说9000次。

有一个精度要求，这种要求小于1e-2的时候就是，或者是算出来是0.02，就是负类，那一减0.02的时候，就算是就算是OK，然后这做了一个小的一个计较，就是说他不是一个一个算他四个四个东西，一次是取那个四个数据，刚才我们已经骇客一下把所有的数据都在该在的在在这里本来有看这这个这么写，就是因为不改其他函数， 为但是为了我们因为没有，反正那四个数据，没有XY我们在这弄给他付了一个那个test raw付成train，就是test一模一样的数据，你就不用改那些函数OK，然后，主过程代码就简单，一个是readdata，然后get validation这都是形式化的，一定要每次都这么调，然后input在里面赋值了2，1所以那是一个二输入，然后两个hidden，然后一个输出。

调参就先用0.01到0.1之间，去调就好了，这基本上就是这样，然后，batch size 1因为一共就四个数据，一次用一个就好了，能够保证你的这个意思就是loss 值小于0.05的时候就停止迭代，就这个后面只是就是说0.05的时候，能保证你的精度小于1e-2，所以有这么个0.05的，一般的情况是没有这种估计的，没有这种东西，就是为了让他停止迭代，然后超参就把这个东西都兑进去就OK了，然后是一个初始化给一个MODEL的名字，然后开始train，我们看看他train的过程是什么样的。

第一个因为数量少，所以很快你看他这个线都很奇怪，就是因为数量太少了，所以好多都是都是跳跃的，双层神经网络以后每次都是看左边右边这两张图，左边是loss，右边是那个准确度一定要习惯看这两张图分析他具体什么意思，这个不太容易看，因为数量太少了，我就不说了，我们在后边儿等到数量大的时候再去解释，然后再看这个精度，四个精度是可能看不清楚，但是念一下这个0.04肯定就是跟零相比是小于一份儿的，然后这0.990.990.04。

所以，然后做一次那个REVERSE做一次阴PRINCE，然后这个矩阵减得到的AR假设那个AR就是刚才那个A，二是等于0.040.990.9940.004那个万事等于0111104就这用这玩意儿互相一减，我现在减以后，后边是沃尔地府小于一衣服，二那是那个NPA，一实际上就是一个出货就是0.04减去零它的觉得它的绝对值是不小于1亿富二，如果是的话就给各处，所以最后得到一个矩阵true的这么一个矩阵，如果其中有一个不是的话就会变成FALSE，这块对不对能否，然后。

用ResultSum把这个数加起来，这相当于一加起来应该等于四吧，如果他不是的话就看他是不是等于四果等于四的话，他们四个都是true，如果只三的话就只有一个FALSE那，大家善于利用numpy里的矩阵的一些东西，你要是不用这种方式的话，然后一个去判断，要用这种方式的话，再大量的矩阵运算的时候速度会快很多精度能满足要求就这样，我们随便改一个东西，比如0.05，可能是什么结果，反正很快就出来了，然后其实长得模样还行，又不是特糟糕，其实，但是他，他自己的时候该怎么有test的结果我把这关了可以这种，否则我们看这几个数没有一个是想要一副20.030.9500:04你要说光看那数值也OK了哈0.03很接近于零，零点九五很接近于那个一也OK了，但要求他精度要小一房，打印输出打印那些那个WB的值还是挺有意思的，你看这个W两个副两个服务区了。

是那个W和22的吗，是两个夫妻俩种都是对唱的这个地步的证，然后这个W也是对症的，然后付12那个壁纸六，这个讲了刚才那个地方地震零点这个每一个都是小樱花，OK逻辑抑或门它，它的工作原理是什么，这个理解的工作原理是对于对于神经网络可视化的一种就可解释性的一种一种重要的手段，首先显示原始数据象人族舅舅这里，但你要写一些额外的扣去去干这些事儿，这些辅助性的客户的，也是很有必要的，然后先显示四个点来这然后显示中间的推理结果就是刚才。

我有有那个1234不是有有四个步骤吗，就是在一等于什么，然后一等于什么字，二等于什么，那个什么然后我们把这个这一ACAR的都当成一个，一个在这里边儿这一是一个有两个A是两个，然后Z，二是各有一个，所以把这个这一是一个一行恶一行两列的数，所以我们把这个这一行两列给他变成一个X一个YEXEY这么看，然后把这个A也是看成XY，然后这个CL不都是一个标量吗，他是没法做的，所以把自二看成XAR看城管怎么看最后能画出什么来，最后得到是。

这个字仪式开始的时候这样四个点都在那个这个分带俩分在四个角上，然后到第二步，这几个训练好了，看这怎么变成三个点了，是因为中间这个点重了他因为给他非常那个看那最高值也不大是已经是已经是看这个零一码零一区间，这已经是这个零到负12，正四到-12了，这也是这儿报道证实了这个值已经特别大了这个点重了这个红色的点，他会把两个蓝点尽量的往两边压把那个红点往中间压，然后再往下看，到到经过计激活函数，这个只是一个线性变化，这是经过了第一个线性变化，经过这个线性化，把这个X变成Z1的过程是这个.这个是X这这一所以第一次他说他是干了，这便是然后第二次再往下下一步的时候，经过那个sigmoid的时候，他这两这这个是不一样的，这个我给他放响一点，这两个蓝点还在那儿，然后还在那个角上面那个红点给他压到那块了，压到这个激活函数以后变成这个。

 

 

 

## 34.

你算一下就好了，你把这个比如这个值是-4，-4，你算一下Sigmoid是多少就变成00了，基本上因为从Sigmoid的激活函数看，如果是复式的话，就这个地方不是这张图，我就拿他举个例子，他基本上外置就是零，因此这个无论是这个红点无论是X坐标，还是Y坐标算法以后就基本上靠近零，那就基本上跑到那个角上去了，刚才在看这个点这个点是负12正二算一下Sigmoid的话，它是往两边分的就负12基本上在这边是靠近零的位置，正2是靠近1的位置，会映射到这个点是个一零，所以是，中间的两点压到左下角，然后这个点到这个角基本上就是这个计算过程就是这样，然后再往后看，再经过一层logistic的时候那两个红点就跑这块来了，然后那个蓝点的就跑这来了，这会儿是Z2到A2，这么算出来的，所以，完全给他，这个不是用这个这条线是后画上去的是为了掩饰说，最后经过那个以后这四个点应该是在这条线上面是为了掩饰这个不是说在线上画了四个点那个那个线之后话就为了掩饰他的拟合程度。

他是划条分割线是干什么，我们一直好奇这件事儿以前我们学线性的时候他两类之间画一条直线，来做这个现象分类，这个是想说明开始想怎么画这个想怎么用图形化方面展示这个因为最后神经网络是一个概率，logistic是一个0-1之间的一个概率值，所以，我们把整个的一个比如说一个0-1一之间的一个网格给他比如分成多少分。

然后每层不都有整点吗，这就形成一个整个网格的一个矩阵，然后把这个每一个点都算出来logistic了，这才出来，比如说这个点的坐标是是0.1，0.01，logistic结果之后，这次有结果以后它的横坐标本来是一四就在网格矩阵上是零到一，是0.1到0.4，然后他的概率值就把她带到我们刚才神经网络已经train好了把0.1，0.4这个当成X1X2给他带到那个inference的过程里。

之后会得到一个A2，是一个标量是一个概率值是0-1之间的一个概率值，然后得到比如说他0.01或者是或者是0.2或者是0.9就是所有的这些.100x100个，这个点都得到一个概率值完把相同概率值给他化成相同的颜色基本上就是这个意思，他这个是说把大于0.5的网格图成粉色小于0.5的涂成黄色，就是0.9的话，那这个点我就给他弄成一个黄色，然后小于0.5的话，这就是一个粉色，最后得出来的样子就是这个，看这个，他这个挺清楚的哈，他虽然聚酯聚酯是因为我们取50乘50或者100乘100就是你没必要取的太精细，他就会算量特别大，所以他把这个两个蓝点就放在这个黄色的区域把两个红点就放在这个红色的区域，他就是靠这种东西把它这个异或问题就解决了，表面上看她是画了两条直线是吧，两条直线来解决一户你实际上这不是两条直线不是两条之间是一个带一个，这已经不是平面问题了，所以绝对不是直线是一个是另一个子平面把它分开了，我们再看看这个子平面是怎么形成的。

这是因为是一个50乘50的网格，所以有马赛克，我们可以先看看3D图，这个是我们前面写过的一个拟合那个用一个平面拟合一个房价的样本点，所以说我们在这里看看能不能用平面来做这事，于是有了这么一个想法，这个挺重要的，这种方式一会儿再说他具体是怎么实现的，我们先看效果，不然没有没有概念，所以效果就是这个模样，这个非常fancy这个东西了，就是说他把这个概率值，首先说这个蓝色红色，这些都是概率值，因为，因为我们刚才做了一个ETS等于0.05就是这种精度要求的精度特别高，所以中间这个过程就这个东西非常陡峭，你要是0.05，比如说不管是最后得到0.95，0.4，0.3这个都不会很陡峭，它都是一个斜面慢慢上去的，包括下面那种鞋面这个斗越陡峭，越能表示你的分类精度越高，就是这样，但是有一个另外一个问题叫过拟合问题，这个我们以后再说，现在跟过拟合还差的远，我们先让他拟合的非常好，这样能够有助于对问题的理解，你看这个，这个红色这两红点这红色的一这个蓝色的零中间这个，青色的就0.5，这是慢慢过渡过渡就是这么来的，所以把它整个一个看上面从上面看的话，他说这个模样，这是刚才我们看的那个粉色黄色，那个图就好像画两条直线，他从上面看是两个区域的叠加，一个是黄色，一个是蓝色，中间有个过渡带，我们刚才为什么没有，刚才因为是用了0.5做分割线吧，0.7，0.8怎么什么0.3，04都都给忽略掉了，所以刚才那条那个黄色粉色那个，是两条直线，实际上，中间是有一个过渡带的，那些东西全都集中在这，如果精度低的话，你就会看到这个过渡带特别宽，可能有这么宽，如果精度高的话，就越来越小，最后可能是不可能变成一条直线了，就是特别特别窄。

所以从上面看他就是这个模样，然后我们再看怎么分，就是说你在这个这会儿用一个平面一切他就把红点分到平面上面，蓝点分到红边这个平台的下面，所以这是一个不用直线拉一个平面，一个平面去切分那个红蓝两色点，这就是一个基本的原理，这个图还不算完了，因为还有一个更好的一个展示方式就一个2.5D现在我们看这个图是一个，这是一二地图，二迪已经能帮我理解很多了，只不过这个很不如说清楚他为什么两条直线实际上是一个带不是两条直线，然后3D图已经帮我理解了，什么网络可能是这样工作的大把，他把两个红色的，就在屏上不能分的那些点给他抬起来，然后把那个负样本给他压下去，这样他就可以分开了，看一个2.5D图的展示方式，他是这个样子，就刚才我说的那个过渡带就是中间这个，这个这个图怎么画以后大家稍微稍微说一下画这个都挺重要的，这个探查一个训练的过程，训练过程什么意思就是你当迭代到500次什么，最后，因为这是迭代2000多次才做出那个结果这训练过程有助于大家理解，就是说这个点是怎么样往两边扩，比如说最开始这款这个是不一样的。

下面的坐标值不一样的开始的时候，开始的是在那个角上，然后第一次500字的时候，你看这个蓝色点在这两个点，这1500这所以他实际上是把这个那个蓝色.1直往两边撇一直往那边比这个红色点是往中间压。

然后最后最后到那个地步的时候，一个红色都重合了吧，所以你画一条直线就能给他分开了就是前面的干这些事都是把一个非线性的问题转变成线性的，然后分网络的最后一步，就给他一刀切给他又变成线性问题，去做一个线性分类，然后在看迭代次数跟分类结果的这种演变的，分类结果就是开始就是用2.5d图的话什么都没分出来，其实就开始500次的时候，我在看这个这些点，也是都集中在那个零点五附近还根本没分开。

他稍微有点趋势了，这个蓝点，大于0.5了，这两个红点，小于0.5了，这个分类图就是这个样子，然后这个很奇怪，看这个这个蓝色的点怎么这个又跑到那边去了转到了红色点的集中在一起了，这两个应该是对这个是两个红色的点红色点，这是两个蓝色点重合了应该是这个应该是，两个红色的吧，或者蓝色的，应该不是这样，就是说，这个应该是两个蓝色的点就是非常接近，这是两个红色点。

大家看这个意思吧，就是说他在他在Sigmoid这个曲线上是一点点往两边撇的，最开始从集中到往两边撇到撇的更大，最后到一直由于精度要求吗就压到那个特别大的位置，对应的右边是一个最后的一个分类结果他刚才说的精度不够的时候，中间的那个分隔带就特别宽特别宽，然后精度够的时候，那过大就特别窄，就这个意思，我在看隐藏神经元刚刚说至少用两个神经元现在是，如果一个神经元你看能得到啥结果，啥都得不到，为什么不一样，有人能猜一下是为什么，是因为不是说过有一个初始化的时候给他写成CREATE NEW的True等于FALSE有一个参数等于处或者是或者否，如果现在如果大家跑我这个code跟我看到结果一定是一样的，因为把所有的参数都设成false了，也就是说，你跑出来的初始化结果，跟我跑出来的结果一模一样，所以他的结果也是一样的就这意思，这个问题完全是因为初始化的问题，就在这个神经网络里，比如说有一座山，这边红色的这边蓝色的可能是这样，所以就看你出的话，如果出的话，再在这个点上这个点的话，你就走到右边，如果在这个点上就走了左边就是这个，所以网络都解就是说多个解的话，他就会，红蓝给他就会，但有一个问题，那个红蓝是没有颠倒，好了吧，这个红色对应的蓝点吧，这个红色也都蓝点吧，这个红蓝是没有颠倒了，只不过他的形式钓钓过了一下就是一个蓝色的带红色带，也就是说，刚才那个起来了门，那个门是上面的零还是下边儿0的问题。

就这个问题，仔细看这个图，蓝色的用对红点，这个是，蓝色对应红点红色对应蓝点是红色的蓝点完全有益处的话的不同造成的，难道看这个四个神经元的三个也OK，也OK，这个就是说，就像两两只蚂蚁干活吧，只蚂蚁干活儿，这个可能就会出现一些，扭曲的一些结果就是说这个团队人多了，也不是好事，基本上就这么解释。

有一个基本的概念，就是说，不是一句话吗，就是开始的时候都是一个sample都是一个样版，也不要开始想着就能够把你这个模型，或者是一些抽象，完全能够match到所有的现实生活中一定要说有至少三个样本或者是三个实例以后，你才能把你整个软件给他变得更加，更加符合于，各种条件各种各样的条件，基本上能覆盖所有的任务就七个不一样的任务，这个软件就是非常非常成熟，大概其实是有这么一个概念，因为我们做一个做一个answer的东西就是在bing里边。

 

 

 

## 35.

过度设计这个玩意儿，这个正好讲完了，所以，这一些总结。

两个神经元肯定OK，要轻松一点，一个神经元就不行了一个一小于二的他，两个飞车吗，然后更多的实际上，实际上，我们看他迭代了多少次，差不多都4000多次。

一个神经元是什么都做不到，两个稍微就是两个地将好的完成任务，但那个是，就跟人力资源配置的就是这个事。人多了，人少了什么的，你人少了就费点劲，但人太多也不行就反正。

只能这么解释，那四个的时候，他也是完成速度很快，4300次，然后再多了不一定快乐就好，互相搞内耗了，这也调经调那个就就反应的时候他无那个参数多吧，差不多就花的时间长就计算着花的时间长。

所以他会用更多的那个自助就调参数各参数之间的关系，有一种内耗。

 

 

 

## 36.

线性非线性，大概就是直观的理解，像异或或者弧形问题都是线行，不能解决的，所以我们用图形二分类，刚才那个异或已经讲完了，所以我们要用同样的网络，同样的条件，，因为双型号分类，它是从头上看他只X和Y两个坐标，所以那个NUMBER of feature还是二，然后他那个shuffle的意思就是，那个输入的数据要要给他打乱一下，然后这个用的真正的datareader标准就是没有，一个train文件一个test文件的，然后这个Normalized是把它标成0-1之间玩这个东西是要把输入的train的部分给它翻成9:分训练集合和一份验证集，这个测试机指标动一直放在这儿，就为了测试用的最后，最后只能用一次测试。

然后这个hidden layer而是隐藏在数量可以是大于二的任何整数这跟刚才异或都能完成这个工作，这个月开始下降了一会儿，蓝色的是train，因为每次用五个样本吧，所以他这个波动非常大，那个validation我看这个size有多大，至少100个左右，所以他是是非常平滑的一个曲线，这个这个红色的，中间的一段他会有一个鞍点。

或者叫做在平面上叫驻点，比如说一个山坡，他不是这样的那个山坡似的，他是这样的，这样的您如果在这儿初始的话，他她祈祷逛荡逛荡逛荡逛的话就忽然找了一个那个坡下来就是它的那个loss曲线可能就这长得这模样给他找半天才能找到一个下坡的路，accuracy跟那个其实基本就倒过来的关系，我们看他每次干四件事儿，一个是loss train，loss validation对应的上面这个红蓝红蓝就对那四个东西，这个LOSS Train是什么。

用这五个样本计算一次loss 值，然后这个是用五个样本计算一次精度值准确度值，这个是用那个validation的一比九，那个东西假设100个，100个样板计算一次loss值，计算一次准确度值是这样，最后实际情况就是，真正的功能里不会有一这种东西出现的这100%就是太邪乎了，因为我们train的那个数量比较多，这个次数比较多，一个是样本量少，还一个是说我们样本基本上没有什么噪音，因为是我们自己做的样板没什么噪音，所以才会有一这种东西出现分类结果完全正确就是这个意思，然后这个是0.007的时候结束了，因为我们在这块儿。

最后W，B值是这样，我们跑一下，看看他是什么，我刚才这个是这大把演示了那个不同的而不同的hidden layer就是什么异或的两个三个四个像这个当成一个参数传进了一个神经元两个神经元三个神经元，这时16一直跑下来可以自己跑一下这个能够得到那个那个123456得到那个六张图，这是解释了刚才我把这个跑一下，因为这个不会花很长时间。

最开始的四个样本，异或是要么在四个角上，然后关掉它，然后这四个样板就会中间这有俩大眼看着吧这两个样本，然后再关掉它，然后这个是经过Sigmoid以后这两个样本给压缩到这个地方，然后那个另外两个样本是在脚上，然后再看这个，这个图我为什么说那是叠上去的，仔细看这个放大以后，首先它是它是两个样本，其次，它不在那个直线上，不在那个曲线，是两个图，这个图能够完全跟那个Sigmoid，这次的曲线是重合表示这个意思，反正也是她，她是不在那个直线不在曲线上面的。

然后这个是要画一个图，因为画那个图就是那个锯齿形的那个玩意儿发这个图很慢很慢，是2500个.2500点冷算一个个算那个概率，如果是大于0.5，就画成这个颜色写0.5化成那个颜色，所以最后就是每一个其实都一个小方块或者从这块儿或者一个锯齿，然后你再看这3d图很快为什么很快的，因为他用了一些标准的一个方法, 3d必须有这么一个库。

开始引入， XY跟刚才那个2d一样，也就是分成50乘50在平面上50乘50是XY，看这个PREPARE 3d data在这儿，X零到一之间，这个COUNT是50兆给了50吗，这块所以看到50就是领导一直年画50个，这样一共是50乘50，2500个应该网格，然后这个match grid实际上是形成了一个，就是形成一个2500，这个match grid的就是把这个XY 50 50合在一起，然后这个是一个。

50乘50的一个矩阵debug一下，看看是什么，然后这个Z，就对于每个每个XY点这个这点就所以形成三维吗，所以这Z应该是一个，2500的一个向量，然后进了刚才我们那个inference，就因为那个train好的神经网络的那异或肯定也是一个2500，然后最后给他Reshape成50乘50，所以说X是个50，所以最后就会存在返回那个，这个函数是PLUS一个SURFACE就是画一个曲面，这个SERVICE，这个画四个样本点，

刚才不说不是有另外一种斜的方向，所以另外一个方向可能就是就长得这个模样，只不过红色在上面蓝色在下面，另外有斜的方向有这个，另外那个2.5d图是在代码是在这主要是那个叫contour就是等高线地图地图等高线

相当于是把刚才那个立体图给他拍平了，平了以后，当时用那个等高线的形式在这儿展示，这个挺有用的，以后好多图都是都是用那个contour化速度要快，然后又简单还是在平面上面这个level2，这个异或展示刚才整个训练的过程就是从开始挺密的，然后一点往两边扯最后一直扯到扯到两端，这是展示的那么一个过程，所以它是用5400画的图1500次画的图500次以上2000画个图。

双弧形这个玩意儿，这图就不用解释了都一样，他到这儿loss值是0.03，这个是0.07的时候，这还是0.08，然后到这是0.07所以就停止了，接下来看一下跑很快。

辅助函数就画那个重点什么的那个，这三个画的就是在平面上画画，那些点还算，因为咱们归一化X1X2归到零和一之间了，变成一个方的，所以我们班用50乘50这种数，这是画一网格这个网格不一定是正方形的就有可能是扭曲的就是根据网络线性变化和和做sigmoid激活函数，压缩以后有可能是扭曲的一个情况，这个挺要技术含量的吧。

理解空间扭曲的一个关键的函数，这显示原始这是空间变化，结果受到训练函数一样就不说了，这展示了20次迭代50迭代100次迭代到600迭代一个过程，OK在看那个开始原本样本就长这模样，你看那个网格现在挣的就是初始的时候，其实让是把这些网格也当成样本点了当成就是这些网格每一个点，这一堆.50，50都没点当成跟这个一模一样的东西，就把他也当成一种特殊的样本点，然后在后面计算的时候，比如说这个这个红色通过空间变化以后最后整个样本他比如过来了，变成一条直线了，那个网格，本来是直的，再过他都会变成一个曲线，所以那个网格你就给他看成看成样本点，只不过保留了，互相之间的拓扑关系，20次训练的时候，他是长得长得这个模样，首先，有两个要注意的是他是平的，他只是一个现象，就是整个东西给他推倒了一个大厦那个倒塌的那种，那种感觉。

然后到这儿以为是激活函数，老说是带来非线性这个效果，实际上他就是把他这个这个拉了一下中间变成一个弧形的两边上下并没有变，我们看这个横向的东西还不是弧形的只是这个竖向的是变成弧形的了，然后50次的时候，这个没有窄，而且长了以前是这样的给他，特别长乐的坐标值变了，给他拉长了到这弧度就比较大，然后右边那个分类结果一眼就能看出来，第一次啥都没干到第二次的时候还是还是一个平的就是什么都没分出来，然后345的时候，这个越来越就往宽里拉，然后还稍微往左边拧了一下，到这个地方时候，这个竖线和横线都扭曲了这个横线就是。

 

 

 

## 37.

稍微小一点，从这网站不去书架，你是往两边扩的横线一直是处在这个地步扭曲这地步，这是被激活函数干的，就把其中每一个网格点就扭曲成这个样子，然后再把这些就比如说这个网格这样的吗，我给他联系下这样的还经过线性变化几乎跟着线性变化，以后就变成这个样子，然后给他联系一下就变成这样了，再经过激活函数可能变成这个样子，就这少画两点，比如我就把这个连起来，他就会变成这个样子，所以就变成一个，然后这儿，如果有一个点，

这个网格的意思就是这个，到这儿的时候其实已经可以用一条直线给他给他分割开来了，所以看到这块还不行，这个任何一个线都没法给他分割，来到这块儿时候这条，黄色的就是和橙色的那个线已经能把这个蓝和红给他分割开来，这也是能分割开来，600次达到这个精度要求能完成更多就是要求的精度要求就是。

这个过渡带完全在两个样本之间不会把这个不像这个，你看还骑在这个样本上面就是一个精度要求让他过度带尽可能的窄，这大概就是这意思，150次的时候，就是第四步的时候用一条直线也能分开了，反正如果有兴趣的话，你可以把这条直线你给他返回到那个原始最高点里边原始坐标点就那条黄色的分割线。

看一下真实的效果，徒步编造的是真的话开始，然后是第一步是200次， 20个epoch就长这模样的，激活以后长这模样，然后分类结果是这样，然后这个是50次矩阵变换，空间变幻长这模样线性的，然后非线性的长这样分类结果是这样，这个是100次。

还放了一些测试数据，我们看到前面的时候到150次的时候是99%，就是跟那个图是对应的那个图150次是大概这个意思，就这个还没有些样子，有些样本还骑在中间那个黄色的线上一会儿蓝一会儿红的大概其实这个99%，然后是100次的时候是85%，也就是说这个图100次的时候，别看这个图分的不咋样，这还85%，所以别觉得80%多是一个挺牛逼的数，80%多只是一个大概的效果而已，从这些站能得到一个初步的一个认识。

双弧形就是大概是这个意思，我们看看代码，有没有跟大家交代，就变成20x20，然后看X值是这个,y值是这个大X就是实际上是把XY组合了一下还算独立一下，然后这个大概值也是把QQ华夏，不过有一什么规律，自己回去看一下，每行都是20个这种数，所以XY就可以理解成是一个网格， 

这个就刚才我说的过程就画这个的过程自己回去看一下，最后这个OK这个意思就是画灰色的线，然后线宽是0.1，然后用那个横线用那个横线置换大概就这意思，这个画一堆了就是一共画400条线相当于。

现在看看多分类，有一个铜钱孔，所谓中国银行也是做的数据，这是为了用例子简单例子来说明神经网络的功能时，实际过程中可能工程中没有这么好的数据都是乱七八糟的数据，这数据明显能看出一个外圆内方就是三层网络能不能画出一个方块和一个圆圈。

蓝红绿三个类别，然后分别是类别就是123根上面区别就是它是一个多分类的分类，你这个其实你在外面再画一个再画一紫的也没关系，他这个反正三分类了，四分类也没区别，我们就用三分类来举例好了，然后是负0.5到正0.5，这个开始是一定要看一定要看就没法图形化展示的话你就用那个什么NP.MAX mean去看一下这个数据分布情况，然后决定你要不要做那个normalization不是相信可分，然后所谓红色红蓝两色是个巨型有规律，但是规律这个东西是不存在的神经网络里是没规律的。

所以你不管这个东西是个巨型也好是个什么锯齿形也好，对神经网络来说一点关系都没有你，这个形状越复杂，可能对他越有利，就不要考虑它是距形还是圆形，这种东西，这也说他是边界是没有意义的，要用概率来理解，然后这个说一下多分类模型的评估标准，这是举个例子就是交叉的一个矩阵，比如说，一共是100个样本点，.100个样本点吧，我有90个类别一的分到了类一，有四个是类别一的分到类二，那对于类别一来说就是90%的准确率，然后当然对于所有类别来说就是89.67%的准确率就是分类的分类就是这么算的，当然你可以具体的算什么，每个类别的PRECISION Recall，但没多大意义，有意义的地方在于假设，有一个类别，比如说那一类比较都OK，就是那个三特别奇怪，就是整个影响你的，你最后老得到70%多完之后一看这个是90%， 这个数95%了，这个是为什么不是不是90%以上，然后你就自己看一下那边的数据，然后再看类别二的什么PRECISION recall就干这事儿用的就只有看那些，不就是出乎意料的那些类别的时候才具体看每个类别的PRECISION recall三个类别，我们，这个用两个神经元好像能不能搞定，主要要大于这个玩意儿就行，然后把矩阵形状都列出来了所以W2就是一个三乘三的矩阵，然后前向计算跟刚才一样，只不过最后只带了一个交叉熵，一个多分类的交叉熵，前向计算的过程就是这样，他第二层生成了一个一行三列的东西，所以他就会是一个三个东西，如果单独划的话，我们就熟悉话属于的话，我们就看这个大的矩阵计算哈，这个是为了推导方便给他分开写，前面讲过了。 

可以看到这个左边的跳的2600，2700是100 100的跳，这儿用了三个隐藏，这个意思就是说这半个随便起的中国银行把这两个输入三个，三个隐藏三个输出得到的结果是这样画出来，这个原来可以就这个东西真的比较糟糕比较多就好。

这个0.952是比较低的，这个不是说网络能力不行，也可能是比如说我们用只用了三个神经元，然后另外循环次数没有那么多，我们可以再用后面还会说怎么改进这个东西，多train几次有可能好有可能，这个loss值，有可能是这样，就这个loss值在网络能力不够的时候他最后就平的了，一直不下去，一直不下去，然后最后你可能也就得了，其实你看从这条线已经能看出一些端倪来了，同这条线你这个看不出来下不下，这基本上都平了。

这个网络能力已经不足以支撑这个模式这个数据的复杂性，可以认为这三个神经元来做这个问题是有好处的，因为我们后面会看到他有一个，一个非线性多分类的一个原理上的解释，我们现在看一下code。要不然老看这个外边了吧，你觉得特简单，好像最后什么也没学到，

这次我都装好了，装好以后它变成理解，这个要是激活函数，实际上一堆identity，这个identity就相当于没有激活函数，就这么理解，他是一个直传的，以前做抽象的时候，比如说为什么要这个东西，你老说有一个full connection的这个层，然后有一个激活函数层，然后一副很大的一句话是，当最后直接接着MSE是吧，这个FC，如果分类的话从从AC那儿。

 

 

 

## 38.

就接了一个logistic的就说这俩他模型上看好像不太匹配了，所以有时候他会用XC，然后接一个IDENTITY，然后后面再接一个什么东西，这样的激活函数相当于计划的这一个占位符。

这个可以忽略，这个反向是这就是说本层的在本能的A就是前线的时候，那个最后A前向的时候的输入，这样输入的代码输出A在反向的时候有点神经网络，它每层都系了，所以把这个ZH传回来了，但有时候用不到那个字。

你比如说SIgmoid这个方向，这个模版像他说他就是A乘1-A吗，反向他没有用到那个字，所以这个用不到，但有的是要用到的，比如relue，这个机构还是它输入再输出A，这个是说把这个函数就是说，Z和0哪个大，如果是大于零的话就输出Z,小于零的话，所以就是那个，像就相当于右边是一个直线左边是0，这个Z和A有了完整。

这会儿这个Z和A都用了，他会把Z大于零的时候先给他弄成一小于零的时候，因为初始化的时候，这是个0吗，然后先看输入这个relue。

如果是他的输入的时候是0或1，比如说如果输了在等于比如说0.3，-0.1，然后0.5，然后输出的话，A就等于0.3，0.4，因为这个小于零吗，所以他再反向的时候，比如下面一个delta传上来，delta比如说是负一，不是看delta的负一正一什么打赢，而是看这个字。

因为正向这个位置是负0.1，是看这个Z的，这是零的话乘以多少，就变成零。

这个是这么来，然后我们把这个分开了，把两个function分开了。

这是分类函数，分类函数就俩一个是logistic二分类，一个是softmax多分类，datareader这个是就全都用的这里边的东西。

Raw的东西首先是两部分，一个是train,一个是test，然后train test里边都是raw, 就是没有经过normalization的，然后这个东西是从raw变成这两个东西，就train和test，是从这个四个东西变成这四个东西要经过normalization的Xtrain，ytest，是9:1的那个比例从xtrain, ytrain分出来的这个，readdata用的是标准的这个格式，所以就会市长这么样先看看他这个。

存不存在，如果存在的话LOAD进来了进来，他是一个dictionary，data是x，label是y，然后得到样本数，feature数量还有。

他会直接用Xtrain raw这个东西，Y是一样，或者跟X一样，然后如果没有validation在的话后面我们我就直接用test当validation，但实际上是不能这么干的，normalize是用来外部调的。

归一化是把Xraw归一化成Xtrain，这个还有一个小的地方，就是说，先把Xtrain和Xtest合并了，因为有可能Xtrain，她的范围是比如说是0-254，然后这个XTEST raw，由于样本分布不完全均匀，可能是1到255，那你如果是分别归一化的话分别0-1，最后你分别归一化，那就变成了0等于1，然后254等于255这帮人这样儿了，然后就变成了在这个Xtrain里边是0，在Xtest里边，1是0变成这样，所以不能这么做，先要把这两个合并，就变成一个0到255之间的一个数就是这个目的就是怕他两个有就是，一个区域是这样的一个区域是这样的，它俩中间有gap，这是一个gap，所以要先合并到同一个地方，在在做normalization，那就不会有gap，所以这句话是先V stack给他合并，合并成一个表然后再统一做，从这个统一的规划结构里的分出来。

Normalize Y有三种方式，第一种是fitting就是拟合，就是我们上回说那个蛇形曲线，假设或者是房价，比如什么几百万，要把它变成0-1，就,这种方式这个要注意是先把Ytrain和Ytest给他合并以后再做normalize，比如说房价有一个100万到200万，然后Xtest是110万到210万，所以你要买的变成100万到210万，然后才能再去normalize。

然后classifier要给他拆成01，因为测试样本不能说只有正例或者只有负例，所以没必要先合并了，因为他以前的标签分类可能是一二，这种分类，所以要把它变成零一，这个base就为了防止这个事儿，就这base是以前比如说分类是23，我就随便写了。

然后它就会把二变成零三变成一，变成one-hot的编码，这种三类的对外的normalize，d normalize是为了做inference的时候。

Normalize的时候先按以前的标准做一次normalize的标准，都已经记在这儿了，这个是一个是最大值最小值，一个是范围，这两个东西已经记在这所以，好最大值最大值在这儿，然后那个范围在这，这就是说输入是10的时候就会变成9:1，他会先把就取validation从train里面取的，这边儿现在假设这1000除以十，那这个就变成100，validation下的就是有100个，那么train用它一减就剩900个，然后这Xdive，就是变成Xtrain先取前100个。

然后Xtrain自己就变成Xtrain后900个，就这个意思，在做这个validation size之前，我一般先干这么一件事儿，shuffle，比如Xtrain进来的时候，按顺序给，然后你要拆成这么几份，那前面可能是01后面是2345，这根本没拆开就拆开两部分。

根本不服从IID分布就独立同分布的意思根本不均匀，所以一定要先给他打乱变成什么032145，就要给他拆开，所以做一个shuffle，实际的功能中，那些工具也不知道是按顺序来的还是不按顺序来的，所以要自己先看一下。

这是生成这个获得，获得的时候一般地在train的时候去给他弄一下。

 

 

 

 

## 41.

我们现在讲三分类以上的，影藏层神经元的数量，最直接的办法就是增加影藏神经元的数量。测试级准确度是0.618，算过了。损失值最低的0.795.太高了，起码是0.00几才可以，准确度百分之90几才可以。开始看不到测试级，比如说一些比赛。测试级都在服务器上，去看准确度也可以，损失函数值是相对的，比如有两个问题，第一个问题损失函数到达一的时候，达到98%的准确度，第二个是到0.01的时候才达到98%，所以是相对的。我们再看4个神经元，他能基本完成分类任务，边缘比较模糊，先不考虑过拟合这个数。有些红色的点还在外面，再看8个神经元，测试的准确度是97%。到现在还看不到网络能不能达到精确值，他是训练了1万个epoch, 像到这种级别，提高了非常非常多了，提高每个百分点都会花费很大的精力。 只能比准确度，网络能力应该是足够了，到红线的趋势是斜的，这个网络还是有个很好的潜力，32的网咯比16的好。一般我们说神经网络一个是宽度，一个是深度。后面我们说一下三维空间变换过程，所以我们用三个神经元，恰到好处。

 

 

 

##  42.

这个是跑一个再往下走那晚上再生还可以，接着训练，这是因为迭代到了零点，我们设置的是到5000次停止。然后关掉的话，这块是做第一层就跟刚才线性变化，这个因为是三维的吧，所以做了一个线性变化，以后还是那个，本来是2维的东西，刚做完做完线性变化以后，因为你这个是，从网络结构上看这个是二维的吧，这一平台能展示。

完这是个三个单元，你要想把这个三个单元把他当成X，把它当成外Y，这样才能搞定，否则你就搞不定，这个就没法在平面上去展示这件事，所以在这里边儿是这样做一次线性变化以后把它变成XYZ这个三种形式，然后把这个图转一下，稍微转看他是一个平面为第一层线性变换就是X+b什么的，他没有任何激活函数，没有任何非线性的东西，所以不管怎么看，它在平面上的时候是一个平面，它的立体上的时候也是一个平面，这是可以理解的，然后把它关掉他做下一步是做了一个激活这个东西怎么出来的就是中间那个点，中间那些蓝色的点。

这些蓝色的点这个东西大家根据那个坐标值，也可以看出来，他是靠间吧，就是属于那绝对值的比较小，所以他在做那个激活函数以后它都缩到那个角上去了，就XYZ这个三个方向都搜到脚上去了，而这个旁边这个绿色点，它是比如说二，这个值他可能就在这。这个-10的值可能就在那块儿，他就是比较大，靠近那个一那一段上面是一，大家可以随便用一个点算一下，看看是不是这个最关键的是啥，这不已经变成这个样子了。

从这儿看他还不能分是吧，我们把它就是稍微拖一下这个程度，这一个椎体是吧，这个塔是不是有这块是有两个平面可以给他分开吧，是这块。

我们就是把一个本来是不能做线性分割的问题，把它转变成了一个可以做线性分割的，平面也是线性的直线也是线性也就可以理解为她是这么干是再高维的就没法解释了吗，咱们也想象不出来，还有一些论文写什么，用四维怎怎么展示什么的那个我没看到那个论文，反正是用高维展示，一般的人是想象不出来的，我们还可以还可以想像出来，所以这个三围。

就是这么搞出来的NO，从侧面看是线性，然后看颜色也能能看出，让你从脑子里有一个直观的时候，它确实能够能够做分类，这就OK，A1是做非线性变换的，所以这个大家也可以从这看出就是激活函数的作用，就没有激活函数你啥也干不成，你最重要，从一个空间变成另一个空间，有了激活函数的一个非线性的变化以后才能得到这种无限的可能，把它变成不同的层，或者什么样的，这还有个3D结果没有在呈现。

3D结果只是用来这个做一个辅助性的，因为刚才那个更准确，因为最开始我理解这跟二分类一样，也是那种效果，实际上，这些网络他不是那样的工作，他不是说把他什么拖起来，什么样的这是怎么画出来的，这个图是说咱这不是有三类吗。

这是第一类二类，第三类，现在是，用一对多的方式，也就是说，我先画一个把那个1变成1, 2和3变成0，从概率的角度，就

因为这个是第三类，第三类就是倒过来了，因为他这个三现在是一了，所以，所以它是倒过来了，红色是一，蓝色是零。

八一乘起来要给他又给他放上去，他就后来一要加20这个玩意乘二加20这所以他就凸起来了，这个123这个三层就分开了，然后你从这个解释就从这张图解释怎么解释一二是吧，用那个两个平面。

 能给他切开，但是人为做的做的，所以他不能开始我觉得这合理但是后来觉得不合理，因为你要把它乘以二你才可以给他分开，这就这不合理，这个这个他挑高了太高了，以后用一条横线一条后面一条后面也能给他分开，这也是。

另外一种理解方式，你是在那个理解不了你就用这个，这个方这个方式来理解也可以，我觉得第一个方式就是上面那个方式最合理，因为它没有经过任何那个那个人为的干预，它中间的计算过程就是这样。

这个其实只是展示了一个最后概率计算的结果，我觉得是可以这么理解概率计算就是红色的一蓝色的是0，看一下分类样本不平衡，他有一个叫imbalanced data专门有一个库可以干这事，我没有研究过那个，他这个意思就是说二分类我们刚才算的，比如什么异或，那个两个三角形那个分类都比较平衡的都是比如说，正样本1000个，副样本1000个有时候就特别大，比如说1900，1900:100这个你训练出来就很糟糕，就是用这种方式，为什么，他因为有一个例子，比如说有一个。

100个人里有两个人有这个症状的，那么分类器100人有两个，100个人，然后等于是98加2。

比如还有一个100个测试样本，我要把所有100个样本都算成是没病的那我的准确率就是98%吧，但那有病的人，我分不出来那个分类有啥用，是吧，我这98%虽然高。

但对于precision来说一点用没有他只是准确度高的，所以有时候我们算那个准确度叫accuracy，然后还有precision recall这几个东西是要组合起来看的，而不是说只看一个98%就OK了吗，并不是这样，这个就是为了解决这个，因为训练的时候只有两个样本，那你能训练成一个什么好东西，所以要把负例增加才能够让他基本平衡。

比如说1:5这种概念才能训练出一个比较好的东西，那这个有一些解释说，如何解决样本不平衡问题，这是一个真实存在的，如果大家以后做分类的时候挺多的就是尤其是疾病预测，

生病的人毕竟是少数吧，那个负例样板有可能很少，有一句话叫做更多的数据往往战胜更好的算法，不管做什么东西现有数据。 数据是数据是KING，数据是国王，其他是皇后数据有更多的数据才能步骤能够深度也好用。

普通的那个machine learning，这个学什么都行，深度只是现在比较流行，好多好多问题用传统的machine learning完全能解决的话，你就不需要用那个用那个深度学习的方法去解决，又费时间，还要学新东西什么的这种。

比如我们在刚才是100:10，然后在收集数据，我们100:10的时候，我们收集了110个数据，然后再收集数据，然后变成1000:100，这个比例是一样的吗，但这个100大了是吧，然后把这里变成500，500：100，这样就完全能train出一个比较好的东西，他第一个想说这个问题，他有别的数据，这个数据仅供参考，可以搜集两个数据，这样一个意思就是说要想方设增大那个负例的样本量，然后再把正例扔一点，这有一些这个什么过采样将采样什么的过太阳就是说最少的东西要供房。

对那个多的东西要要道歉，有时候比例是1，:1的时候是不行的，刚才我们几个例子都是1:1的，这个刚才说过了，因为这个98%准确度不OK，因为只看着都是不OK的，我们还可以看precision recall，还有AOC这个就是Precision recall一个组合一个产品。

我这个大家查一下前面公式就行了，这个是AOC是评价一个面积就这么一个东西，然后画这么一个曲线，如果是是这样的，这是最糟糕的情况就是基本上是一半，一半是猜猜的话能猜对50%，而且曲线是这样，然后下面的面积越大，这个AUC下的面积越大，表示你这个模型准确度越好，这同时也考虑正例和负例，能够避免98:2能够避免那种情况，然后有一个库可以可以人工产生样本，这个科学的一种方法，我没有仔细研究过这个库是现成的，大家如果碰到这个问题的时候再去研究一下这个然后这有一个叫imbalance learn应该是这个东西是可以有一堆函数可以用的，然后尝试新的角度解决问题就是我们当由98:2的时候，我们不要把它看成一个分类问题，我们把它看为异常点检测问题，但异常点检测是经常被提起的，比如说一块儿钢板照个相，然后通过那个什么纹路，什么乱七八糟就能看出这块刚制造出来的钢板有没有缺陷，这个叫异常点检测，这个已经实行的，就是就经常看到这种例子。

还有些那个齿轮轴承轴承转速，它转动的时候，它有一些传感器可以检测到震动，从那些一系列的振动数据也可以做异常检测就是这个轴承的，是不是工作的好。

修改算法什么的，这个就比较麻烦，我们后面会讲集中学习怎么用。

看看三层的，我们今天下午就把这个12学完就好，他们搜集了一些欧美的数字你看那个四那个七不都不一样，。

对于汉字就不OK，要用更牛逼的办法来解决，这个问题是这样，我们先不用卷积，也可以解决是因为大家看到，比如这是一个28乘28的一个矩阵，然后他那个一都是由点组成的，

变成28行，然后那个就变成000000等等，把这个碟在后面。你整个看下来那个二肯定不是这样，咱这个特征的就跟图像的可能有点偏离了，但是他也是有特征的，所以这个机我们基于这个原理来了做minist，这个识别，首先是归一化，跟以前不太一样的地方就是以前我们是说每个FEATURE做一个归一化，比如说X1X2，把所有的X1看一下所来看一下做归一化，然后现在你是28乘28吧，就一共有784个feature，而现在我们假设第一个feature就是左上角这个点，一般都有空的我们我们举个例子就是000，那这玩意儿怎么归一化都是零是吧，最多有个一那就那就变成变成变成0122种东西然后对于这个有可能有一个2581个零一个025，那这个就是零变成了零，然后255变成了一。

对于这个来说，假设有把0变成1，所以这个东西跟刚才我们说那个要合并train和Test两种数据以后再故意似的这东西不能做feature几何归一。

所以这个跟dataReader里边不一样，他是把这个整个的图片做一个归一，不能按列走，这会儿就干了这么一件事，刚才那个code。

这儿有举例，就是你看点一般左上角点都是零吗，然后点2可能有十二，五十九点三什么不一样，你这个要单独归一的话肯定不行。

我们这个用一个三层的都OK，然后第一左边那个蓝的输入层不算书中不算的话，是1到784，然后第一层用64个就那个橘色的是64个，第二个隐层用16个。

第三个输出层是零到九，所以就是十个用十个神经元，那就这么定一些输出，这是因为，这我们784我可能要用比如1024才能解决是吧，当我们实际看这个东西，它是非常密集的是吧，3*3的点，这也是非常密集的，你要给他看到28*28，784的时候，他非常稀疏，他有好多零点，那些零点是可以被忽略的都是零，比如。

 

 

 

## 43.

我俩都是男的队友有什么区别，你说是吧，100个男的和一个女的那女的特别突出，然后那个男的都不突出就那意思就基本上就可以忽略那种特征，是短头发，他们这种特征就可以忽略了，唯一不能忽略的是强壮程度，所以那个稀疏点，有很多特征是没有用的，所以我们用了一个64个就可以解决你还可以用更少的，因为它整个图片里头它的有效信息是非常少的，不像是一个什么彩色的图片，比如说猫狗，包括本身花纹就多，然后背景绣花，所以每个点都是有用的，这个不是都有用，我这刚刚搞一堆这个矩阵形状，这是为了帮助大家理解。

然后784乘64所以这俩成绩来就出了个64，然后第二层六四乘16再乘起来出了个16，然后第三层是1010是16乘十，前向计算就不写那么细了，都用矩阵方式，这上面是把这个尺寸大家脑子里过一下就别乱了，哪层到哪层，反向传播用这种方式写的就不再细推了，其实都都一样，这里用明显的那个差乘表示，这个是点乘，这个是叉乘就是标准矩阵乘，代码实现我们可以看一下。，一个是说加了一个三，然后，在forward的里边会有多出来，这块正确的写法应该是这样，我在试那个CPU那个东西所以把它改了正确的做法应该是这样，然后第一个用sigmoid，第二层的用tan，然后第三层，应该是走这个分支走一个么的反应，所以走softmax，梯度下降，numpy本身没有GPU，所以做一个更复杂的事儿就是如果自己笔记本带GPU的话，你可以这么玩，这是Amazon机器学习的一个库，

Forward和backward各加一层，其实都是一些重复的工作好多重复的那你四层五层怎么弄，所以后面我们会做一个miniframe work把这这玩意儿都给他变成一个统一的代码，想多了怎么玩，这个是一个结果一个结果，然后就卡最后看这准确率是96.83%那MINICT6万个数据1万个数据由9683个是准确分类到零到九。

最牛逼的是99点，%儿多用CN的话90%多一般到97%什么的上面就已经已经很牛逼了，其他的没有变化所以三层神经网络，一点变化都没有，只是在那个神经网络的结构上有变化，发过来，6万个你要用两三个多慢，所以就用一个大的，你看这俩偏较大，实际上是，说明这个网络还有进一步优化的可能性，很明白用更深的和更宽的网络能把这两个给他mean，就是让他这两个稍微MATCH一点，这是97.4%，我看看应该是百分之七点几。

三层网络就这么干，这三层神经网络一般能干不少事，除非特深的那种问题那个目前我估计大家手工测试一下训练效果，这个很有意思，我们手工测试一下，到现在为止我们只看到96.8%吧，我们还不知道这个到底是什么效果，low net就是跟刚才定义的一模一样的那个那神经网络，然后它的一个low result造成的， 他会打一个界面，那个界面在这个界面是让大家输入的，然后还有一个界面，这两个界面，然后我们看一下这个输入写个一，然后按个回车后面它是debug告诉你price，现在按回车，他是先做了一个图象处理，因为这个东西本来是640乘640，他要变成28乘28，刚才那不是0-255嘛，所以他那个变得特别丑，好多都给都给吃掉了，然后你看这个结果这个result就是一就说这个这个网络已经识别对了，

第二个是二，这个真的识别出来的。这个东西是怎么实现的，这用了好多相当于hook，那个勾的那种东西，所以要用一些全局变量一个勾在那儿，你传进去的其他什么都没有，因为那个图片进来，本来就是640乘640的吧，然后你要先把它变成28乘28的才能去做那个inference，这个228*228怎么变，用了一个库。

就是画完图以后看mouse Press就是就是按下，然后release就是弹起，我在每个Press时候都把那个数据都都记下来了，所以记下来一系列的点，这按下弹一系列点的在屏幕上就能画出一堆像，中间一个mouse move就相当于，他这个存到一个X，好的一个一个PRADA就是连续的把那个X，就是连续的给他给他画，其实这说话的好多直线，然后连成一条曲线，然后是一个黑色的线，然后一旦mouse release以后，回车在哪，是在figure这边注册，在按那个画板上注册了这四个事件，事件这个KEY这回车，这是鼠标按下鼠标弹起鼠标移动这四个东西我们看的时候干了一些什么事。

Inference是怎么实现的，read file是打开那个文件，比如640乘640，先resize成28乘28，在这个期间会丢失一堆信息，什么标，什么都没了，所以在这个地方，line width等于40就给他弄的很宽，然后等于一的话一条细细的线，以后就就中间断了，所以28*28，然后把这个变成一个array，因为这个resize出来是数组或者其他什么形式，然后变成一个array，然后这个array现在是反的，就应该是白底黑字，然后训练的时候，黑底白字的意思就有好多零，然后中间有个一这是那个黑底白字，现在我们刚才画的那个白底黑字吗，所以我们要用25减去这个东西作用，所以inference一定要注意这些细节，差一个小细节都过不了，所以这个一定要黑白先翻转，然后取最大值最小值.

这是一个28*28的矩阵，他们的image show能够直接把它显示出来，但这个显示出来已经是Resize之后28*28，所以大家可以看到好多东西挺小的就是一个是小一个是，缺好多东西，这现在是个image，image SHOW这个代码出来的东西，所以他已经很少了，这是最后的显示这些，然后把它这个东西变成一负一这个意思是就变成一个向量，一个横的一个向量，也可以。

把这变成一个向量，然后image return，在key press里面回到这里，因为我们是本来就用向量去train那个东西，所以inference是输入的时候也必须是一个向量，这个返回OUTPUT或者十个结果这个十个结果是大概是这个样子就是，零到九，然后他给一个什么0.01，0.03，0.82，这个是0123说明这个数字就是二，这个最大的返回一个ARGMAX是取向量或者数组最大的下标值，最大的下标值就是二也是从零开始的012，所以是2的话，那正好就是那个数字二，那个Label值就是二，这就是干这事的，把下标值打印出来，所以就变成这个下标值。这个inference就是这么做，大家学过用C#去写一个客户端的程序，这个完全用PYTHON写完了，但这个东西，在实际中，没有人用PYTHON写客户端code，所以大家不要抱怨说为什么客户端用C#，没有让你用。

 

 

 

## 44.

后面有一篇专门讲这个，但那是我Intern写的我根本没有试过，这个安卓要搞这个东西，它太麻烦了，所以我一般都用WINDOWS模型的部署来做这事儿WINDOWS就是差不多最好用的。

所以C#我们大家在以后的工作中，要学会这个最差，那是一个非常好的一个工具，不要抱怨说我们为什么又新学了一个语音，那个对你有好处对你有好处，要要学。

落地，必须用那个正经八板的客户端。然后现场部署什么都慢么就慢慢好多能能差几十倍，OK，它这个结果可能是这个样子，所以我们就去你认为这个是不是，下面说一下梯度检查。

意思是说，所以你要没那个公式，没有那些，哪知道说，这个是TR还是不剃这个好说，要不是T的话，这玩意过不去，因为他矩阵尺寸不一样，但你这个是0还是1，你这个。

还有类似好多好多东西你怎么怎么解决这事，所以我们要有梯度检测的能力，它是能帮你检查这堆反向的code是不是写对了，这个第二个特别牛逼，我们看看他怎么做的，实际上你要做一个研究的时候，比如说你自己当一个网络你都不满意自己搭一个或者说纯学习的目的，你想创建一个新的网络。你应该学会这个梯度检测这种方法来检测网络反应传播是不是正确，这对任何网络都适合的什么CNN，target比较，数值梯度，然后和反向传播的梯度，证明反应传播的代码是无误的，所以我们先看看数值梯度是什么概念，他起码有一个可以比的东西，那个数学解跟我们以前学最小二乘，学什么正规方程，有了那些我们才能比较神经网络的是否正确，先进行一下痛苦的回忆。

F传进来的指针这个函数指针，我们就这样做就能得到一个在X点的数值微分就是这个意思，真的，所以先有这么一个函数能保证数字微分是可以得到，然后再看这个H，不能太小，比如1e-10会造成那个计算结果差异，两点太近了有可能就是基本上重合了或者相当特别大，所以不能太小，也不能太大，一般我们用1e-4-1e-7之间的数值，然后用这个方式了，还有个问题，这个方式数据微分是看蓝色的那条曲线，蓝色的直线，就整个这条蓝色的曲线是这个函数，然后他的真实导数是那个红色的线就在这个点的切线。

这个是他单边逼近的时候会产生一个误差，真实的角就是这个这块会有一个误差这个角度，就因为你是单边逼近就是从这儿到这儿画了这么一条线，然后跟真实的有一个误差，所以我们就想一个办法是双边逼近，就是从f(x+h)是减去f(h)，这样会从这儿得到一个交点，从这得到一点，画出这个来这线别看他偏离了我们算的我们算的是它的斜率，，所以这个绿色的直线跟红色的直线的斜率，只要非常非常近似的话，这个数值微分的结果就是对的，因为导数就是一个斜率，不是其他的东西，所以我们就用这个公式。有一个泰勒展开他这个很好理解，大家不要怕稍微看一下，然后把它利用一下说，泰勒展开意思就是，在X零点附近的泰勒展开我们就给他换成那个theta，它就相当于这个点叫theta，这里其实应该用X可能更好哈，大家就脑子里转换一下，这个就是那个X的，然后泰勒展开以后这一项FC的就是这样。

高阶无穷小，我们就不再往下分了，这还是在theta点那个展开，

比较他俩误差就是一个H，H是1e-4到1e-7，就说明这个东西比这个东西要精确很多，就是说他他小于他的意思，他要精确。

这个误差项和这个误差要小很多，因为114的平方等于这个是一一副四这个补偿性，所以要要消防，然后，我们用一个实例来说明梯度检查这个大家都比较清楚了，

实际过程中假设的7.0001减去7小于1e-4，他们就认为满足精度要求这有数值说明的话，一下就简单好多，这个写的复杂就是那个化成LOG，参数向量化这个比较难理解，这个需要仔细说一下，现在WB都是矩阵吗，因为你不知道你的梯度有多么复杂，像刚才我们那个矩阵是784乘64还有那个B也是，不是一个标量，所以梯度检查的时候要先把这个向量把它变成一行或者一列。

比如说是W是长得这个模样是一个简单的举例就是比如w11,w12,w21，如果给他向量化以后就变成W一W11W12，如果第二层的，接着往后加就加就不管多少都加进去。

N指不定多大N大概是2000多吧，这个是什么意思，这里有一堆W，w11变一点儿一点，其他都不变了，然后只算这一个梯度，然后第二次我们在用W2.10.1121。

一共做了2000多次就这个意思就是，每一个都除以2就得到这么个东西，对于一个每一个theta都求他的LOSS直对这个对theta的偏导，

这个还有一个判定条件判定条件是一个二范式，用那个真实的东西减去双边逼近的东西，这个真实的东西就是你的code这段代码出来的那个地图是多少，然后减去那个感觉是外面这个下边，这也是，这是有一套理论，这个不太好解释说，这个算两个向量之间的欧式距离法就这么算的。

有几种可能大于1e-2肯定出问题了，然后-2到-4之间的可能有问题了，需要检查，然后-4到-7的一般是可以接受的。

然后对于-7以上的就是就你可以庆祝一下了，如果是特别深的网络，1e-2也可以，所以算法是说在初始化，然后随机触发X值。零一之间初始化，然后做一次前向，做一次反向，然后得到各参数的，因为，这个是用code得到的，我们看后面有那个具体的那个计算过程，然后是得到梯度的变化成向量的形式，第一个是用code得到了一大串儿，第二个是用双边逼近得到一大串，所以，毕竟就是刚才说的那个一个一个搞这事。

把这两大串儿之间的值，然后通过这个去比，我们看一下code吧，我们先看一下结果怎么样。

我们稍微改一下反向，比如说在这里我们稍微改一下，假设是。不是说那个数小了，就什么他那个主要的距离本来就这么大，他这个距离就变大了，

然后再看一下，tanh，然后改成sigmoid，因为我们前面是一个sigmoid一tanh，假设我们这个忘记了，我们正向的时候是是用前面正向的时候是用这个tanH。

反向给我们用sigmoid你看看情况怎么样，别忘了改回来。

其他的基本上差不多吧，反正基本能不再加个一吧，这个不加，加一小的数的0。01和少出这种错误，就是一下，这也是每次0.002。OK，我们看看code，他是怎么实现的。

 

 

 

## 45.

弄一个网络三层的，然后做一次正向一个反向这刚才所说的，然后得到得到了这几个的就带着大哥逼大大逼什么的给他变成一个定时放在这个里边，然后这个里边是。

这个东西gradient to vector,这个东西是把它转成一个向量，他本来是一堆矩阵，这一共六个矩阵，能给他转成向量A，就是我们想做双边逼近的那个参数完这个是现在该把他做双边逼近了，然后跟这个结果去比，然后真正去做双边逼近的时候是这样做，这个N现在是这个Shape零就是说，每一个参数2000个参数。

然后对2000个东西所有的都这么干，把那个结果放在这个里边儿放在一个一个这是从零到2000的一个向量，一个2000维的向量，比如说我们就得到了。

下面是两个范式相加，然后这个分子分母，然后用分子除以分母就是difference就得到这个玩意儿，这个玩意儿，整个就做一件事，这个我没有做一些挺好的一个模版上的东西就是把这个东西应该变成一个。可以输入的参数，不然每次要用这个时候你还要改，这些code就比较烦，比如四层的你很改成四层，这个标准，所以可以考虑给他做成一个模版似的东西输入的是一个的士就OK。想深度研究自己的和网络的时候就可以这样做。

 

 

 

## 46.

样本量大的话给128,256都没问题，batch size去调参数，不要两个一起动，只能两个都大或者两个都小。深度学习里一般会有一些问题，第一个是很难调学习率，太小会收敛慢，太大会很难收敛。相同的学习率不适用于所有参数。实际上鞍点可能是平的，可以找到一些局部最优解。我们先说初始学习率，他在学习过程中应该不是一直不变的，会开始大，然后小。这有两个条件，sigma N，k是指迭代步数，等于无穷大的意思就是可以任意次数得迭代，他的平方小于无穷大，这说明都是小于一的数，绿色曲线是正常的，开始的时候比较快，后面越来越缓，遇到了坡度和鞍点，不可能一条直线到底，这条红色的一开始下降会比绿色的曲线要快，但这只是一个暂时的现象，前面有等高线图，最终一直能往中间点逼近。大的步

长有助于跳出局部最优解。右边的图是loss，他学习率是不断变化，他就用这个东西去试，最好的学习率是0.2,0.3附近，这是一个理想的曲线。看这个T型，每次都是大迭代，他是以log10函数显示的。

我们看看后期学习率怎么纠正，一开始我们不知道iteration是多大。Batch size小的时候学习率也要小，保证你的收敛速度，但你用0.7也没关系，也能收敛，就是精度差点，蓝色的线能到98%，红色的96.5%。最开始都用线性的，学习率样本从128-164，学习率从0.2-0.25，一般都用线性的东西，但问题是学习率是不能大于1的，样本变成128x2的，学习率0.5x2变成1了是不可能的。数值理解上回讲过一次，红色线是拟合线，绿色线是标准拟合线，红色线对蓝点loss值，0.33，如果加两个样本，样本多了。

 

 

 

## 47.

这只是举个例子，实际上不是这么算的，那个一如果想他一把就等于零的话就一次到位哈，就是说这条红色的线一次就能拟合到绿色的线，像那个位置哈，那么就需要他等于零，那么等于0.15，也就是说三个样本的时候对应的是一的值零零点一五，然后再看五个样本五个样本的时候就是这个玩意儿，对应的值就是。

这说明，这个样本量越大，它的那个learning rate应该越大，才能够就互相匹配这个因素的帮助来解释这个是不是科学的这个解决的科学我也不知道回去再琢磨估计先先这么写着吧，先从数值方面理解一下吧，OK，这个是知乎上有一个东西，大家可以看一下记录数据对那个以前那个标准的。

他有一个方差一弄，然后最后得到一个方差有一个平方的关系，开根号的话就冒出一个号来，最后根号M就是就是这么来的。

步长如果大了的话，他可能跑这边来了，但实际上，如果步长小的话，就可以。

跑到几点，这个老总MINI模式局部的，比如说你可以得到一个局部的，如果不长最大的你可能错过局部左右，但有可能奔向那个全局作用或者啥都没有，就是就是这个意思，对未知，但这个走了那个。

Batch size是256的时候，还有零的开始值应该是五到15倍的东西,学习率大小都增大或者都减小了，最好固定一个调另一个，学习率是最关键的一个数比大小不能太小的一的话就没法玩了，就根据样本数来定一下，然后我们Step6，我们简单说一下吧，这个然后明天再说STEP7。

模型推理把参数存起来，然后弄进来，然后每次往前项计算一下就模型文件，然后模型文件描述什么

他会有一个execute engine，还有一个数据流图把这俩东西接进来，再加参数，它就能完成前向计算，就是一个图的概念，权重那些，

tensorflow并不原生支持,这是第三方的，但是有一个问题，那个变了，他不变，他们漏掉了它也不变，所以就没用了。这个模型文件要有客户端支持，那个模型是本地支持GPU，新模型的时候有一个好处，你只把模型更新就好了，不用把整个APP去更新就有这么一个好处。

这个工具还是挺好用的，能看各种模型的那个长的什么样儿的，

这个表然后给认成那个啥了，不能那个整个表示那个东西了，这样好点。

就这两种方式，一个是WPF，一个是delta_P，这两种方式。

车牌识别我们查一下，车排我还能跟你说一下，人脸也先label，

先他先label一些点，比如先label这几个点，他会给你弄一个椭圆的，鼻子也是内部的几个点，然后嘴那个这几个点就这样，然后用这种方式就是label好了以后，然后他就把这个图像特征的用卷积把这帮都抽出来。

再到一张新的图片的时候，新的图片，比如说，就这样了图片了，然后这个眼睛可能是这样的。

然后这样的话他就，他不是有一些feature嘛，就是说，这个比如说feature1就说，feature2

是眼睛位置在整个位置有一个横坐标与纵坐标，然后feature3，比如说两眼之间的距离，或者什么，还有一些边缘的特征，比如说它的特征是这么一个圆，

没有说可视化没有这么理想，他当然会得到一些什么数字东西根本不能解释的那种东西的以后他就知道那个区域是眼镜，这个是车牌那个系统。

比如说我们拿声音来说吧，有两种方式，第一种叫语音识别，你说ABC他能知道是ABC，然后第二种方式叫声纹识别，意思就是说，你说个ABC，我知道这个真是假，然后你另外说个ABC，我知道那个人是已，从人到声音上，你说的这个玩意儿是吧，这是两种不同的东西。

他这个东西一般是说是比较general的东西就你说我说没区别，而这个东西，是必须要训练的，就跟咱们的手机不是后边有那个指纹什么的吗，比如。

这个训练他做声纹识别之前一定会说让你说几句话，然后把那个SIGNATURE一边录制好，那些固定化，比如说声音的波纹是这样的。在某些人，他那可能这个尖儿是这样的，在某些人，可能是这样的，他就看一些非常细小的一个部分来区别，声音之间不一样，就跟指纹一模一样，这个叫声纹识别在认知服务里就会有这个，我们过一下认证服务国家人民服务，就我们先看一下车牌识别，大家看看这车牌识别到底怎么玩。

其实这个东西在用网上现成的东西自己都能搭出来。

第一把它变成黑白，他是彩色的，然后变成那个HSV，这个是变成饱和色饱和度什么的那些。

然后就会得到这样一些图，反正彩色的是不能识别的，就一定是变成黑白的这个东西，然后是对比增强这个。

 

 

## 48.

然后是二式化，变成0和1,然后应该定位同样支付订阅一堆事儿，还得拿标准的做个label往那个STYLE上靠，因为你这个东西都不知道多大，你离得远就小，那是一个标准区域是什么，就是多少，反正很少能提的准，他从黑白的里面提轮廓还是挺容易的，从这里提个什么9，5大家看到能容易，再根据那个彩色值去定位那个车牌的位置分析好画了一个红框，因为他也可能是斜的吗，这玩意儿就是斜的，还一个二应该是，定位以后应该出了。

所以说后来应该做correction，比如F和4搞得一起去了，那你可能要要去问一下区分四和F还有可能，中间这个6你变成什么S了，不可能，就没有这个车牌吗，前面后面是数字前面是字母，因为它那个东西太标准了。 对一个值的意思就是W11，W12 ，W13什么什么，然后比如说三个值，其他都不变，W11+H，然后后边这是第一个涵数，第二个数是W12-H，然后看前面那个是啥，这个loss是函数，所有W值，因为你现在这个我们刚才不是已经变成变成这个，然后弄进去以后就是做forward的时候等于做了两次，做两次forward以后再算两次LOG，因为这个函数被调了两次。

 