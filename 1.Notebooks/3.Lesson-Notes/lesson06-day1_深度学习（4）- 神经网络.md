

## 4.

我们神经网络里的损失函数, 只有MSE和交叉熵，别的损失函数不用了解。一层网络只要求个dw,db就可以了，还没有到一层一层往前去求导，所以四大公式在两层以后我们就能明白里面的意义了。

我们先讲线性分类，神经网络两大功能，一个是线性回归，一个是线性分类，回归就是你和一条直线，分类就是把两类点或者多类点分开。

这里有一个概念logistic regression，这个就是分类，也叫逻辑回归，这个其实就是sigmod,大于0.5是正类，小于0.5是负类。前面我们讲的是linear regression。

 

## 5.

神经网络最后一层没有激活函数，最后一层有的是分类函数，Z=wx+b,这就完成了一个拟合，，这是一层，后面并没有接激活函数，分类函数。

分类里面有二分类和多分类，二分类我们会用一个logistics，就是所谓的sigmod，这个叫对率回归，为了简化问题我们就不做归一化的问题了，现在已经把这个问题归到0和1之间了，但在实际过程中，第一步就是先要归一化，我们看到中间有一条直线可以把他分开，那就是一个线性问题，但在实际的工作中，经常不是可视化，有多个feature的，一般是用数据分析的方式，一一做比对看是不是线性关系，跟标签值作比对。

所有的样本都是竖着的,Z=wx+b实际运算中特别困难，一般的教材中都会用w的转置，在实际中都是x*w. 你拿到一批data，他可能不是0和1，问题是三个坐标值问是楚还是汉。 

*自变量既可以是连续的也可以是分类的，线性二分类和非线性二分类的区别是直线搞不定了就一定要用一个曲线来搞定。*

非线性二分类我们会在两层神经网络里会学到。我们先看看实例，我们第一次学的是单入单出，今天我们要讲的多了一个分类函数，这里的softmax就相当于一个A，就是三个神经元同时接受两个输入，二分类之后就会有个A，二分类函数，非线性神经元就是一个神经元变成了三个神经元。

所以这一步步地实践跟第五章比只多了一个A，logistics计算，输入层特征，特征值是横着的，权重矩阵就是竖着的了，这里的b永远和w的列是一样的，然后行数永远是1,w的列数相当于神经元的个数，有几个神经元就有几个偏移值。Z经过之前计算之后是负无穷到正无穷内的任意一个数，经过logistic压缩之后会变成一个0-1的数，所以A的输出是一个表示概率的值。

A大于0.5的时候我们就认为是正，小于0.5是负。实际中的A不是按0.5划分，而是取决于自己的需要。如果我们要提高precision，尽可能把所有的正类都提出来，这叫提高他的recall值。交叉熵我们用的是loss而不是J，因为这里我们用了一个单样本去计算。单样本交叉熵的计算我们叫做loss，多样本的时候叫J。现在Loss必须经过A才能到Z，所以会用一个链式，偏loss比偏A*偏A比偏Z。

 

 

## 6.

a = 1/(1+e^(-z)), l利用除法的导数公式，算的时候就不用去算Z，直接a用自己的求导。最后的结果是a-y。 我们看损失函数，其实这些函数都是满足梯度下降，也满足单调递增递减的，就是因为那两个函数配合起来可以使运算简化，不是一种巧合，最后一层永远是z-y，这样你就不用考虑不同算法之间的差别了。

标量对象量求导你就把他拆进去，把标量放到矩阵里面。三样本举例a-y就不能小看了，他是三个样本一起计算，为什么一定要写成矩阵形式，因为代码里都是矩阵去写的，里面有很多支持类，logistic和Sigmod他们的形式是一样的，hyperparameter他是一个超参，这里有一个神经网络的类型，比如说拟合，还是做二分类，eta就是learning rate，max epoach就是一共跑了多少次，这里的batch size就是每次用五个样本去跑， 如果不能整除他会往上加，因为还差一两个样本也要被训练到。

训练的时候一个内循环一个外循环。所以真正的超参一个是learning rate,一个是max epoach，第三是batch size。Input output是可调的参数，是普通的参数。

Loss function里一共有三种，MSE就是均方差，都是照着公式去实现的，这里的np.multiply是差乘。

一共有三种损失函数，交叉熵，二分类，均方差，在check loss的时候他调这个函数，先判断是拟合还是二分类还是多分类，然后调不同的函数。 我们先看training history，外围的，就是把所有训练的历史记下来，为了画那条曲线，然后simpledatareader就是我们所有数据都是自己做的，做的都是同一种格式的，

用一个simpledatareader就能很方便的把它分成两个，X-train，y_train和X_raw,y_raw。

一旦经过normalize之后就变成0-1之间的了，读进来的是x_raw，如果不需要normalize的话，先把他赋值过来，一旦需要normalize的话先把X_train normalize好了，这样x_train和x_raw就不是同一个数组了。

 

 

## 7. 

Normalize Y也是需要的有时候，比如几百万的房价，最后就要归一到0-1之间，shuffle就是在每个epoach的时候要把所有的样本数据打乱一遍，这样对于网络训练是有好处的，防止神经网络对于数据顺序会有一些拟合。

一个w,一个b,就这两个，之所以要用sum是因为多样本，这个参数是0-1，就是x轴和y轴，这里m的意思是一共个样本所以要除以m，inference和forward其实一样，完全是一种封装的考虑。 

这里加两个下划线表示private函数，这样外部就看不见了调这个类的时候，这里的inference是外面调的所以不加下划线，这里的init就有两个下划线，因为是内部的不能被调用，但也没有那么严格。

Train的时候我们主要看两个循环，看轮次，一个是控制iteration, 然后checkpoint是说多少轮迭代去check一次loss，还是说每次迭代都check，每次算loss function也是需要花时间的，一般都控制比如说100个iteration的时候才去计算一次loss值，这里的checkpoint 0.1是指0.1个epoach去算一次loss值，大家记住这个曲线的形状，这种曲线是非常理想的，一个是比较平滑，没什么震动，一个是趋势是向下走的，这是神经网络里最希望看到的。

我们要用可视化的方法去看一下，这张图有点问题，分类器还没有train好，直线用该是再往右偏个两三度，大家自己把epoach改成10000再跑一下。

 

## 8.

比如我们有10万个feature，一个样本就要占比如10兆，这种情况我们基本现在碰不到。我们过一下level3的东西，怎么把一些东西show出来，simplereader就是读数据，draw_source_data就是显示在屏幕上。

X1的shape就是200x2，x0就是200。

我们看主程序，number input是2，output是1，一个二分类，建一个neuralnet把超参传进去，他会自动初始化w矩阵和b矩阵，然后调一个net_train，把datareader放进去，然后再定义一下checkpoint，这里的draw_source_data填show=false，不让他显示，这样就能显示在一张图上了。

那根线画出来是有讲究的，看结果可视化这部分，这里两个w一个b怎么解释，我们在前项计算中这三个数是train出来的数，因为神将网络只能计算概率，在这方块里面任意给一个点进去，假设概率是0,7，那就属于第一类，比如0.3就属于第0类，0,5就是我们的边界，我们就用这种方式来画这根线，a=0.5的时候就相当于Z＝0，我们来看下面这个代码，因为w,b都是0 based，这里都减去一个1，这里随便打了10个点，其实打两个点也够了，目的就是在0和1之间画等份的十个点，然后把x都算进去，为了验证工作的正确性。在axis上取了-1到1，防止挤住，边就不显示了。

 

## 9.

我们再看一万次时候的三个值，这个值更接近于1说明分类越来越精确，中间其实应该是个过渡带。 然后二分类的原理，二分类的回归和分类的区别就是线性回归是用来拟合样本的，线性分类是用来分割样本的，用代数来解释分类就是，分类的样本在经过线性变化之后的概率值，

然后再经过logistic函数之后的概率值大于0.5的话说明正样本，小于0.5就是负样本。几何方式解释的话就是画一条直线，正样本都是在直线上，所有的负样本都是在直线下。

现代入几个点，假设中间的线是斜率为45度的线，这个不妨碍我们讲原理，我们看中间的红色点，本来应该是一个负类，但被判为正类了，他的loss值是1.7，比之前两个数都大很多，这个loss值如果反向传播的话，给当前值的惩罚值就特别大，对不同问题loss的值域是不一样的，在这个问题0.12可能是大的，

别的问题里可能是小的，这就是一个正确分割的图，比较一下公式5和6，就是比较公式的斜率和直线的偏移，这也就是我们做二分类结果可视化的前提。下面看对数几率，这不是必须的，

讲一下对数函数的来历，假设有一个硬币，正反概率是5比5，正面的概率除以反面的概率是odds

,几率就是a/(1-a)，正好是一个对数的关系，去log之后就把非线性问题变成直线了，然后两边再取e，就等于原始的值。这也就是他为什么叫逻辑回归的原因，前面加了log就变成逻辑回归了。

 

然后是双曲正切，这个东西很复杂，tanh函数是值域是-1到1，tanh有个好处就是他在零点是左右对称的，在反向传播的时候就会有好处。这部分对前面要求特别高，要求深刻理解才能明白。如果用tanh做分类的话我们需要改好几个东西，偏a比偏z就是1-a^2,最后在反向计算中计算量就要大很多。公式6 run起来会有问题，会有divide by 0，因为我们算的时候是没有分母的，现在多出个分母，很可能等于0，这时候我们就要修改损失函数，想办法把a去掉，我们在交叉熵公式里都加一个1+，最后就变成一个非常简洁的形式，2（a-y），会让你很好理解整个推导过程。我们再运行的时候发现有负数。

 

 

## 10.

一直到loss等于0的时候，有负数是因为改了损失函数，这里的a和1-a 都属于0-1之间，所以取ln的时候的范围都在负无穷到0之间了，1+a的范围就是0-2了，把整个loss function都除以2就又在0-1的范围了。我们看分类结果，这条线偏右边去了，原因是本来的标签值的范围是0-1，现在tanh的范围是-1到1，现在要把标签值 改成-1到1，其他的都不用变了。这两张图看纵坐标是不一样的，一个是0-1，一个是-1-1, 这两张图是横坐标不一样。我么再说一遍，第一个问题是样本的标签值y，logistics里本来是属于0或1的，Tanh里就属于-1或1的。第二个是在反向时，logistics是a(1-a), tanh是（1+a）(1-a). 所以偏loss比偏z就是a-y，tanh里就是,
$$
{(y-a)(1+a) \over a},
$$
  因为分母有a，可能被0除，不make sense，我们把loss function把他变一下，求导的时候分母会多一个1+a来，偏loss比偏Z是2(a-y), 我们再看第三步，是因为输入值域是0-2之间，现在我们想把它变到0-1之间。Tanh值域是-1到+1，a是属于-1到+1之间的数，那1+a就是属于0-2之间的数，1-a就是属于0-2. 我们要让loss function永远都大于等于0，所以logistic就是1-a.a都是属于0-1,
$$
1-a \over 2
$$
,都是属于0-1，来保证loss function大于0. 有了这三个步骤以后这两个函数就近似等同了。

 

## 13.

我们来看正规方程法公式推导，我们先假设前面这一项是x, 后面一项是y, 就不用管x*w这一项，x用四个数值，y也是四个数值，1就是先用sigma求和的方式来解释一下每个实例的平方，然后用向量的方式去解释这两个为什么相等。大家可以看到，本来有四个数，变成两个向量，转置就是把这个横过来，
$$
J(w) = \sum (x_i w_i -y_i)^2=(XW - Y)^T \cdot (XW - Y) \tag{12}
$$
这就一个差乘。 

第一个样本，0-1变成-1到1，第二我们在求反向的时候求loss到z的过程，在分母上有东西，有个a,所以我们改了loss function, 改了loss的形式，算
$$
{\partial loss \over \partial z}的时候

\
$$
让他分母上没有任何东西而分子上，就等于
$$
2(a-y)\over x
$$
的形式由于tanh的导数是（1+a)(1-a),这样分母上就抵消掉了，改了loss function 之后做链式的时候就会变成一个很简单的形式，分母上没有任何东西。第三步，loss function 小于0是不满足要求的，1-a属于0-2了，1+a也属于0-2了，所以我们用


$$
{1-a\over \ 2}和{1+a \over \ 
2}来表示，这样都属于0-1，loss function就会大于0
$$
求导的时候分母没有东西，最后loss算出来大于0，这样就能满足tanh函数的整个条件。所以我们loss function要改就是为了适应tanh的反向。loss function的形式改了以后发现loss function已经小于0 了，这是不满足loss function条件。



## 14.

所以改一下它的输入域，变成0-1之间，这样满足loss function大于0的固有条件。所以从二分类来看有三个主要的条件，第一个是样本标签必须是0和1，在训练的时候假设是男和女，你一定要给他变成0和1，第二个条件是loss function和logistic函数一定要配合使用，不能说用MSE去对应logistic或者用loss去对应tanh，这样都不行。这就是二分类部分的，然后我们看一下多分类，刚才是楚汉，现在是三国，魏蜀吴就是蓝点红点绿点，我们先从直观上来看，应该有三条直线来将它们分开，

先来看线性问题就是说两条直线能把他们区分开，非线性就是说必须用曲线才能搞定，这是线性和非线性的区别。再看二分类和多分类的关系，同样是三个类别，第一个例子是说一对一的分类，把这俩单独训练，在训练的时候把这部分样本扔掉。但是你需要训练3个二分类，这就是一个一对一的关系，我们这么做，但是一对一的关系有个问题，预测的时候当告诉你一个蓝色样本的时候，你不能确定他就是蓝色样本，可以确定他不是绿色样本，但他有可能是红色样本，因为红色样本并不在里面。下面是一个一对多，是这么分类，这回不扔样本，但训练分类器的时候，他是先把蓝色绿色的样本算成一类，然后红色算成一类，这样train出一个也是二分类，最后做prediction的时候也是要把三个组合起来才能告诉你最后是哪个类别。我们并不涉及到多标签学习，我们只涉及到多分类学习。多分类函数特别复杂，我不想给大家推导，也是用举例的方法，多分类函数是一个叫Softmax的东西，为什么要叫Softmax是因为他是和hardmax是比较比如[-3,1,3] 三个数，他的max就是3，如果变成二进制就是（0,0,1），表示第三位就是最大的数，但他们之间的关系就表示不出来，第二个就是说他们的量纲不一样，没法比，所以没法用于反向传播。所以就要用softmax，假设j=1，m=3，上式为：
$$
a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}
$$


第一个特点是这三个数相加得1，这是以概率，这是一个样本，一个样本属于三个类别的概率分别是什么 。他能告诉你他们之间的差别，这是0和1之间的数，就可比了，我们把这种编码方式叫one-hot，假设我们有10各类别一个样本，x=2 就是[0,1,...0], 只有一个值是1的时候，softmax会给每一个值打一个概率，所有的概率相加肯定是等于1。 

| 输入原始值  | (3, 1, -3)            |
| ----------- | --------------------- |
| MAX计算     | (1, 0, 0）            |
| Softmax计算 | (0.879, 0.119, 0.002) |



最后看就看哪个数大，哪个大就属于哪个类，这儿有一个举例，我们仍假设网络输出的预测数据是z=[3, 1, -3]，而标签值是y=[1, 0, 0]。在做反向传播时，根据前面的经验，我们会用z-y，得到：
$$
z-y=[2,1,-3]
$$
我们看反向传播的时候怎么用，在使用Softmax之后，我们得到的值是a=[0.879, 0.119, 0.002]，用a-y：
$$
a-y=[-0.121, 0.119, 0.002]
$$
我们在计算梯度的时候，如果dw是个负数的话，那相当于给w了一个提升，最理想的情况是让他等于[1,0,0]。 多分类的正向传播，先是
$$
z=x \cdot w + b \
$$
然后是分类计算，也就是softmax, 
$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} \
$$
损失函数用的是多分类的交叉熵，loss就会等于-[y1lna1+y2lna2+y3lna3], 所以最后肯定保留了其中一项。这里用了实例化的方式做了一个反向传播的推导。
$$
{\partial{loss}}/{\partial{z_1}}= \frac{\partial{loss}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_1}} + \frac{\partial{loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_1}} + \frac{\partial{loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_1}}  
$$
先算exp,把这三个相加的一个和，再把单独的去除，得到a1,a2,a3, 现在A1不但从Z1来。





## 15.

因为有个求和，所以Z2，Z3对他也有贡献，而算logistic的时候，Z1，Z2，Z3之间都是没有关系的，最后算出来是a-y，公式21和公式22再结合损失函数的时候，最后就会得到a-i这个数，需要强调的是softmax先做了一个Z1，Z2，Z3， 再做exp,就变成e^z1, e^z2和e^z3, 这都是在矩阵上直接完成的，然后再做一下sum，现在有个改变就是要减这个np，当x很大时，np.exp(x)很容易溢出，因为是指数运算。所以要改进的代码，减掉最大值，这个数就非常小，所以要用Softmax2这个function来解这个数，防止大于100，多样本的时候可能有10个这样的，所以要单独对每个轴做sum，然后我们看看他的实现。可以理解为三个神经元，前面有两个特征值输入，所以这个是个2x3的矩阵，我们都要先看一下样本的基本属性，看要不要归一化，然后把one-hot编一下码，有三类，一共140个，
$$
Y = \begin{pmatrix}y_1 \\ y_2 \\ \dots \\ y_{140}\end{pmatrix}=\begin{pmatrix}0 & 0 & 1 \\0 & 1 & 0 \\... & ... & ... \\1 & 0 & 0\end{pmatrix}\
$$
1表示第几类，这里有代码，one-hot是怎么转的，反向传播最困难的地方已经过去了，就是a-y，计算损失函数会多一个CE3

```
 def CE3(self, A, Y, count):

​        p1 = np.log(A)

​        p2 =  np.multiply(Y, p1)

​        LOSS = np.sum(-p2) 

​        loss = LOSS / count

​        return loss
```

,多分类的损失函数，CE3的实现看着麻烦其实不是，相当于-ylnA /m, 最后有个Sigma, 前向计算是调一个inference，这里做了归一化了，我们跑一下看看， 跑出来的四组数是横着看的，因为0.7最大，所以我们认为是第一类，后三个样本也一样，都横着看。第三个差别特别小，0.38,0.37,0.24,只大了0.01，第一步我们得到了结果但我们不知道对错，我们用一个老办法，也就是可视化的办法去解。





## 16.

可视化我们先跑一下code看看结果，然后我们再来说这个怎么做，他先显示原始数据看看对不对，这里的learning rate是0.1， batch size是10，这是一个结果，这个蓝点确实是处于分类边界，靠上一点点，所以还是故意选了这么一个值，先看最外端的代码，其实很简单，先是simplereader，然后read data, 然后normalize, 因为在输入的时候，x的最小值是0，x最大值是9.9，x我们先要归一化把他们变成0-1，用了normalize(), 原始值是1,2,3，我们要把他变成0,0,1， base是1， input是2是因为只有两个输入，然后超参里面batch size是10，最大循环100轮，0.1是学习率。这里的predict是做原始样本1-10的时候把x最小值等于，x最大值等于10，把这个数记下来放在datareader里，inference就是把一个数组传进去，是个4x2的。

开始数据其实蛮简单，先调了一个函数，告诉你一共三类，第一类是蓝色方块，第二类是红色x,第三类是绿色圆圈。下面就取下标，0-2，我们看显示结果，先把分类样本点画出来以后然后画那几条线，我们反过来看原理，当标签值是[1,0,0]的时候我们看loss值，只有一个值有效，只有第一个有效，如果标签是第一类，我们算的0.879也是第一类，他非常match，所以loss值是0.123。 这三个数里面，数字越大表示传播力度越大，为什么没有两个数同时最大呢，是因为本来是一个第二类的样本，我们给他错分成第一类了，所以对第一类惩罚很大，对第二类惩罚也很大，第三类挺好的。我们先看看多分类的几何原理是什么，基本是用一个一对多的关系去用的，softmax对Z来说是单调递增的，我们把Z1，Z2，Z3的共识都带进来，再把X2挪到左边来，我们先假设：
$$
w_{21} > w_{22} > w_{23} \
$$
所以公式13，14左侧的系数都大于零，两边同时除以系数：
$$
x_2 > {w_{12} - w_{11} \over w_{21} - w_{22}}x_1 + {b_2 - b_1 \over w_{21} - w_{22}}, 

x_2 > {w_{13} - w_{11} \over w_{21} - w_{23}} x_1 + {b_3 - b_1 \over w_{21} - w_{23}} 
$$
再简化一下就变成
$$
y > W_{12} \cdot x + B_{12} \tag{18}
$$

$$
y > W_{13} \cdot x + B_{13} \tag{19}
$$

凡是这两条直线以上的还是属于第一类，所有的蓝色点都在绿线以上，并且所有的蓝色点都在红线以上，它就属于第一类。这里有个问题，神经网络到底是一对多的关系还是一对一的关系，我们从这个图上可以看出来好像是一对一的关系。从Softmax这个公式来看，他是一对多的关系，因为同时兼顾了a1a2,a1a3.理想的情况应该是这样的直线，蓝线把所有颜色点都区分开。

神经网络中有这么一个公式，Z1=Z2且Z1=Z3，这是在边界上，对于蓝色的线来说，是绿线和红线组合在一起才是他的分类线，这样就能做到一对多的关系了。



## 17.

讲逻辑与门或门实现，这是一个二分类的具体表现方式， 与操作是试图找到这么一根直线，可以一条直线把红色和蓝色点分开。训练样本就不用npz data,直接写就好了，每个都是四个样本，与非，或非这些。从simpledataReader去扩展一个类出来，他是一个子类，然后的四个函数分别是读取不同的数，不用从文件里，直接从内存里读就可以了。



## 18.

把datareader弄进去后先画出原始样本点，然后超参，batch size不用改，inference的值和标签值，difference要小于1e-2,用与来解释，神经网络是不可能得到数学解的，只能得到概率值的。train出一根线很简单，但要达到精度就要花点时间，我们看一下结果是啥，先画出四个点来，这应该是逻辑与。这里有一个特点，精度越高，这两个点的中点越会接近于0.5,0.5是一个极限值，应该达不到，这个true怎么来的，上面四个输出值，第一个是一个特别小的数。第三个是或，左下角是负的，其他三个点是正的，训练了2266次，比与要简单一点，四个值也是满足1e-2的这个条件，第四个是或非，或非和或应该训练次数一样，2266，对于我们目前学习进度来说，多训练几轮就能保证精度，这就是逻辑门的一些东西。

我们用debug的方式看看Softmax, 到底是用x=1还是x=0, 看Z的时候，这是10个样本，每个样本都有分类值，然后看exp(Z),a的值我们先拷出来，要横着看，竖着看是样本没意义，三个相加是1.keep dimension的意思就是说要保持是两维的，现在我们debug一下整个过程，这样对nertalnet这一部分就会有一个比较好的了解，我们看y的值，self.yraw 是123，还没有归一化。normalize要做的事是先取feature，shape1是2，x_norm是把x的最大值最小值存起来，是一个2x2的数，columize是把原始数列第i列取出来，所以出来就是一个140的向量，可以理解为140x1, 然后取最大最小值，他记了一个mean value， 一个scope，归一化就是最大值减去最小值再除以scope，所以现在new column已经是一个0-1之间的数了。现在的x_trian就是归一化好的了。

进neuralnet的时候就初始化赋值，一个w,一个b，然后train，train的过程new 一个history，这就是记录我们一些历史记录，画图，然后loss function, 这个type我们看是multiple classifier,checkpoint是用了14次，每迭代14次就去check一次，就相当于每一个epoach去check一次，max epoach是我们给定的值，这里的shuffle我们先给了一个seed，取到一个xp，然后seed同时对yp有效，这是一个shuffle函数，打乱，这两seed都是16，意思就是x和y的打乱顺序都是一模一样的，否则x和y就对不上了。 进内循环，先batch，得到batch sample给个batch size是10，每次都算一下start和end，第一次相当于取0-9，然后做一次前项，矩阵运算看分类，然后进softmax，因为现在都是0，所以softmax所有值都是0.333，前项完了以后进反向，反向就是a-y，然后取db,dw就好了，虽然都是0，但不妨碍db, dw不会是0，因为batch y 不是一个数，这里的iteration是13，14次正好是我们epoach。



## 19.

loss值跟踪一下F11，现在我们四个样本，现在我用这个样本去train，w,b都会根据这个样本来就算出最佳值，但现在check loss的时候，用这个样本一定很小，因为train过了，所以我们一定要用四个样本一起去算这样才公平值，否则第一个样本特别好，得不到正确的结果。check loss的时候每次都要取一个权值。所有的样本都要去check才合理，check loss得到样本数，应该是140，然后做一次前向，得到A值才能去check，初始化的时候我们指定了type，他会分支到三分类上面，loss最后得到等于1.04.如果loss小于eps的话，0.01的话就会break。两个停止条件，一个是小于eps,一个是epoach等于100. 我们看predict是怎么工作的x norm是刚才我们做归一化的两个值。然后是做inference,F11相当于做一个前向，并且返回output。F10是什么，r值是2311，整个过程就是这样。

现在我们一般分成两类，一种是机器学习也就是machine learning,新一代的就叫deep learning. 里面的SVM就是做分类的，但精确度不是很高，SVM的好处就是他需要的样本数比较少，第二就是速度非常快。

machine learning里面还有个比较有用的就是decision tree神经网络里面没有这个，做决策树的时候一般怎么用呢，一般做ranking去用，基本就是if...else的形式。神经网络是监督学习，监督学习里包括linear regression, polynomial，他的意思就是多项式回归，这条曲线去拟合一般用多项式回归，第二种方式是两层的神经网络，做非线性拟合。

decision tree是监督学习，然后看classification里看分类器，KNN是k临近算法，就是K nearest neighbor。如果本来能画一条直线把他分开，肯定是一堆点离得近，然后logistic就是逻辑函数。

贝叶斯也是很成熟的.

第十章里面的双弧形，我们要使用线性二分类解决不了，必须用非线性的。第二张图是把它拉长了，撑长了之后弧形的效果就比之前的效果要大。

下面三张图还是一个平行变换，画一条线已经可以线性分类了。高维的时候就看不出来了。这图完成是用神经网络的公式来写的，我并没有人为的去加一些东西。



## 20，21

把连续值x变成一个y上的曲线，一共就6个参数。

超出范围可能就是不准，然后我们拟合出一条直线过来，他的训练样本的值域在这儿，拟合出一条直线，但不一定对。这个图的范围是0-1的，如果我给一个1.2的话就不知道在哪儿了。我们不知道拟合成什么样才对，很多问题是没有可视化的，w,b不是唯一解，有两种办法，一种是测试，用数学解的方法就是为了得到一个精确的解，得到一个相当于baseline的东西，实际上正规方程也属于最小二乘的一个例子，他只不过是一个矩阵的方式。

最小二乘如果是在高维的情况下，或者二维的情况下是ok的，一旦学好了神经网络，我们就可以用神经网络解决任意维度的问题而不止二维的问题，整个思路就是这样，复杂问题不一定能解。

对数几率，这是要解决分类问题，然后求ln之后就变成线性问题了，把一个分类问题变成概率，这个对率会变成一个非线性的东西，既然加了ln能变成一个线性的，那就用线性方程来表示他，线性方程本来是用来做拟合的，一旦用了这个方法我们就可以用来做分类，这就是对率函数的由来。用另外一种几何的方式来解释，用一条线穿起来。

每次实现计算的时候都是一个概率，先把这几个值代入公式，Z=X1W1+X2W2+b, 算出Z的值，A=logistic(Z),算出来是0.65. 然后算出在0-1之间的概率值是多少，实际上在坐标上所有点都有概率值，把三个是0.5的点穿起来就是一条分割线，就会把大于0.5和小于0.5的给彻底分开，这样就会把分类问题转化为拟合问题。

其实不应该用分类和拟合去比较，因为拟合是不算概率的，拟合只是一个方式，他是算距离的，而分类是算概率的。所以概率计算一定要记在脑子里，后面的多分类也是概率计算，多分类就是在一个0-1的空间里面，有一个点分别属于三个类的概率是a,b,c，而其中a+b+c=1, 三分类也是算空间中某一坐标的概率的。