## 24

不知道外面的油泥和对照这个。不是有两种黑纸了，所以让他一个鼻子一个黑点儿，就s得了这种。困难就可以了。双层网络所以有激活函数，我们看看激活函数是什么。

神经网络二层比之前的复杂一点，只需要克服一个困难就可以了。双层网络所以有激活函数，我们看看激活函数是什么。

就开始的激活函数是什么东西，就是Sigmoid函数，继电器，从高低压到低电压，一合闸的话会有火花，所以他会有一个继电器，继电器会在低电流状态下有一个电流产生磁性，把高电压自动合上，产生了一个阶跃信号，就有一个问题就是导数是0，基本不能用。

强调一下，激活函数跟分类函数的区别，我们在单层的时候，输出的时候没有激活函数。单层的线性回归的时候，这时也没有分类函数。

单层的分类，二分类多分类的时候，就会有个二分类激活函数。神经网络最后一层是没有激活函数的，是有分类函数。激活函数是用在两层之间。

解压性激活函数，就是昨天大概说过了，就是对数几率，Sigmoid函数的意思是S型的挤压型函数，他是一类函数实际上。用来用去最好的就是对率函数，所以把他叫做Sigmoid，原来是一类函数的统称。从数学上来看，Sigmoid函数对中央区的信号增益较大，这块最大是0.25，就是这一点的斜率是0.25。就是这么看的。

所以0.25传回四分之一的信号值，所以不是特别理想，在大于4的时候，到了快0.9以上了，这个4很容易

所以那个Z就很容易超出去了，就造成Z很大，这个输出值域就变成这个地方和这个地方。反向的倒数特别小，0.00几.

这个Sigmoid函数一个一个往回传，这个梯度很快就消失了。这就是梯度消失的问题。

这个图是用实例说明了一下，输入值0.9，然后算那个data最后是否接近？那个最后是否接近0.5的时候，他用了很多很多步。

哦，就是在这儿一步两步，三步四步，五六步，一共67次，就说明他的最开始的这个斜率就特别低，大概不知道。后面会看一个函数，收敛速度就会快很多。Sigmoid有这个漂移，因为他是0到1。中值变成了0.5。他是有个漂移，看看双曲线正切函数是这样。

为什么写成这种形式？这两种形式有什么区别？有有人能回答这个问题吗？为什么用第二种形式，经常我们用第一种形式。啊？应该是那个算了少一点儿，因为因为这个再没办法。

所以他最后只算一次，前面要算两次。最后它的计算的次数就会多一点儿。对这个事在coding的时候，要时刻注意的那种地方。

那个有一个的优点就是说它是零均值的，因为这点是0。他第二个好处，他的零点的斜率是一，所以如果这个方向的时候，中间那个区域的话，他会基本上全都传过来，在这个时候比那个Sigmoid的地方。

其他的一些函数，那都是s型的，只留下了一个对数行的，为什么不留他，一个是比如计算量考虑。

一个是考虑反向，计算量要大，你从算法上看不出什么区别来。

然后再看看半线性的激活函数。ReLU函数，小于0的时候就干掉了神经元。

正向的时候也是被干掉了，特别大那个为什么不会梯度都爆炸了，这个以后再说。
他最大的优点，也就是说，你传过来一个梯度值，比如那loss一个惩罚10，他那个反向的时候，就是成为10了，Sigmoid就变成2.5了，最小可能是0.0几。

他会避免梯度消失。这个神经网络活跃度使得计算机整体的计算会下降。

他意思是说他有很多情况，整个在矩阵运算的时候很有可能算出来，我假设啊假设，也就是说其中有一项等式小于0的。多层的时候一堆的神经元，小于0的时候就给灭掉了。

属于死神经元了。他一个问题，就有很多死神经元，死掉了就再也回不来了。这个梯度很大到时候会导致神经元死掉。

这个刚才说死神经元的时候，他会有一个立刻的leaky relu。

Elu最近用的比较多了，他是平滑的曲线。我们再在这个这些东西也就用了relu。知道就好。

我们可以进到单入单出的双层神经网络了。我们先说一个问题，我们怎么拟合正旋曲线。

分类很容易解释，就是假设我们有三个样本，然后他本来属于第一类第二类第三类，然后判别的时候就是第一类第二类第一类，拟合这儿有一条曲线，然后就不太好算了。他的评估方式是用二次方来评估的。

讲一下用普通方法是怎么解决正弦曲线的拟合问题的，就做多项式拟合，在神经网络没出现之前还经常经常用的一种方式。

就刚刚这个例子吧，如果有这条线，他会有这条曲线曲线去拟合。

把X自平方一下，比如X的三四次方，就是左脚踩右脚，这当然有用，$$z = x_1 w_1 + x_2 w_2 + ...+ x_m w_m + b $$

就得到了标准方程。

先看这边有一个例子。多项式次数过高，过拟合了。所以次数越高，权重值会越大，特征值与权重值的乘积才会是一个不太小的数。

所以都用二次多项式，就是乘以平方然后加上去。

## 25

假设是一二三四五这么一个东西，五行一列，然后平方。

然后shape把这两个合在一起，一个五行两列的一个样本，然后主程序，用的二次多项式拟合。

先看这个loss，一个是噪音太大，开始到这会一种是这样，这说明网络的能力不够。这个我们怎么做的，用前面学的一层的神经网络去做的，那个房价拟合。

不是一个直线，往下弯一点点，所以二次项不能达到预期。

把训练数据的再增加一列x的三次方，变成五乘以三，input就会是3，再次运行就是一个非常漂亮的值。

大家把ch09打开，每个人都运行运行一下。

然后我们再看一个蛇形达到东西。

我们用这个多项式去拟合这个眼镜蛇，刚才看到多项式三次就可以，这个肯定不行。所以我们可以从四次多项式开始做一下，效果就是这样的，但总感觉肯定没戏。

看这个损失函数，这个不死心，我们又训练了十万次，关键是看这个玩意平了，为什么每次都打印出来。可以看出来没什么希望了。

损失函数值到了一定程度后就不再下降了，说明四次多项式没戏了。然后我们用六次试一下，看起来还凑合，那个趋势要好一点，到后面跳的太大了。直接用6次多项式来拟合，看这几个玩意，六次也不够。

用八次还可以，弄了一个10000万，两个小时。所以说到八次多项式，拟合能力还是可以的。

多项式拟合虽然可以，但是代价非常高，咋们休息一下，回来再讲。

## 26

来我们看看两层神经网络怎么玩这个曲线。我们看看万能近似定理，91年就有了，有几个条件，

第一个两层前馈神经网络，第二个条件至少一层具有任何一种挤压性质的激活函数，就是相当于Sigmoid。

第三个是说只要隐层的神经元的数量足够，就那个hidden。所谓足够是对不同的问题来说的。

第四个它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。当然这个函数需要是单调递增有界的。

像正弦曲线就是这个例子。

ReLU函数能不能做，待会我们再说，我们先用Sigmoid函数来说。

我们知道一个大的MLP一定能够表示这个函数，左mlp就是多层感知机，他是神经网络的一个基础。

OK，然后我们用两层神经网络去拟合他。我们先用三个神经元去试一下。两个三个四个五个测试。

因为有些东西需要可视化，所以尽量用低维的，人可以理解。所以他这个就变成了两个W。

代表第一层，第一行第一列


$$
W1=

\begin{pmatrix}

w^1_{11} & w^1_{12} & w^1_{13}

\end{pmatrix}
$$
要有激活函数嘛
$$
A1 = \begin{pmatrix}

​    a^1_1 & a^1_2 & a^1_3

\end{pmatrix}
$$
所以前向计算就是这个样子。要写成矩阵就是这个某样。
$$
a^1_{1} = Sigmoid(z^1_{1})

a^1_{2} = Sigmoid(z^1_{2})

a^1_{3} = Sigmoid(z^1_{3})

A1 = Sigmoid(Z1) 
$$
 激活函数A1就是a1，a2，a3这个东西。

再往后推得话就不用写那些了小东西了。

然后反向传播这里要说一下。

前面为什么有个二分之一，这是数学家算出来的，导数一乘就没了。

求损失函数对输出层的反向误差
$$
{\partial loss \over \partial A1}

=

\begin{pmatrix}

​    {\partial loss \over \partial Z}{\partial Z \over \partial a_{11}}

​    &

​    {\partial loss \over \partial Z}{\partial Z \over \partial a_{12}}

​    &

​    {\partial loss \over \partial Z}{\partial Z \over \partial a_{13}}

\end{pmatrix}
$$
注意W2的形式是竖着的

所以要求偏loss对偏w2矩阵的求导
$$
d W 2=\frac{\partial \operatorname{los} s}{\partial W 2}=\left(\begin{array}{c}{\frac{\partial \operatorname{los} s}{\partial z} \frac{\partial z}{\partial w_{11}^{2}}} \\ {\frac{\partial \log s}{\partial z} \frac{\partial z}{\partial w_{21}^{2}}} \\ {\frac{\partial \operatorname{los} s}{\partial z} \frac{\partial z}{\partial w_{31}^{2}}}\end{array}\right)=\left(\begin{array}{l}{(z-y) \cdot a_{1}^{1}} \\ {(z-y) \cdot a_{2}^{1}} \\ {(z-y) \cdot a_{3}^{1}}\end{array}\right)
$$
对矩阵的每个元素做相同的求导。

W2本身的形式是这个样子，我就简写了。

最后就成这个样子
$$
=\left(\begin{array}{ccc}{a_{1}^{1}} & {a_{2}^{1}} & {a_{3}^{1}}\end{array}\right)^{T}(z-y)=A 1^{T}(Z-Y)
$$
所以他必须是一个向量或者矩阵才能推出这个T来。推出来整个就是1，就是z—y

下面求损失函数对隐层的反向误差，这个部分呢怎么求Dw1跟Db1。

我先看Dloss比DA1的
$$
\frac{\partial \log s}{\partial A 1}=\left(\frac{\partial \operatorname{los} s}{\partial Z} \frac{\partial Z}{\partial a_{11}} \quad \frac{\partial \operatorname{loss}}{\partial Z} \frac{\partial Z}{\partial Z} \quad \frac{\partial \operatorname{los} s}{\partial Z} \frac{\partial Z}{\partial a_{13}}\right)
$$
就这个图我们看loss对A1的求导。

最后这个$$
(Z-Y) \cdot W 2^{T}
$$就是这么过来的。写成矩阵就是这样。然后z1到a1就比较简单了。

## 27

用矩阵算起来是a点乘1-a，不是叉乘，刚才loss点a1算好了是这个玩意。再往后推一层把那个a1加进去就变成了公式8，然后再乘以这个玩意就OK了。

Da1就简写了，他也是点乘的关系，就得到了神经网络的梯度，这块，一层两层，中间不管有什么链接，然后反向的时候，一旦有了Dz1他可以干任何事。

有了Dz1这个梯度传进来，做自己内部的一些计算。求Dz1是关键的一部，最后得到的结果就是这个玩意。再点乘Da1，然后是这么一个形式，然后后面就一模一样了。

这不一样了，这么以前五六七学得都是一层网络，所以这个2就是表示两层网络。

```
self.Z1 = np.dot(batch_x, self.wb1.W) + self.wb1.B
self.A1 = Sigmoid().forward(self.Z1)
```

这有一个Sigmoid函数，实例化，以后调他的forward，一会我们再看code，我们先大概的说一下。

调调他的forward就相当于一个前向计算，然后第二层就想到于a1乘以w2加b2，然后看一下，由于我们是做一个拟合，所以到了这个以后直接走这个else。所以self.A2 = self.Z2，否则A2必须要通过二分类和多分类函数单独计算。

我这个公式跟代码都对上了，记住这个顺序，跑的时候会告诉你跑不过去。单值要是在一个维度上做，做python运算的时候不知道会是什么样子，不同的格式进行运算。我们都用矩阵方式来计算，避免出什么错，这里又多了一个能力就是保存Result。想用的时候load进来，

打出来的图像都是两个，这边那个大家都熟悉了就是loss，然后又边那个是什么？然后每个里面都是两根线，给大家解释解释这两根线是啥。

一般会给你一个训练集，测试集去测试，对于我们一般的工程，自己分训练集和测试集，我有一万个样本数据，训练的时候，只有一层的时候，同样train。然后在做loss时候还是用这个样本，loss你是不能用当前的五个样本去验证，这样不合理。

现在到了多层神经网络的时候，我们用一个标准的算法。xtrain比较大，算起来很慢。会有一个validation，dev，从这出来的。不参与到数据训练里来。

蓝色的线就是xtrain，就是跳跃的比较大。红色的线就是xvalidation比较稳。

评估一个模型的好坏，就用accuracy。

我们看一下模型，这里面用两个神经元就把正弦曲线搞定了，已经很不可思议。

一共七个参数搞定这个曲线，这是一个连续值。

然后我们在看一下loss值比那个xtrain要高一点。作业level2，这个是level4，

我们过一下这个code。

helperclass2是为了双层神经网络。

神经网络随着初始化的不同有很大的不同。

## 28

我们把他的初始化也给记下来了。

不会因为初始化不同，而出现不同。

我们来看两层神经网络有什么不同。

WB他已经用一个类来表示了，越来越多的话要写多少，所以就用了wb。这个类是长什么样的，我们可以debug、一下来看。

createnew等于false就是不create new data。

这个w12是指一行两列，指初始化，如果有就load进来。我们看一眼是什么样。因为b的初始化时0所以没关系。w不许是0，因为0是对称的，也就学不到东西。

debug代码，吃饭。

## 29

曲线拟合，我们做过实验了。

OK，刚才是121这个模式，我们看111这种模式会怎么样。他这个log到了0.04的时候就再也下不来了。accuracy到0.8的时候就上不去了，基本他拟合就是这个样子。

这个拟合其实跟那三个分段线性一样，这就是所谓的万能近似定理。神经元的数量要足够多，就是这个意思。对于这个sin曲线，隐藏的一个神经元是不够的。然后把这个改成2再试一下。我们再看看复合函数的拟合情况，2的话能拟合成这个样子。一共四段组成的。

对于这个问题，隐藏两个是不够的，必须要用三个。那我们用三个来举例。最后我们跑一下level4的code，复杂曲线的拟合。

这里有要说的地方是这，你看在到一千的时候，这个速度下降已经比较平缓了。有时候这个地方就这么一直横着下去。但是对于这个问题来说，他不是。就是梯度在特别缓，突然就下来了。

如果你的loss是这样的，还想再下来，一般的情况是等一等。所以不太好说。这也没有理论上的一个依据。

看这个图，两千四千的时候比较平缓，到四千五的时候就下降了，然后又上去了。

我们再看这个拟合程度，基本上就是太粗了，不太好拟合，可以回去试一下。刚刚这个拟合度是百分之九十六点八。

所以，这里面我们一共用了多少个参数呢，一共是10个参数。而且我们用了四千个，这说明双层的比单层的神经网络要厉害很大。

然后广义拟合。

在实际的过程中，哪有那么多可视化的，都是多元的。如果是多元的有三个输入，五个单元，这么拉就好了，w2是一个五乘一，然后输出。

广义拟合可以用于多变量的复杂非线性回归。

多项式为什么能拟合曲线，本来是拟合成一条直线，我们这边有一个例子在level5里面，x平方往下转，三次方往上转，所以他可以获得非线性的能力。

我再看看神经网络原理，左侧为单特征多项式拟合，右侧为双层神经网络。

先只看右边的，神经网络后部分，第二层跟多项式网络结构是一模一样的，所以Z1Z2Z3一定能拟合一条曲线，这已经证明了。第一层这个x是经过一个线性变换把他变成了Z1Z2Z3，就是乘以W再加B。所以就知道神经网络线性变换把一个X变成Z1Z2Z3，就相当于多项式拟合。

第二层是跟多项式拟合同一个道理的公式。

但是光有线性没用，这有个a1，把线性变成了非线性。

具体看怎么做的变换。

我们有一个黑色的线，sin正弦曲线。



## 30

这个东西，不用看y坐标，只看x。就是把他比如分成一百份。就是从0到99，黑色的线就假设给他分了二十份吧，前提我们先有一个两层的网络来求出w21w22，一共有这七个参数了。把他带回去，然后x就是0到21，带进去得到一系列的z1z2，然后得到z1是红色的点，z2可能是绿色的点。

为什么一条直线能分成两份直线，就是做了一个线性的拆分。就跟x，x平方这两个道理是一样的。再举一个例子，就是有一个复数拆下来。

这样黑色能拆成红色和绿色两条线。

做完线性分割完，这儿有个激活，激活成A1和A2，a1a2把正的都变成从0到1之间的数。你看这个红线，由于坐标系的问题歪了一点，然后这个黑线还在这。然后这个红叉呢，就是那个激活函数，激活函数已经是那个圆的了，已经有曲率了不是一个直线。

斜度比较小，黑线是一个y=0.5的直线，x=0.所以他这个斜率越大，曲线越明显。然后我们接着往下看那条绿色的线，绿色的线也是Z2，激活函数变成a2，就是比较明显。这个差也比较大。

通过一个x变成z1z2再变成a1a2，是一个线性组合，从那个地方到那个地方是一个线性拆分。

计算这个值肯定用这个公式，就是神经网络第二层的公式。通过相同x值的红点a1和绿点a2，经过公式6计算后得到蓝点z，就是那个叉子。然后把他经过一个线性变换，那个公式$$
z=a 1 \cdot w_{11}+a 2 \cdot w_{21}+b
$$31,正弦曲线就这么形成了。

一个参数能控制一堆的事，是几个参数组合起来能控制一堆的东西，团队组合起来作战能干很多东西。

大家明白了吧，从线性到非线性的东西。0到100分成100份，竖着的他就变成一个曲线了。转了一个九十度变成了不同的y值。

两个神经元搞不定，用了三个神经元，多一个东西就变成3条直线，然后在分成三条曲线。着跟几维没关系，因为我们都用横着的线性去做比较。

这不是数学证明，这只是我们事后了解一下是怎么一个情况。计算机不能直接获得w，必须要train，数学家能直接把w算出来。

最初的wb可能是初始值，我们把他train出来的。

参数调优

我们有一堆参数，那个属于超参。wb不算超参，算普通参数，下面那些东西算超参。比如输入层神经元数。第二个隐藏神经元数，第三是跟拟合相关的1不能动。第四个是斜率是可以动的，第五个批量是可以动的，然后最大epoch也行，然后损失门限值不用改，然后损失函数不能动，然后Xavier可以不用动，所以三个玩意隐层神经元数，学习率，样本批量。避免权重矩阵初始化的影响，create_new=False，只要隐层神经元数量不变并且初始化方法不变，就会用第一次的初始化结果。true的话他又会new一回。

手动调整超参数，学习率，隐层神经元数量，批大小，别三个一起变，先固定好。

我们先固定好，学习率和隐藏神经元数量，是这种网格。跑了一些数据，这个eta不是越大越好，合适最好。

## 31

神经元多的时候，没准eta取个0.01是最好的，也许永远也不会好，就是说多了，效率反而会低了。神经元多了速度就慢了。

我们看学习率的调整，在我们固定这个这个玩意不变多的时候，10000次的时候还没有收敛。0.7的时候是3500次。这个怎么解释啊，这个学习率其实在前面有一个解释。是说这个是终点，学习率大是什么意思呢，梯度在这，学习率大的话w每次变化比较大，可能跳出去了，下次还那么大又跳回来了。就比如打把一样。

所以有一种算法叫学习率衰减，在正确的一个loss训练过程中，在这我们可能用学习率0.5，这可能就降到0.1了，开始的时候我们步子比较大，然后也来越小，不断的修正。

然后再看批大小，我们保持隐层神经元ne=4、eta=0.5不变，调整批大小，15的时候也下不去。然后用5个10个的时候是比较OK的，最小1又不行了。

我们来看隐层神经元的数量，batch_size=10、eta=0.5不变，调整隐层神经元的数量，神经元是2，4，6，8。

2拟合能力达不到，4OK8千次，到12的不知道怎么样了，自己可以试试。所以8反而是最好的地方。

为什么3000次到6000次之间为什么有个平缓的坡，有一个鞍点概念，即一个维度向上倾斜且另一维度向下倾斜的点。鞍点就是说在中间那个圆点上，一个维度上梯度上升，一个维度上梯度下降。他在这可能造成平滑，来回晃可能。如果找不到就会这特慢，一直到这突然下降。有时候是整个解空间是一个曲线，梯度下降到这到这又回来了。这个点叫做局部最优，这个点全局变量最优。有可能产生这种情况遇到一个坡上不去了。

最好的是那个绿色的，下降的比较满意，红色特别高的学习率，后面就下不来了，在最优点附近来回的跑。黄色的曲线太高了，发散，蓝色曲线有点低，一步一步往下走，收敛速度慢。

我们在课堂上做作业二，隐层神经元：4 ，批大小选择：1，5，10，15，学习率选择：0.1，0.3，0.5，0.7，提交一个四乘以四的表格。level4 。epoch最多给10000就好了。1做批量可能上不对，改成2.

## 32、33、34

level6更麻烦，输出一堆东西。

大家填那个epoch的10000的话的话，如果有可能的话，最好把这个也填一下，看看哪个精度会更好。没填的一会再跑一下填，否则说明不了说明问题。

批量最小不用那个1了，用2。

vs2017是可以的，我们在这跑一下，然后不管，我把他改成10再跑，又会出来。

冲突的概率特别小，除非特别特别快，一起去调用这个shelf，

批大小跟学习率有一个正比，批越大，学习率也就越大，batch size越大，学习率也要越大。

第一个作业改成五次方，因为代码已经有了四次方。

