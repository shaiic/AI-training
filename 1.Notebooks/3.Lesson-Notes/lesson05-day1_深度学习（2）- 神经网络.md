<<<<<<< HEAD
## 2

今天我们来学习神经网络的基本原理，这是理论课。首先为什么要学这门课？这段教材系统的从入门到进阶，系统的把神经网络的基本给大家介绍一下。他把深度学习归纳成9个步骤，基本概念是最基础的也是最耗时的，把基础学好了，后面就比较快了。

我们来看基本概念，现将基本原理，再讲是怎么工作的，然后再介绍需要前导的东西，导数的推导和反向传播的一些公式来主导一些概念。这是神经网络的基本工作原理，这是一个神经元模型，它有多个输入，每一个输入给的权重不一样，第一个就是输入，外界的信号，对于人工智能来说就是多个数据的样本属性，有几个特征值就有几个输入，那么如果有多个样本怎么办呢？就是一个矩阵，因为每一个权重不一样，w就代表特征值，再有就是偏移，是因为在脑电波中，要保证输入信号大于某一个临界值，才能激活细胞处于兴奋状态。经过一个线型的计算之后，把这些都算出来加在一起，大于等于t之后，就激活了，整个值就是最后结果，有时候我们输出是个线性的，但我们不想是线性的，后面我们就需要一个激活函数，激活函数在神经网络中是非常重要的一个部分，A不见得是必须的，但加上他之后会比没有他效果好很多，在有些情况下必须要有这个激活函数。这是一个非常经典的激活函数，它是非线性的。

小结我们来看一下，一个神经元可以有多个输入，但只能有一个输出，然后再有神经元的权重数量和输入数量是一致的，一个神经元只能有一个偏移一个bias，一个神经网络训练的过程，也就是不断改变w，b的值，知道他们达到一个比较好的值，激活函数不是必须有的，如果没有激活函数的话，A就等于Z。 神经网络训练的过程我们通过这个图来看一下，他是两个神经元，就有两个输出，三个输入配给了两个神经元，所以序号是不一样的，w的第一个值指的是输出，第二个指的是特征值，B就有两个。我们简单地把训练流程走一下。



## 3

均值，方差中心化       去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

## 4

 均值，方差中心化     去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

神经网络最重要的三大概念，反向传播，梯度下降，损失函数。第一个例子是猜数，初始化就是一个蒙的过程，损失函数就是乙告诉甲的过程，梯度下降的目的就是为了让损失更小。，达到最接近真实值的结果。 第二个黑盒子，就是输入不等于输出了，只能看到输入输出值，而不能看到中间做了什么事情。现在就是要通过通过一种方式去模拟，近似得到一个黑盒子。第三个就是比较接近于神经网络训练的过程，打靶。神经网络不是一次打一个靶，一般都是多发，取平均值可能就是更稳定的发挥。 样本里就有一个输入一个输出，输出就是标签值，真实值。一般情况下我们会有个假设，正负表示方向，整个模型就是训练一个权重的矩阵。我们单样本叫误差，多样本叫损失。我用一些模拟的方法改变函数的权重值，导致我输入的样本能够匹配上，但也有可能样本换了。并不是真实解析了黑盒子，我们只是模拟近似。通俗的计算，整个神经网络训练就是，初始化，第二就是正向计算，第三件事是损失函数，第四件事是梯度下降。我们接下俩介绍线性反向传播。



## 5

 先来介绍反向传播，这个例子有点tricky的地方是，我们为了简单直接把样本值直接大家了，为了最大限度简化我们的过程。在这里面我们要求解w和b的值，我们看到变量w就是我们的权重之一，变量b是权重之一，常数2和1就是我们给的样本的值，他就代到线性方程里面，在神经网络计算的整个过程中，第一步是初始化，所以就给w和b一个值，然后是正向计算，算出x,y,z的值，第三步是损失函数，162和150去比，做一个绝对值的比较，假如z150是一个标签值，反向传播的过程其实就是一个求导的过程，求完导数我们去更新w和b的值，再重新进行计算。 首先是正向计算的结果，我们算出来了，代入就可以了，然后第二个就是反向传播，这个过程要仔细看。加入我们让Z变小，变成150，那w,b要怎么变才能变到150，我们从输出层一层一层往回看，改变它的值，现在是有个w，有个d。我们先看单变量的，单变量我们现在只关心一个b的值。Z的变化是162，b怎么变化，整个过程就是偏Z比偏b,,Z是对X和Y产生影响，X和Y是跟b有关，这里牵涉到连式法则 ，偏Z比偏X是y, 偏x比偏b是3.y算出来是9，x算出来是18，最后算出来的偏导数是3.，偏Z比偏b的过程。一步一步往前走，最后能算出来Z和b之间的关系。反向传播去计算，deltaZ是12，deltab是12/63 等于0.19, 损失函数是预测值减真实值，反向传播给出来两个概念，一个是大小，一个是梯度。梯度下降就是让他方向一直向下走，当b是3.81的时候，这时候Z算出来是150.246，跟150差距就很近了，但我最后结果想达到e^-4,这时候发现这个值还是有点大，所以还要继续算一遍，请自己动手迭代一次。 

 

提示大家，要算偏Z比偏b,核心内容就是求导。这个例子一定要算对，后面都要用这个例子。第一次偏Z比偏b是63，b的值是12/63.



## 6

大家取的位数不一样，结果就不一样，结果近似差不多就好。我在算Z的时候就等于再算了一遍x和y。所以当b改变时，x和y也改了，所以这三个值都出来了。 所以更新b的权重的时候，这里唯一有用的就是这里的算偏导。我们再看双变量，我们这是个近似模拟，双变量的时候怎么求反向传播？偏Z比偏W就是2y。课堂作业就是用python写一下反向传播的代码。第一个就是线性反向传播，这里首先是target
function，给一个w的值，给一个b的值，然后求出x,y,z是什么。第二个就是single的，课下作业之一就是这里分别有个带new和不带new的，请大家思考一下，这个不带new的方式和带new的方式，她差别在哪里？首先我们定义一个error。我的error一定要小于e的负四或者e的负五。如果没有达到这个，继续。如果达到了这个，就break。在while true时候。我们首先正向计算。给定一个w有给点一个b, 都是三和四，然后求出x和y的值。否则我要算出z和w的导数, 在这里求导，就是calculate the factor。所以刚才大家问矩阵求导这个过程就是这个意思。直接把求好的的导数给传进去，矩阵求导，你给他的值错了，那他就算不出来。算完之后x和y就有了导出就能求出来。然后根据这个导数，我们就能算出delta。双变量就除以一个二，因为双变量两个因素都对他有影响，所以我就默认每个影响都差不多，就都除以一个二。这个操作其实是我们一个很tricky的操作。如果这个factor太大了。我们深度学习中有两个重要的调参，一个就是调learning rate，这个值怎么设呢？这是凭借经验，一般情况下都是0.01或者0.1。还有很多算法，就是不断的去改变learning rate。双变量的过程就是做了一个减项计算求导。求梯度。反向传播。给他更新。然后最后再求一遍新的，直到新的结果是对了。整个函数就是做了这个，第一步就是初始化。所以现在请大家写一下现场练习。



## 7

大家一定要自己运行一遍，不然到后面可能会想听天书一样，大家一定要背着把函数写出来。有的时候梯度太大就发散了，在整个计算过程中，单一一次走遍整个神经网络的过程，我们刚刚说了初始化，正向计算，梯度下降，反向传播，再正向计算。对于整个神经网络来说，第一个就是数据，第二个就是要建立模型，这里是一个线性模型，多入单出的线性模型。过程中我们要训练模型，训练有很多种标准，有人用loss做标准，有人用accuracy做标准。当你模型参数固定了，他一般有两部分，第一部分叫计算图，就是我们刚才x算到z的过程，第二个就是每个过程中有个w的值，这是权重，所以模型一般分两个部分，有的深度学习框架两个是在一个过程中进行的，有的深度学习框架是在两个不同模型中完成的，但不管怎么样，rerain的model值都是非常好的。推理的过程就是把你训练好的参数再一次正向计算过程中用到它，正向计算就是解决实际问题的一个例子，所以你要写inference的code，整个过程中会给你一个training的过程，一般会附带inference code。Train的过程就是六万个数据迭代的结果，真正的inference就是一个数，一个image，预测他到底是什么。整个神经网络就是数据的收集，模型的搭建，训练和推理。我们今天学的是里面最核心，最基础的一个东西。



## 8

像我们线性的例子，误差一次性传递回来，经过一步就修改了w和b的值。所以不管中间计算多么复杂，都是线性的。大部分复杂的问题都是非线性的，所以在神经网络中基本都是非线性的。需要两个激活函数，先给大家来个非线性的例子，5个人分别代表x,a,b,c,z和y，X就是做了一个输入，a就是做了第一次的计算，b做了第二次的计算，c做了第三次得出的结果。Y是标签值，这里a是x^2, b是lna, c是sqrt(b),这里没一个结果都有了，我们要找五个人，每一个人分摊一个人的任务，做正向计算，得出一个值，还要做反向传播。第一个人是x,给我一个数，有一个范围，给了2，y给了2.14.现在我们开始第一步，非线性的正向计算，如果x是2，那第二步的结果就是4，第三步正向计算的结果是ln4，就是1.386. 下一步就是开根号，1.17. 下面开始算反向，从第五个开始算，算delta y, c-y就是-0.963，下一步就是c对b的求导，我们要求的是delta b,算出来是-2.267，然后再反向传播，delta a 是-9.068，然后delta x是-2.267，x的更新值是4.267.在更新，直到c的值和y的值差不多，有一个误差范围，达到一个误差范围就算结束了。给大家展示的就是非线性的例子，x就是input，我们要改的就是权重。 这里求的是损失，这里的损失函数是最简单的，c-y，然后第二个就是要怎么反向传播，就是求损失函数对他的导数，以后我们就有一套算损失函数，梯度下降的公式，就是反向传播的四大公式，都是和损失函数有关。

 

非线性反向传播的实例，现在要给出来两个方式，要怎么样算，才能算出x到底要给多少，其实有两种方法，最简单的是数学解析解，这是一个最精确的解，但在我们实际的过程中我们可能求不到那么精确的解，我们不知道每个环节最精确的值是什么，所以我们需要梯度下降的迭代解，这个迭代解就是我们刚才列表里面算的，就是要求c对b的解，b对a的解，a对x的导数，然后我们就能算出delta的值然后更新他的值。 大家去理解一下code是怎么写出来的，主要的核心是第一部分是计算，里面都有正向计算，第二个就是反向传播，反向传播就是求一个loss，然后每一个都要求一个delta的值，这里只是把结果放进去了，求出来之后就是算一个反向传播。反向传播完了就要更新权重。

 



## 9

整个过程就是正向计算，反向传播，更新值，这三个。这个main函数，首先初始化传输数据，传了一个x的值进来，然后传了一个c的值进来，就是传一个标签的输入值和输出值，然后算出来error，就有了这样后迭代20次，他默认迭代20次就能达到这个值，在真正的计算中我们不会用这样的循环来做，真正的计算中我们要求error，while true,直到error小于他的时候，才break，第一步就是正向计算，计算之后我们把x和z的值放在了数组里面，这个过程是为了可视化用的，最后loss小就break，不然就更新x的值，然后再走一遍，最后draw function，就是我每次更新完x,y我都更新一个图，我们需要用matplotlib.plt来做的, 第一个函数是numpy的函数，这个函数就是求一个范围，从1.2到10之间画一个x,第二个就是随机生成一堆x的值，然后每一个x都求一个abc的值，然后画出来x和c的关系。就是一个输入一个输出的关系，用×来表示每一个横坐标和纵坐标之间的关系。第二个就是求它的导数，最后是能够求出来的，c对x的求导。最后就是画了两个曲线图，我们运行一下就知道了。他是1.2到10之间随机取的一些值，一些点，把他画出来，然后底下这个就是他导数的值，斜率值的变化，迭代到最后的时候，已经达到精确值了。

 

下一个讲梯度下降，我们从不同的角度来理解，第一个是自然角度来理解，水从不同的方向流下来，有的方向比较平滑，有的比较陡，陡的需要弹跳的次数比较少，平滑的可能迭代了好多次才流下来，他走的方向，路径长短都不一样，最后都能达到最低点，真实中我们会遇到很多问题，在真正的梯度下降中我们保证损失函数是一个凸函数，找全局最低点，让我们在局部最低点能够避免一些特别高的点，现在我们不考虑了。梯度的数学公式是这个，大家一定要记下，我们要更新theta值，theta就是我们的权重值，权重值是怎样更新的呢，theta的n+1就是新的权重，theta n 是当前值，减号就是指梯度的反向，梯度下降就是往下走的最快，theta就是一个学习率，保证一次往下走多少，J就是loss function, 梯度下降有三点要素，第一个就是当前点n，第二个就是方向-，第三个就是步长，我们来逐个了解一下，首先我们来根据一个简单的函数，真实的cost function不会跟底下的距离这么大，我们看这个函数怎么去求梯度下降，首先x在cost function上有一个初始值，这里的x0我随机给他一个值，这时候就能求到x的导数，就是偏y比偏x,就是2(x-2), 就是-1.2，这就是我们的梯度下降，它是对cost function的下降，我要给他一个步长，学习率来降低一些速度，这时候就是0.1*-1.2，当经过一次梯度下降之后，就到了x等于1.52这个点，这里的learning rate要根据你不同的需要去调整，根据经验，不管是在右侧还是在左侧，更新过了就趋向于最低点，都是往最低点去移动的，所以都是在往下走，最后只要设置合适的步长，如果是凸函数有全局最低点，就一定能走到这个最低点。这就是我们梯度下降的过程，如果不是凸函数的话就会出现一些情况，就到了一个极值点。

 

第二个就是单变量函数的梯度下降，我们现在所有给的function都是损失函数，目的就是找到这个函数的最小值，最小值就是2x。下面这个图就展示了迭代过程，y=x^2的图形，经过了四次迭代基本达到了最低点。然后双变量的梯度下降，这是一个x和y相关的损失函数，这里面x和y都要梯度下降，求出两个偏导数之后分别代进去。双变量可以用在一个碗型的图上，这个碗图有三个点，一个x,一个y和一个cost, 中间有条黑线，表示迭代过程是从哪个方向下降的，这是一个凸函数，就能看到一个全局最低点，初始化的位置是极为重要的，一定要对输入数据进行预处理，然后就是对权重进行处理。

 

梯度下降的概念理解，学习率的选择，我把损失函数画在这儿，选了100个点，学习率太大就会导致发散。调learning rate让它达到最好的输出。



## 10

一般都是0.01，0.1, 这里告诉我们怎么选学习率，先create 一个sample，选了100个点，在-1到3上，大家回去一定要把代码看出来。 然后就是损失函数，然后就是推反向传播的四大公式。一般情况下在一个变量，一个偏差误差error，在多个样本中，就用cost lost损失代价，所以我们在本教程中，损失就是误差的集合，所有的样本合在一起就会叫损失，常用的损失函数有好多个，第一个就是均方差损失函数，m是指样本数，第二个就是交叉熵损失函数，选他就是别简单因为求导特别简单，损失函数的作用，为了反向计算的时候我们知道改多少。

首先介绍均方差损失函数，深度学习主要解决两个问题，第一个是拟合，均方差就是线性拟合，a就是预测值，y是真实值，预测值和真实值之间求一个平方差，然后再把它所有求和，这里的2就是为了求导的时候方便，我们最容易想到的是预测值减真实值，第一个问题是误差值有正负，误差值抵消了。我们看不同的损失函数得出的结果是不一样的，第一种是绝对值的方式，第二种是均方差的方式，我们比较两个样本，标签值是一样的，预测值不一样，反向传播中我们希望放大这个误差，用均方差能放大误差值，所以我们对偏离比较大的函数会进行大幅度的调整，我现在想用一条直线来你和这些点，能够使这些点都落在这条直线上，我们先找一个3x,，发现loss比较高，说明我们要减小loss, 往上移来减小loss，所以我们先找一个线性的y=ax+b, 然后我们求a的值，和b的值是什么，最后就得出来一个结果，我们怎么确定这条线是最好的呢，有很多标准来评判给定的函数的好坏。 A减y的值有正有负，这样就能保证正数就在下面，要往上移。假如我们拟合的函数是y=3x+1, 当我改b的时候我们看一看loss的变化。 这个碗图就是3d示意图，w,b和loss，2d示意图是一个等高线的图，一个是w的值，一个是b的值，在w和b取不同值的时候，那他可以达到不同的loss，同样颜色代表loss是一样的，我给大家看一下代码。

 

首先target function是y=3x+1，是默认的，这一串数据是怎么生成的，用create sample data 就能做到。算一个target function 加一个noise，noise是随机取的，用numpy这个库去算公式是非常方便的，然后我还要帮x,y打印出来，深度学习里一定要数据很多，数据越多，得出来的模型就会越好些，一个近似拟合，最后算出来可能不是x，可能是x^2,要是x1+x2_x3就是曲线了，我们来看一下cost for b这个函数做了什么，画了四张图，分成2x2,每一个核心就是算cost function. 得出来一个lost，放在result里面，cost function就做了（c-y）^2, 最后show了result，底下是四种方式在一张图下变化。最后就是画一个等高的2d的图，这个是最难的，



## 11

一圈颜色一样的就是loss都是一样的，首先我让loss function是0，如果不等于0的话，如果第一次算的时候是true的时候，就让他等于loss i,j的值，第二次就没有else了，这个loss我们来算第二次，如果这次的loss等与之前的loss,或者特别相近，所有loss相近的都把他tend进来了，从外往内一圈一圈画出来了，到最后我可能所有的loss都选完了，就可以退出了。为什么这个函数是椭圆不是圆？可能不止一个w和b达到这个值，w和b对函数的影响不一样，现在w和b同时增，就是正的。生成一个函数y=x+1, 这时候w和b的贡献就一样了，复现一下看看结果，过程很简单。



## 12

大家会去想一想怎么把这个画成一个正圆，loss最低点肯定是跟真实值是一样的时候是最低的，当w是增加的时候，b就一定要减。

最后一个是交叉熵，这个损失函数主要用于分类，就是两个概率分布的差异信息，然后交叉熵就从信息量来，就是概率乘以信息量就是熵，这就是一个真实分布对一个预测分布，我们取了后半部分p对q的熵，对于一个二分类的问题我们常用到，一个是1，一个是0. 二分类就是这么一个公式，前面一部分是真实值，后面log里面是预测值，a是我们假设函数，当y是1的时候，这一部分就是0，那loss就是负的LogA, 当标签值是0的时候，y是0。

当y＝0的时候预测值越接近于0损失越低，越远离0，损失越高，交叉熵函数的好处就是二分类很容易判断是否为0，幅度很大。举一个例子，假设出勤率高的同学都学会了课程，所以理想情况下，1就会学会课程，预测就是概率是0.6，第二次预测是0.7，第二次更好，这里算loss的时候代入公式，只要知道这个公式就能求解，这时候1是真实值。越准的loss越低，多分类函数，对每一个都要做y*logai,。我们用交叉熵损失函数来看一下，第一个就是每次都要把它乘过去，真实值应该是0 0 1，下一个问题是为什么不用均方差而是交叉熵损失函数，为什么回归问题用均方差，分类问题用交叉熵，回归问题是要保证是凸函数，有最低点。如果是分类问题，就是一堆值不断跳跃与0，1，导致无法求到最优解。分类函数也能作为激活函数。现在看一下代码。

 



## 13

反向传播四大公式，希望能够推一下，很多时候直接去用了，在这个过程中有两种方式，一种是直观的，给了一个均方差，每一个都有一个激活函数，底下的z1,z2都是线性的，跟前面的A有关的，公式都给出来了。 假设我们在矩阵推导要做几件事情，第一件就是delta是什么，就是想要去改变的值，c是损失函数，sigma是激活函数，L是指最后一层，l是指某一层。 假设cost function是c，c跟a有关，线性就是c=（a-y）^2, a跟z有关，z跟b,w有关，我现在要求最后一层，delta L的值就是delta c/delta z, 反向传播就是每一层都要求个delta，假设是某一层，delta l 就得是某一层的delta c / 某一层的delta z,。

这一段要用到矩阵求导，最后我们求c对b的求导和c对w的求导。 下面将第二部分，线性回归，作为神经网络起点的问题，是一个完成函数拟合的过程，首先第一个单入单出，我们用一个实例来说明这个问题，假设在新建好的机房里，计划部署346台服务器，需要空调来保证降温，要多大功率的空调才能保证，这里总是会有误差，我们也可以通过神经网络的方式来确定一下，神经网络的好处就是我可能不需要知道数学解析的原理，但是我可以通过大规模的数据就能得到一个比较好的预测值，这些样本画出来分布在，x就是服务器数量，y就是功率，一眼能看出来是个拟合问题，很容易看出来有一条线，满足这条线就能近似最优解，一元线性回归模型就是只有一个特征值。如果自变量的个数大于1就是多元回归，如果因变量的个数大于1就是多重回归，最小二乘法是一个数学解析解，梯度下降法是用神经网络去描述一个近似的最优解。简单的神经网络和通用的神经网络告诉我们到底在实际情况下，我们到底怎么去设计，就是y=wt x +b, 我们一般选这种，定义x不一样，三个值要怎么分配。



## 14

一元回归就是只有一个输出，对所有的样本乘以相应的权重，不同深度学习的框架读出来的数据是不一样的，这时候我们对数据怎么读取，一行表示一个样本的n个特征值，n行表示n个样本。接下来看一下最小二乘法，就跟我们均方差的损失函数是一样的，它是通过最小化误差的平方来寻找一个数据的最佳匹配，在我们里面做一个数学的解析解。如果我希望他的误差最小，也就是说我们找一条线的距离之和最小，说明这线是最容易拟合的点，如果想让他最小，就要求导，导数为0就是最低点，对b求导是一样的，如果我通过公式7，我可以得到所以b的样本是m*b,右边所有b的和是m*v, 把yi代进去就是求平均值。底下y的平均，和x的平均代入这个公式，这样我就能分成两个部分，一个有w参数，一个没有w参数，然后我把w提取出来，然后我再把y的平均和x的平均替换回去就变成，两边都把m除了，就得到这个。在这个过程中大家能够看到不是这么复杂的一个公式。

 

我们来看代码，第一步就是读取真实数据，这里定义了一些常用的公式，我们可以直接调用这个公式，simpledataRead 定义了从哪个文件去读，x参数特征值是什么，y的标签值是什么，所有的data都是以np.array的形式传出来的，load出来data由两部分，第一部分是data，第二部是label,data是x, label是y. shape0是他有多少行，我们是以行来作为他的个数，列来作为他的特征数，我现在要算w1和b1，读出来这个类我们就能get到所有的值。M就是样本的个数，x和y就是它的特征和标签，然后我现在就要算w1和b1，怎么样去得到w1和b1，随便用哪种方法。矩阵的运算可以直接运算，把矩阵带过去就能直接做运算。



## 15,16

这个过程让大家联系矩阵怎么去计算。给大家一点提示，什么叫一步达成，偏C/偏bl,就是偏c/偏Zl*偏Zl/偏b。我们留的作业第一个就是计算在chapter2里面single, double variable里面new和pre-new的区别是什么，第二个就是把最后这个求一下，明天我们来解答这个问题，明天再把第四章剩下的部分，讲一个单分类。
=======

## 2

今天我们来学习神经网络的基本原理，这是理论课。首先为什么要学这门课？这段教材系统的从入门到进阶，系统的把神经网络的基本给大家介绍一下。他把深度学习归纳成9个步骤，基本概念是最基础的也是最耗时的，把基础学好了，后面就比较快了。

我们来看基本概念，现将基本原理，再讲是怎么工作的，然后再介绍需要前导的东西，导数的推导和反向传播的一些公式来主导一些概念。这是神经网络的基本工作原理，这是一个神经元模型，它有多个输入，每一个输入给的权重不一样，第一个就是输入，外界的信号，对于人工智能来说就是多个数据的样本属性，有几个特征值就有几个输入，那么如果有多个样本怎么办呢？就是一个矩阵，因为每一个权重不一样，w就代表特征值，再有就是偏移，是因为在脑电波中，要保证输入信号大于某一个临界值，才能激活细胞处于兴奋状态。经过一个线型的计算之后，把这些都算出来加在一起，大于等于t之后，就激活了，整个值就是最后结果，有时候我们输出是个线性的，但我们不想是线性的，后面我们就需要一个激活函数，激活函数在神经网络中是非常重要的一个部分，A不见得是必须的，但加上他之后会比没有他效果好很多，在有些情况下必须要有这个激活函数。这是一个非常经典的激活函数，它是非线性的。

小结我们来看一下，一个神经元可以有多个输入，但只能有一个输出，然后再有神经元的权重数量和输入数量是一致的，一个神经元只能有一个偏移一个bias，一个神经网络训练的过程，也就是不断改变w，b的值，知道他们达到一个比较好的值，激活函数不是必须有的，如果没有激活函数的话，A就等于Z。 神经网络训练的过程我们通过这个图来看一下，他是两个神经元，就有两个输出，三个输入配给了两个神经元，所以序号是不一样的，w的第一个值指的是输出，第二个指的是特征值，B就有两个。我们简单地把训练流程走一下。



## 3

均值，方差中心化       去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

## 4

 均值，方差中心化     去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

神经网络最重要的三大概念，反向传播，梯度下降，损失函数。第一个例子是猜数，初始化就是一个蒙的过程，损失函数就是乙告诉甲的过程，梯度下降的目的就是为了让损失更小。，达到最接近真实值的结果。 第二个黑盒子，就是输入不等于输出了，只能看到输入输出值，而不能看到中间做了什么事情。现在就是要通过通过一种方式去模拟，近似得到一个黑盒子。第三个就是比较接近于神经网络训练的过程，打靶。神经网络不是一次打一个靶，一般都是多发，取平均值可能就是更稳定的发挥。 样本里就有一个输入一个输出，输出就是标签值，真实值。一般情况下我们会有个假设，正负表示方向，整个模型就是训练一个权重的矩阵。我们单样本叫误差，多样本叫损失。我用一些模拟的方法改变函数的权重值，导致我输入的样本能够匹配上，但也有可能样本换了。并不是真实解析了黑盒子，我们只是模拟近似。通俗的计算，整个神经网络训练就是，初始化，第二就是正向计算，第三件事是损失函数，第四件事是梯度下降。我们接下俩介绍线性反向传播。



## 5

 先来介绍反向传播，这个例子有点tricky的地方是，我们为了简单直接把样本值直接大家了，为了最大限度简化我们的过程。在这里面我们要求解w和b的值，我们看到变量w就是我们的权重之一，变量b是权重之一，常数2和1就是我们给的样本的值，他就代到线性方程里面，在神经网络计算的整个过程中，第一步是初始化，所以就给w和b一个值，然后是正向计算，算出x,y,z的值，第三步是损失函数，162和150去比，做一个绝对值的比较，假如z150是一个标签值，反向传播的过程其实就是一个求导的过程，求完导数我们去更新w和b的值，再重新进行计算。 首先是正向计算的结果，我们算出来了，代入就可以了，然后第二个就是反向传播，这个过程要仔细看。加入我们让Z变小，变成150，那w,b要怎么变才能变到150，我们从输出层一层一层往回看，改变它的值，现在是有个w，有个d。我们先看单变量的，单变量我们现在只关心一个b的值。Z的变化是162，b怎么变化，整个过程就是偏Z比偏b,,Z是对X和Y产生影响，X和Y是跟b有关，这里牵涉到连式法则 ，偏Z比偏X是y, 偏x比偏b是3.y算出来是9，x算出来是18，最后算出来的偏导数是3.，偏Z比偏b的过程。一步一步往前走，最后能算出来Z和b之间的关系。反向传播去计算，deltaZ是12，deltab是12/63 等于0.19, 损失函数是预测值减真实值，反向传播给出来两个概念，一个是大小，一个是梯度。梯度下降就是让他方向一直向下走，当b是3.81的时候，这时候Z算出来是150.246，跟150差距就很近了，但我最后结果想达到e^-4,这时候发现这个值还是有点大，所以还要继续算一遍，请自己动手迭代一次。 

 

提示大家，要算偏Z比偏b,核心内容就是求导。这个例子一定要算对，后面都要用这个例子。第一次偏Z比偏b是63，b的值是12/63.



## 6

大家取的位数不一样，结果就不一样，结果近似差不多就好。我在算Z的时候就等于再算了一遍x和y。所以当b改变时，x和y也改了，所以这三个值都出来了。 所以更新b的权重的时候，这里唯一有用的就是这里的算偏导。我们再看双变量，我们这是个近似模拟，双变量的时候怎么求反向传播？偏Z比偏W就是2y。课堂作业就是用python写一下反向传播的代码。第一个就是线性反向传播，这里首先是target
function，给一个w的值，给一个b的值，然后求出x,y,z是什么。第二个就是single的，课下作业之一就是这里分别有个带new和不带new的，请大家思考一下，这个不带new的方式和带new的方式，她差别在哪里？首先我们定义一个error。我的error一定要小于e的负四或者e的负五。如果没有达到这个，继续。如果达到了这个，就break。在while true时候。我们首先正向计算。给定一个w有给点一个b, 都是三和四，然后求出x和y的值。否则我要算出z和w的导数, 在这里求导，就是calculate the factor。所以刚才大家问矩阵求导这个过程就是这个意思。直接把求好的的导数给传进去，矩阵求导，你给他的值错了，那他就算不出来。算完之后x和y就有了导出就能求出来。然后根据这个导数，我们就能算出delta。双变量就除以一个二，因为双变量两个因素都对他有影响，所以我就默认每个影响都差不多，就都除以一个二。这个操作其实是我们一个很tricky的操作。如果这个factor太大了。我们深度学习中有两个重要的调参，一个就是调learning rate，这个值怎么设呢？这是凭借经验，一般情况下都是0.01或者0.1。还有很多算法，就是不断的去改变learning rate。双变量的过程就是做了一个减项计算求导。求梯度。反向传播。给他更新。然后最后再求一遍新的，直到新的结果是对了。整个函数就是做了这个，第一步就是初始化。所以现在请大家写一下现场练习。



## 7

大家一定要自己运行一遍，不然到后面可能会想听天书一样，大家一定要背着把函数写出来。有的时候梯度太大就发散了，在整个计算过程中，单一一次走遍整个神经网络的过程，我们刚刚说了初始化，正向计算，梯度下降，反向传播，再正向计算。对于整个神经网络来说，第一个就是数据，第二个就是要建立模型，这里是一个线性模型，多入单出的线性模型。过程中我们要训练模型，训练有很多种标准，有人用loss做标准，有人用accuracy做标准。当你模型参数固定了，他一般有两部分，第一部分叫计算图，就是我们刚才x算到z的过程，第二个就是每个过程中有个w的值，这是权重，所以模型一般分两个部分，有的深度学习框架两个是在一个过程中进行的，有的深度学习框架是在两个不同模型中完成的，但不管怎么样，rerain的model值都是非常好的。推理的过程就是把你训练好的参数再一次正向计算过程中用到它，正向计算就是解决实际问题的一个例子，所以你要写inference的code，整个过程中会给你一个training的过程，一般会附带inference code。Train的过程就是六万个数据迭代的结果，真正的inference就是一个数，一个image，预测他到底是什么。整个神经网络就是数据的收集，模型的搭建，训练和推理。我们今天学的是里面最核心，最基础的一个东西。



## 8

像我们线性的例子，误差一次性传递回来，经过一步就修改了w和b的值。所以不管中间计算多么复杂，都是线性的。大部分复杂的问题都是非线性的，所以在神经网络中基本都是非线性的。需要两个激活函数，先给大家来个非线性的例子，5个人分别代表x,a,b,c,z和y，X就是做了一个输入，a就是做了第一次的计算，b做了第二次的计算，c做了第三次得出的结果。Y是标签值，这里a是x^2, b是lna, c是sqrt(b),这里没一个结果都有了，我们要找五个人，每一个人分摊一个人的任务，做正向计算，得出一个值，还要做反向传播。第一个人是x,给我一个数，有一个范围，给了2，y给了2.14.现在我们开始第一步，非线性的正向计算，如果x是2，那第二步的结果就是4，第三步正向计算的结果是ln4，就是1.386. 下一步就是开根号，1.17. 下面开始算反向，从第五个开始算，算delta y, c-y就是-0.963，下一步就是c对b的求导，我们要求的是delta b,算出来是-2.267，然后再反向传播，delta a 是-9.068，然后delta x是-2.267，x的更新值是4.267.在更新，直到c的值和y的值差不多，有一个误差范围，达到一个误差范围就算结束了。给大家展示的就是非线性的例子，x就是input，我们要改的就是权重。 这里求的是损失，这里的损失函数是最简单的，c-y，然后第二个就是要怎么反向传播，就是求损失函数对他的导数，以后我们就有一套算损失函数，梯度下降的公式，就是反向传播的四大公式，都是和损失函数有关。

 

非线性反向传播的实例，现在要给出来两个方式，要怎么样算，才能算出x到底要给多少，其实有两种方法，最简单的是数学解析解，这是一个最精确的解，但在我们实际的过程中我们可能求不到那么精确的解，我们不知道每个环节最精确的值是什么，所以我们需要梯度下降的迭代解，这个迭代解就是我们刚才列表里面算的，就是要求c对b的解，b对a的解，a对x的导数，然后我们就能算出delta的值然后更新他的值。 大家去理解一下code是怎么写出来的，主要的核心是第一部分是计算，里面都有正向计算，第二个就是反向传播，反向传播就是求一个loss，然后每一个都要求一个delta的值，这里只是把结果放进去了，求出来之后就是算一个反向传播。反向传播完了就要更新权重。

 



## 9

整个过程就是正向计算，反向传播，更新值，这三个。这个main函数，首先初始化传输数据，传了一个x的值进来，然后传了一个c的值进来，就是传一个标签的输入值和输出值，然后算出来error，就有了这样后迭代20次，他默认迭代20次就能达到这个值，在真正的计算中我们不会用这样的循环来做，真正的计算中我们要求error，while true,直到error小于他的时候，才break，第一步就是正向计算，计算之后我们把x和z的值放在了数组里面，这个过程是为了可视化用的，最后loss小就break，不然就更新x的值，然后再走一遍，最后draw function，就是我每次更新完x,y我都更新一个图，我们需要用matplotlib.plt来做的, 第一个函数是numpy的函数，这个函数就是求一个范围，从1.2到10之间画一个x,第二个就是随机生成一堆x的值，然后每一个x都求一个abc的值，然后画出来x和c的关系。就是一个输入一个输出的关系，用×来表示每一个横坐标和纵坐标之间的关系。第二个就是求它的导数，最后是能够求出来的，c对x的求导。最后就是画了两个曲线图，我们运行一下就知道了。他是1.2到10之间随机取的一些值，一些点，把他画出来，然后底下这个就是他导数的值，斜率值的变化，迭代到最后的时候，已经达到精确值了。

 

下一个讲梯度下降，我们从不同的角度来理解，第一个是自然角度来理解，水从不同的方向流下来，有的方向比较平滑，有的比较陡，陡的需要弹跳的次数比较少，平滑的可能迭代了好多次才流下来，他走的方向，路径长短都不一样，最后都能达到最低点，真实中我们会遇到很多问题，在真正的梯度下降中我们保证损失函数是一个凸函数，找全局最低点，让我们在局部最低点能够避免一些特别高的点，现在我们不考虑了。梯度的数学公式是这个，大家一定要记下，我们要更新theta值，theta就是我们的权重值，权重值是怎样更新的呢，theta的n+1就是新的权重，theta n 是当前值，减号就是指梯度的反向，梯度下降就是往下走的最快，theta就是一个学习率，保证一次往下走多少，J就是loss function, 梯度下降有三点要素，第一个就是当前点n，第二个就是方向-，第三个就是步长，我们来逐个了解一下，首先我们来根据一个简单的函数，真实的cost function不会跟底下的距离这么大，我们看这个函数怎么去求梯度下降，首先x在cost function上有一个初始值，这里的x0我随机给他一个值，这时候就能求到x的导数，就是偏y比偏x,就是2(x-2), 就是-1.2，这就是我们的梯度下降，它是对cost function的下降，我要给他一个步长，学习率来降低一些速度，这时候就是0.1*-1.2，当经过一次梯度下降之后，就到了x等于1.52这个点，这里的learning rate要根据你不同的需要去调整，根据经验，不管是在右侧还是在左侧，更新过了就趋向于最低点，都是往最低点去移动的，所以都是在往下走，最后只要设置合适的步长，如果是凸函数有全局最低点，就一定能走到这个最低点。这就是我们梯度下降的过程，如果不是凸函数的话就会出现一些情况，就到了一个极值点。

 

第二个就是单变量函数的梯度下降，我们现在所有给的function都是损失函数，目的就是找到这个函数的最小值，最小值就是2x。下面这个图就展示了迭代过程，y=x^2的图形，经过了四次迭代基本达到了最低点。然后双变量的梯度下降，这是一个x和y相关的损失函数，这里面x和y都要梯度下降，求出两个偏导数之后分别代进去。双变量可以用在一个碗型的图上，这个碗图有三个点，一个x,一个y和一个cost, 中间有条黑线，表示迭代过程是从哪个方向下降的，这是一个凸函数，就能看到一个全局最低点，初始化的位置是极为重要的，一定要对输入数据进行预处理，然后就是对权重进行处理。

 

梯度下降的概念理解，学习率的选择，我把损失函数画在这儿，选了100个点，学习率太大就会导致发散。调learning rate让它达到最好的输出。



## 10

一般都是0.01，0.1, 这里告诉我们怎么选学习率，先create 一个sample，选了100个点，在-1到3上，大家回去一定要把代码看出来。 然后就是损失函数，然后就是推反向传播的四大公式。一般情况下在一个变量，一个偏差误差error，在多个样本中，就用cost lost损失代价，所以我们在本教程中，损失就是误差的集合，所有的样本合在一起就会叫损失，常用的损失函数有好多个，第一个就是均方差损失函数，m是指样本数，第二个就是交叉熵损失函数，选他就是别简单因为求导特别简单，损失函数的作用，为了反向计算的时候我们知道改多少。

首先介绍均方差损失函数，深度学习主要解决两个问题，第一个是拟合，均方差就是线性拟合，a就是预测值，y是真实值，预测值和真实值之间求一个平方差，然后再把它所有求和，这里的2就是为了求导的时候方便，我们最容易想到的是预测值减真实值，第一个问题是误差值有正负，误差值抵消了。我们看不同的损失函数得出的结果是不一样的，第一种是绝对值的方式，第二种是均方差的方式，我们比较两个样本，标签值是一样的，预测值不一样，反向传播中我们希望放大这个误差，用均方差能放大误差值，所以我们对偏离比较大的函数会进行大幅度的调整，我现在想用一条直线来你和这些点，能够使这些点都落在这条直线上，我们先找一个3x,，发现loss比较高，说明我们要减小loss, 往上移来减小loss，所以我们先找一个线性的y=ax+b, 然后我们求a的值，和b的值是什么，最后就得出来一个结果，我们怎么确定这条线是最好的呢，有很多标准来评判给定的函数的好坏。 A减y的值有正有负，这样就能保证正数就在下面，要往上移。假如我们拟合的函数是y=3x+1, 当我改b的时候我们看一看loss的变化。 这个碗图就是3d示意图，w,b和loss，2d示意图是一个等高线的图，一个是w的值，一个是b的值，在w和b取不同值的时候，那他可以达到不同的loss，同样颜色代表loss是一样的，我给大家看一下代码。

 

首先target function是y=3x+1，是默认的，这一串数据是怎么生成的，用create sample data 就能做到。算一个target function 加一个noise，noise是随机取的，用numpy这个库去算公式是非常方便的，然后我还要帮x,y打印出来，深度学习里一定要数据很多，数据越多，得出来的模型就会越好些，一个近似拟合，最后算出来可能不是x，可能是x^2,要是x1+x2_x3就是曲线了，我们来看一下cost for b这个函数做了什么，画了四张图，分成2x2,每一个核心就是算cost function. 得出来一个lost，放在result里面，cost function就做了（c-y）^2, 最后show了result，底下是四种方式在一张图下变化。最后就是画一个等高的2d的图，这个是最难的，



## 11

一圈颜色一样的就是loss都是一样的，首先我让loss function是0，如果不等于0的话，如果第一次算的时候是true的时候，就让他等于loss i,j的值，第二次就没有else了，这个loss我们来算第二次，如果这次的loss等与之前的loss,或者特别相近，所有loss相近的都把他tend进来了，从外往内一圈一圈画出来了，到最后我可能所有的loss都选完了，就可以退出了。为什么这个函数是椭圆不是圆？可能不止一个w和b达到这个值，w和b对函数的影响不一样，现在w和b同时增，就是正的。生成一个函数y=x+1, 这时候w和b的贡献就一样了，复现一下看看结果，过程很简单。



## 12

大家会去想一想怎么把这个画成一个正圆，loss最低点肯定是跟真实值是一样的时候是最低的，当w是增加的时候，b就一定要减。

最后一个是交叉熵，这个损失函数主要用于分类，就是两个概率分布的差异信息，然后交叉熵就从信息量来，就是概率乘以信息量就是熵，这就是一个真实分布对一个预测分布，我们取了后半部分p对q的熵，对于一个二分类的问题我们常用到，一个是1，一个是0. 二分类就是这么一个公式，前面一部分是真实值，后面log里面是预测值，a是我们假设函数，当y是1的时候，这一部分就是0，那loss就是负的LogA, 当标签值是0的时候，y是0。

当y＝0的时候预测值越接近于0损失越低，越远离0，损失越高，交叉熵函数的好处就是二分类很容易判断是否为0，幅度很大。举一个例子，假设出勤率高的同学都学会了课程，所以理想情况下，1就会学会课程，预测就是概率是0.6，第二次预测是0.7，第二次更好，这里算loss的时候代入公式，只要知道这个公式就能求解，这时候1是真实值。越准的loss越低，多分类函数，对每一个都要做y*logai,。我们用交叉熵损失函数来看一下，第一个就是每次都要把它乘过去，真实值应该是0 0 1，下一个问题是为什么不用均方差而是交叉熵损失函数，为什么回归问题用均方差，分类问题用交叉熵，回归问题是要保证是凸函数，有最低点。如果是分类问题，就是一堆值不断跳跃与0，1，导致无法求到最优解。分类函数也能作为激活函数。现在看一下代码。

 



## 13

反向传播四大公式，希望能够推一下，很多时候直接去用了，在这个过程中有两种方式，一种是直观的，给了一个均方差，每一个都有一个激活函数，底下的z1,z2都是线性的，跟前面的A有关的，公式都给出来了。 假设我们在矩阵推导要做几件事情，第一件就是delta是什么，就是想要去改变的值，c是损失函数，sigma是激活函数，L是指最后一层，l是指某一层。 假设cost function是c，c跟a有关，线性就是c=（a-y）^2, a跟z有关，z跟b,w有关，我现在要求最后一层，delta L的值就是delta c/delta z, 反向传播就是每一层都要求个delta，假设是某一层，delta l 就得是某一层的delta c / 某一层的delta z,。

这一段要用到矩阵求导，最后我们求c对b的求导和c对w的求导。 下面将第二部分，线性回归，作为神经网络起点的问题，是一个完成函数拟合的过程，首先第一个单入单出，我们用一个实例来说明这个问题，假设在新建好的机房里，计划部署346台服务器，需要空调来保证降温，要多大功率的空调才能保证，这里总是会有误差，我们也可以通过神经网络的方式来确定一下，神经网络的好处就是我可能不需要知道数学解析的原理，但是我可以通过大规模的数据就能得到一个比较好的预测值，这些样本画出来分布在，x就是服务器数量，y就是功率，一眼能看出来是个拟合问题，很容易看出来有一条线，满足这条线就能近似最优解，一元线性回归模型就是只有一个特征值。如果自变量的个数大于1就是多元回归，如果因变量的个数大于1就是多重回归，最小二乘法是一个数学解析解，梯度下降法是用神经网络去描述一个近似的最优解。简单的神经网络和通用的神经网络告诉我们到底在实际情况下，我们到底怎么去设计，就是y=wt x +b, 我们一般选这种，定义x不一样，三个值要怎么分配。



## 14

一元回归就是只有一个输出，对所有的样本乘以相应的权重，不同深度学习的框架读出来的数据是不一样的，这时候我们对数据怎么读取，一行表示一个样本的n个特征值，n行表示n个样本。接下来看一下最小二乘法，就跟我们均方差的损失函数是一样的，它是通过最小化误差的平方来寻找一个数据的最佳匹配，在我们里面做一个数学的解析解。如果我希望他的误差最小，也就是说我们找一条线的距离之和最小，说明这线是最容易拟合的点，如果想让他最小，就要求导，导数为0就是最低点，对b求导是一样的，如果我通过公式7，我可以得到所以b的样本是m*b,右边所有b的和是m*v, 把yi代进去就是求平均值。底下y的平均，和x的平均代入这个公式，这样我就能分成两个部分，一个有w参数，一个没有w参数，然后我把w提取出来，然后我再把y的平均和x的平均替换回去就变成，两边都把m除了，就得到这个。在这个过程中大家能够看到不是这么复杂的一个公式。

 

我们来看代码，第一步就是读取真实数据，这里定义了一些常用的公式，我们可以直接调用这个公式，simpledataRead 定义了从哪个文件去读，x参数特征值是什么，y的标签值是什么，所有的data都是以np.array的形式传出来的，load出来data由两部分，第一部分是data，第二部是label,data是x, label是y. shape0是他有多少行，我们是以行来作为他的个数，列来作为他的特征数，我现在要算w1和b1，读出来这个类我们就能get到所有的值。M就是样本的个数，x和y就是它的特征和标签，然后我现在就要算w1和b1，怎么样去得到w1和b1，随便用哪种方法。矩阵的运算可以直接运算，把矩阵带过去就能直接做运算。



## 15,16

=======
## 2

今天我们来学习神经网络的基本原理，这是理论课。首先为什么要学这门课？这段教材系统的从入门到进阶，系统的把神经网络的基本给大家介绍一下。他把深度学习归纳成9个步骤，基本概念是最基础的也是最耗时的，把基础学好了，后面就比较快了。

我们来看基本概念，现将基本原理，再讲是怎么工作的，然后再介绍需要前导的东西，导数的推导和反向传播的一些公式来主导一些概念。这是神经网络的基本工作原理，这是一个神经元模型，它有多个输入，每一个输入给的权重不一样，第一个就是输入，外界的信号，对于人工智能来说就是多个数据的样本属性，有几个特征值就有几个输入，那么如果有多个样本怎么办呢？就是一个矩阵，因为每一个权重不一样，w就代表特征值，再有就是偏移，是因为在脑电波中，要保证输入信号大于某一个临界值，才能激活细胞处于兴奋状态。经过一个线型的计算之后，把这些都算出来加在一起，大于等于t之后，就激活了，整个值就是最后结果，有时候我们输出是个线性的，但我们不想是线性的，后面我们就需要一个激活函数，激活函数在神经网络中是非常重要的一个部分，A不见得是必须的，但加上他之后会比没有他效果好很多，在有些情况下必须要有这个激活函数。这是一个非常经典的激活函数，它是非线性的。

小结我们来看一下，一个神经元可以有多个输入，但只能有一个输出，然后再有神经元的权重数量和输入数量是一致的，一个神经元只能有一个偏移一个bias，一个神经网络训练的过程，也就是不断改变w，b的值，知道他们达到一个比较好的值，激活函数不是必须有的，如果没有激活函数的话，A就等于Z。 神经网络训练的过程我们通过这个图来看一下，他是两个神经元，就有两个输出，三个输入配给了两个神经元，所以序号是不一样的，w的第一个值指的是输出，第二个指的是特征值，B就有两个。我们简单地把训练流程走一下。



## 3

均值，方差中心化       去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

## 4

 均值，方差中心化     去掉一些不好的东西，然后把训练数据进行预处理之后做成输入，然后把输入训练到神经网络中，神经网络就是w和b权重的矩阵，可能w和b表现的形式会不一样，CNN是kernel的形式 ，线性的就是Z=wx+b，虽然不一样，但他是w和b的权重矩阵。最后就输入到神经网络之中，得到一个输出，这个输出是一个预测值，我们预测和真实值就有差距，所以在这时候就要计算一个损失函数，就是预测和真实值的差距，当我们算出来差距越大，损失越大。我们要做的是尽量减少损失，使损失降到最低，这样真实值和预测值才会最接近，所以我们通过误差的反向传播来修改权重，再次进行训练。

 

我们根据一个例子来看，有三个特征值，y是他的真实值。然后我们拿一个或者一批数据代入矩阵中进行计算，然后通过激活函数传到下一层，这个过程叫做正向计算，最后我们得到了A值，就是一个预测值，第一个样本我们看到了一个真实值，预测值和真实值有点差，我们就可以用损失函数来计算他们的差是多少，差距有很多方式来表示，每一种表示差距的方式不太一样，达到的效果也不太一样，线性的会用均方差，反向传播结束了，我们有第二个样本，继续进行训练再走一个，直到走到迭代的次数不够了，或者说得到的损失太小了。

 

在第一层我们就计算Z1,Z2,Z3的值，字母上角的是表示第几层，下角的表示第几个，我们一般把他们变成一个矩阵运算，然后最后矩阵变成一个大矩阵，然后我们会有一个激活函数。然后神经网络主要功能我们来介绍一下，基本上是两个，第一大类叫回归拟合，第二大类叫分类。回归拟合就是找出线来模拟出这个样本，第二个就是我有一套样本，我想找一条线来把它分开，把它分成两种不同的类型，一般情况下都是用神经网络来得到一定结果，一般情况下都是有误差的，他只是一个概率问题，得到一个近似解，这是不一样的。激活函数的作用，如果Z1，Z2，Z3都是线性的，我们需要一个非线性变换使他不再是一个线性的值。对于现行的拟合是如图这个样子。CNN是最经典的神经网络，大家基本都要学，卷及网络最大的好处就是在分类上有突破，自从卷积神经网络出现，训练过程加速了循环神经网络做了什么事？就是卷及网络中没有有办法去表现出持续的特性，后面的输入和前面的输入有关，在这个基础上就产生循环神经网络。

 

1,.1就是基本的函数导论公式，在我们的后续课程中肯定要用的。连式法则是非常重要的，在我们后面的求导过程中肯定是要用的，反向传播一共有四大公式，我们在讲完第三章交叉相乘的时候我们会回过来推这个。反向传播是深度学习的基础。

 

神经网络最重要的三大概念，反向传播，梯度下降，损失函数。第一个例子是猜数，初始化就是一个蒙的过程，损失函数就是乙告诉甲的过程，梯度下降的目的就是为了让损失更小。，达到最接近真实值的结果。 第二个黑盒子，就是输入不等于输出了，只能看到输入输出值，而不能看到中间做了什么事情。现在就是要通过通过一种方式去模拟，近似得到一个黑盒子。第三个就是比较接近于神经网络训练的过程，打靶。神经网络不是一次打一个靶，一般都是多发，取平均值可能就是更稳定的发挥。 样本里就有一个输入一个输出，输出就是标签值，真实值。一般情况下我们会有个假设，正负表示方向，整个模型就是训练一个权重的矩阵。我们单样本叫误差，多样本叫损失。我用一些模拟的方法改变函数的权重值，导致我输入的样本能够匹配上，但也有可能样本换了。并不是真实解析了黑盒子，我们只是模拟近似。通俗的计算，整个神经网络训练就是，初始化，第二就是正向计算，第三件事是损失函数，第四件事是梯度下降。我们接下俩介绍线性反向传播。



## 5

 先来介绍反向传播，这个例子有点tricky的地方是，我们为了简单直接把样本值直接大家了，为了最大限度简化我们的过程。在这里面我们要求解w和b的值，我们看到变量w就是我们的权重之一，变量b是权重之一，常数2和1就是我们给的样本的值，他就代到线性方程里面，在神经网络计算的整个过程中，第一步是初始化，所以就给w和b一个值，然后是正向计算，算出x,y,z的值，第三步是损失函数，162和150去比，做一个绝对值的比较，假如z150是一个标签值，反向传播的过程其实就是一个求导的过程，求完导数我们去更新w和b的值，再重新进行计算。 首先是正向计算的结果，我们算出来了，代入就可以了，然后第二个就是反向传播，这个过程要仔细看。加入我们让Z变小，变成150，那w,b要怎么变才能变到150，我们从输出层一层一层往回看，改变它的值，现在是有个w，有个d。我们先看单变量的，单变量我们现在只关心一个b的值。Z的变化是162，b怎么变化，整个过程就是偏Z比偏b,,Z是对X和Y产生影响，X和Y是跟b有关，这里牵涉到连式法则 ，偏Z比偏X是y, 偏x比偏b是3.y算出来是9，x算出来是18，最后算出来的偏导数是3.，偏Z比偏b的过程。一步一步往前走，最后能算出来Z和b之间的关系。反向传播去计算，deltaZ是12，deltab是12/63 等于0.19, 损失函数是预测值减真实值，反向传播给出来两个概念，一个是大小，一个是梯度。梯度下降就是让他方向一直向下走，当b是3.81的时候，这时候Z算出来是150.246，跟150差距就很近了，但我最后结果想达到e^-4,这时候发现这个值还是有点大，所以还要继续算一遍，请自己动手迭代一次。 

 

提示大家，要算偏Z比偏b,核心内容就是求导。这个例子一定要算对，后面都要用这个例子。第一次偏Z比偏b是63，b的值是12/63.



## 6

大家取的位数不一样，结果就不一样，结果近似差不多就好。我在算Z的时候就等于再算了一遍x和y。所以当b改变时，x和y也改了，所以这三个值都出来了。 所以更新b的权重的时候，这里唯一有用的就是这里的算偏导。我们再看双变量，我们这是个近似模拟，双变量的时候怎么求反向传播？偏Z比偏W就是2y。课堂作业就是用python写一下反向传播的代码。第一个就是线性反向传播，这里首先是target
function，给一个w的值，给一个b的值，然后求出x,y,z是什么。第二个就是single的，课下作业之一就是这里分别有个带new和不带new的，请大家思考一下，这个不带new的方式和带new的方式，她差别在哪里？首先我们定义一个error。我的error一定要小于e的负四或者e的负五。如果没有达到这个，继续。如果达到了这个，就break。在while true时候。我们首先正向计算。给定一个w有给点一个b, 都是三和四，然后求出x和y的值。否则我要算出z和w的导数, 在这里求导，就是calculate the factor。所以刚才大家问矩阵求导这个过程就是这个意思。直接把求好的的导数给传进去，矩阵求导，你给他的值错了，那他就算不出来。算完之后x和y就有了导出就能求出来。然后根据这个导数，我们就能算出delta。双变量就除以一个二，因为双变量两个因素都对他有影响，所以我就默认每个影响都差不多，就都除以一个二。这个操作其实是我们一个很tricky的操作。如果这个factor太大了。我们深度学习中有两个重要的调参，一个就是调learning rate，这个值怎么设呢？这是凭借经验，一般情况下都是0.01或者0.1。还有很多算法，就是不断的去改变learning rate。双变量的过程就是做了一个减项计算求导。求梯度。反向传播。给他更新。然后最后再求一遍新的，直到新的结果是对了。整个函数就是做了这个，第一步就是初始化。所以现在请大家写一下现场练习。



## 7

大家一定要自己运行一遍，不然到后面可能会想听天书一样，大家一定要背着把函数写出来。有的时候梯度太大就发散了，在整个计算过程中，单一一次走遍整个神经网络的过程，我们刚刚说了初始化，正向计算，梯度下降，反向传播，再正向计算。对于整个神经网络来说，第一个就是数据，第二个就是要建立模型，这里是一个线性模型，多入单出的线性模型。过程中我们要训练模型，训练有很多种标准，有人用loss做标准，有人用accuracy做标准。当你模型参数固定了，他一般有两部分，第一部分叫计算图，就是我们刚才x算到z的过程，第二个就是每个过程中有个w的值，这是权重，所以模型一般分两个部分，有的深度学习框架两个是在一个过程中进行的，有的深度学习框架是在两个不同模型中完成的，但不管怎么样，rerain的model值都是非常好的。推理的过程就是把你训练好的参数再一次正向计算过程中用到它，正向计算就是解决实际问题的一个例子，所以你要写inference的code，整个过程中会给你一个training的过程，一般会附带inference code。Train的过程就是六万个数据迭代的结果，真正的inference就是一个数，一个image，预测他到底是什么。整个神经网络就是数据的收集，模型的搭建，训练和推理。我们今天学的是里面最核心，最基础的一个东西。



## 8

像我们线性的例子，误差一次性传递回来，经过一步就修改了w和b的值。所以不管中间计算多么复杂，都是线性的。大部分复杂的问题都是非线性的，所以在神经网络中基本都是非线性的。需要两个激活函数，先给大家来个非线性的例子，5个人分别代表x,a,b,c,z和y，X就是做了一个输入，a就是做了第一次的计算，b做了第二次的计算，c做了第三次得出的结果。Y是标签值，这里a是x^2, b是lna, c是sqrt(b),这里没一个结果都有了，我们要找五个人，每一个人分摊一个人的任务，做正向计算，得出一个值，还要做反向传播。第一个人是x,给我一个数，有一个范围，给了2，y给了2.14.现在我们开始第一步，非线性的正向计算，如果x是2，那第二步的结果就是4，第三步正向计算的结果是ln4，就是1.386. 下一步就是开根号，1.17. 下面开始算反向，从第五个开始算，算delta y, c-y就是-0.963，下一步就是c对b的求导，我们要求的是delta b,算出来是-2.267，然后再反向传播，delta a 是-9.068，然后delta x是-2.267，x的更新值是4.267.在更新，直到c的值和y的值差不多，有一个误差范围，达到一个误差范围就算结束了。给大家展示的就是非线性的例子，x就是input，我们要改的就是权重。 这里求的是损失，这里的损失函数是最简单的，c-y，然后第二个就是要怎么反向传播，就是求损失函数对他的导数，以后我们就有一套算损失函数，梯度下降的公式，就是反向传播的四大公式，都是和损失函数有关。

 

非线性反向传播的实例，现在要给出来两个方式，要怎么样算，才能算出x到底要给多少，其实有两种方法，最简单的是数学解析解，这是一个最精确的解，但在我们实际的过程中我们可能求不到那么精确的解，我们不知道每个环节最精确的值是什么，所以我们需要梯度下降的迭代解，这个迭代解就是我们刚才列表里面算的，就是要求c对b的解，b对a的解，a对x的导数，然后我们就能算出delta的值然后更新他的值。 大家去理解一下code是怎么写出来的，主要的核心是第一部分是计算，里面都有正向计算，第二个就是反向传播，反向传播就是求一个loss，然后每一个都要求一个delta的值，这里只是把结果放进去了，求出来之后就是算一个反向传播。反向传播完了就要更新权重。

 



## 9

整个过程就是正向计算，反向传播，更新值，这三个。这个main函数，首先初始化传输数据，传了一个x的值进来，然后传了一个c的值进来，就是传一个标签的输入值和输出值，然后算出来error，就有了这样后迭代20次，他默认迭代20次就能达到这个值，在真正的计算中我们不会用这样的循环来做，真正的计算中我们要求error，while true,直到error小于他的时候，才break，第一步就是正向计算，计算之后我们把x和z的值放在了数组里面，这个过程是为了可视化用的，最后loss小就break，不然就更新x的值，然后再走一遍，最后draw function，就是我每次更新完x,y我都更新一个图，我们需要用matplotlib.plt来做的, 第一个函数是numpy的函数，这个函数就是求一个范围，从1.2到10之间画一个x,第二个就是随机生成一堆x的值，然后每一个x都求一个abc的值，然后画出来x和c的关系。就是一个输入一个输出的关系，用×来表示每一个横坐标和纵坐标之间的关系。第二个就是求它的导数，最后是能够求出来的，c对x的求导。最后就是画了两个曲线图，我们运行一下就知道了。他是1.2到10之间随机取的一些值，一些点，把他画出来，然后底下这个就是他导数的值，斜率值的变化，迭代到最后的时候，已经达到精确值了。

 

下一个讲梯度下降，我们从不同的角度来理解，第一个是自然角度来理解，水从不同的方向流下来，有的方向比较平滑，有的比较陡，陡的需要弹跳的次数比较少，平滑的可能迭代了好多次才流下来，他走的方向，路径长短都不一样，最后都能达到最低点，真实中我们会遇到很多问题，在真正的梯度下降中我们保证损失函数是一个凸函数，找全局最低点，让我们在局部最低点能够避免一些特别高的点，现在我们不考虑了。梯度的数学公式是这个，大家一定要记下，我们要更新theta值，theta就是我们的权重值，权重值是怎样更新的呢，theta的n+1就是新的权重，theta n 是当前值，减号就是指梯度的反向，梯度下降就是往下走的最快，theta就是一个学习率，保证一次往下走多少，J就是loss function, 梯度下降有三点要素，第一个就是当前点n，第二个就是方向-，第三个就是步长，我们来逐个了解一下，首先我们来根据一个简单的函数，真实的cost function不会跟底下的距离这么大，我们看这个函数怎么去求梯度下降，首先x在cost function上有一个初始值，这里的x0我随机给他一个值，这时候就能求到x的导数，就是偏y比偏x,就是2(x-2), 就是-1.2，这就是我们的梯度下降，它是对cost function的下降，我要给他一个步长，学习率来降低一些速度，这时候就是0.1*-1.2，当经过一次梯度下降之后，就到了x等于1.52这个点，这里的learning rate要根据你不同的需要去调整，根据经验，不管是在右侧还是在左侧，更新过了就趋向于最低点，都是往最低点去移动的，所以都是在往下走，最后只要设置合适的步长，如果是凸函数有全局最低点，就一定能走到这个最低点。这就是我们梯度下降的过程，如果不是凸函数的话就会出现一些情况，就到了一个极值点。

 

第二个就是单变量函数的梯度下降，我们现在所有给的function都是损失函数，目的就是找到这个函数的最小值，最小值就是2x。下面这个图就展示了迭代过程，y=x^2的图形，经过了四次迭代基本达到了最低点。然后双变量的梯度下降，这是一个x和y相关的损失函数，这里面x和y都要梯度下降，求出两个偏导数之后分别代进去。双变量可以用在一个碗型的图上，这个碗图有三个点，一个x,一个y和一个cost, 中间有条黑线，表示迭代过程是从哪个方向下降的，这是一个凸函数，就能看到一个全局最低点，初始化的位置是极为重要的，一定要对输入数据进行预处理，然后就是对权重进行处理。

 

梯度下降的概念理解，学习率的选择，我把损失函数画在这儿，选了100个点，学习率太大就会导致发散。调learning rate让它达到最好的输出。



## 10

一般都是0.01，0.1, 这里告诉我们怎么选学习率，先create 一个sample，选了100个点，在-1到3上，大家回去一定要把代码看出来。 然后就是损失函数，然后就是推反向传播的四大公式。一般情况下在一个变量，一个偏差误差error，在多个样本中，就用cost lost损失代价，所以我们在本教程中，损失就是误差的集合，所有的样本合在一起就会叫损失，常用的损失函数有好多个，第一个就是均方差损失函数，m是指样本数，第二个就是交叉熵损失函数，选他就是别简单因为求导特别简单，损失函数的作用，为了反向计算的时候我们知道改多少。

首先介绍均方差损失函数，深度学习主要解决两个问题，第一个是拟合，均方差就是线性拟合，a就是预测值，y是真实值，预测值和真实值之间求一个平方差，然后再把它所有求和，这里的2就是为了求导的时候方便，我们最容易想到的是预测值减真实值，第一个问题是误差值有正负，误差值抵消了。我们看不同的损失函数得出的结果是不一样的，第一种是绝对值的方式，第二种是均方差的方式，我们比较两个样本，标签值是一样的，预测值不一样，反向传播中我们希望放大这个误差，用均方差能放大误差值，所以我们对偏离比较大的函数会进行大幅度的调整，我现在想用一条直线来你和这些点，能够使这些点都落在这条直线上，我们先找一个3x,，发现loss比较高，说明我们要减小loss, 往上移来减小loss，所以我们先找一个线性的y=ax+b, 然后我们求a的值，和b的值是什么，最后就得出来一个结果，我们怎么确定这条线是最好的呢，有很多标准来评判给定的函数的好坏。 A减y的值有正有负，这样就能保证正数就在下面，要往上移。假如我们拟合的函数是y=3x+1, 当我改b的时候我们看一看loss的变化。 这个碗图就是3d示意图，w,b和loss，2d示意图是一个等高线的图，一个是w的值，一个是b的值，在w和b取不同值的时候，那他可以达到不同的loss，同样颜色代表loss是一样的，我给大家看一下代码。

 

首先target function是y=3x+1，是默认的，这一串数据是怎么生成的，用create sample data 就能做到。算一个target function 加一个noise，noise是随机取的，用numpy这个库去算公式是非常方便的，然后我还要帮x,y打印出来，深度学习里一定要数据很多，数据越多，得出来的模型就会越好些，一个近似拟合，最后算出来可能不是x，可能是x^2,要是x1+x2_x3就是曲线了，我们来看一下cost for b这个函数做了什么，画了四张图，分成2x2,每一个核心就是算cost function. 得出来一个lost，放在result里面，cost function就做了（c-y）^2, 最后show了result，底下是四种方式在一张图下变化。最后就是画一个等高的2d的图，这个是最难的，



## 11

一圈颜色一样的就是loss都是一样的，首先我让loss function是0，如果不等于0的话，如果第一次算的时候是true的时候，就让他等于loss i,j的值，第二次就没有else了，这个loss我们来算第二次，如果这次的loss等与之前的loss,或者特别相近，所有loss相近的都把他tend进来了，从外往内一圈一圈画出来了，到最后我可能所有的loss都选完了，就可以退出了。为什么这个函数是椭圆不是圆？可能不止一个w和b达到这个值，w和b对函数的影响不一样，现在w和b同时增，就是正的。生成一个函数y=x+1, 这时候w和b的贡献就一样了，复现一下看看结果，过程很简单。



## 12

大家会去想一想怎么把这个画成一个正圆，loss最低点肯定是跟真实值是一样的时候是最低的，当w是增加的时候，b就一定要减。

最后一个是交叉熵，这个损失函数主要用于分类，就是两个概率分布的差异信息，然后交叉熵就从信息量来，就是概率乘以信息量就是熵，这就是一个真实分布对一个预测分布，我们取了后半部分p对q的熵，对于一个二分类的问题我们常用到，一个是1，一个是0. 二分类就是这么一个公式，前面一部分是真实值，后面log里面是预测值，a是我们假设函数，当y是1的时候，这一部分就是0，那loss就是负的LogA, 当标签值是0的时候，y是0。

当y＝0的时候预测值越接近于0损失越低，越远离0，损失越高，交叉熵函数的好处就是二分类很容易判断是否为0，幅度很大。举一个例子，假设出勤率高的同学都学会了课程，所以理想情况下，1就会学会课程，预测就是概率是0.6，第二次预测是0.7，第二次更好，这里算loss的时候代入公式，只要知道这个公式就能求解，这时候1是真实值。越准的loss越低，多分类函数，对每一个都要做y*logai,。我们用交叉熵损失函数来看一下，第一个就是每次都要把它乘过去，真实值应该是0 0 1，下一个问题是为什么不用均方差而是交叉熵损失函数，为什么回归问题用均方差，分类问题用交叉熵，回归问题是要保证是凸函数，有最低点。如果是分类问题，就是一堆值不断跳跃与0，1，导致无法求到最优解。分类函数也能作为激活函数。现在看一下代码。

 



## 13

反向传播四大公式，希望能够推一下，很多时候直接去用了，在这个过程中有两种方式，一种是直观的，给了一个均方差，每一个都有一个激活函数，底下的z1,z2都是线性的，跟前面的A有关的，公式都给出来了。 假设我们在矩阵推导要做几件事情，第一件就是delta是什么，就是想要去改变的值，c是损失函数，sigma是激活函数，L是指最后一层，l是指某一层。 假设cost function是c，c跟a有关，线性就是c=（a-y）^2, a跟z有关，z跟b,w有关，我现在要求最后一层，delta L的值就是delta c/delta z, 反向传播就是每一层都要求个delta，假设是某一层，delta l 就得是某一层的delta c / 某一层的delta z,。

这一段要用到矩阵求导，最后我们求c对b的求导和c对w的求导。 下面将第二部分，线性回归，作为神经网络起点的问题，是一个完成函数拟合的过程，首先第一个单入单出，我们用一个实例来说明这个问题，假设在新建好的机房里，计划部署346台服务器，需要空调来保证降温，要多大功率的空调才能保证，这里总是会有误差，我们也可以通过神经网络的方式来确定一下，神经网络的好处就是我可能不需要知道数学解析的原理，但是我可以通过大规模的数据就能得到一个比较好的预测值，这些样本画出来分布在，x就是服务器数量，y就是功率，一眼能看出来是个拟合问题，很容易看出来有一条线，满足这条线就能近似最优解，一元线性回归模型就是只有一个特征值。如果自变量的个数大于1就是多元回归，如果因变量的个数大于1就是多重回归，最小二乘法是一个数学解析解，梯度下降法是用神经网络去描述一个近似的最优解。简单的神经网络和通用的神经网络告诉我们到底在实际情况下，我们到底怎么去设计，就是y=wt x +b, 我们一般选这种，定义x不一样，三个值要怎么分配。



## 14

一元回归就是只有一个输出，对所有的样本乘以相应的权重，不同深度学习的框架读出来的数据是不一样的，这时候我们对数据怎么读取，一行表示一个样本的n个特征值，n行表示n个样本。接下来看一下最小二乘法，就跟我们均方差的损失函数是一样的，它是通过最小化误差的平方来寻找一个数据的最佳匹配，在我们里面做一个数学的解析解。如果我希望他的误差最小，也就是说我们找一条线的距离之和最小，说明这线是最容易拟合的点，如果想让他最小，就要求导，导数为0就是最低点，对b求导是一样的，如果我通过公式7，我可以得到所以b的样本是m*b,右边所有b的和是m*v, 把yi代进去就是求平均值。底下y的平均，和x的平均代入这个公式，这样我就能分成两个部分，一个有w参数，一个没有w参数，然后我把w提取出来，然后我再把y的平均和x的平均替换回去就变成，两边都把m除了，就得到这个。在这个过程中大家能够看到不是这么复杂的一个公式。

 

我们来看代码，第一步就是读取真实数据，这里定义了一些常用的公式，我们可以直接调用这个公式，simpledataRead 定义了从哪个文件去读，x参数特征值是什么，y的标签值是什么，所有的data都是以np.array的形式传出来的，load出来data由两部分，第一部分是data，第二部是label,data是x, label是y. shape0是他有多少行，我们是以行来作为他的个数，列来作为他的特征数，我现在要算w1和b1，读出来这个类我们就能get到所有的值。M就是样本的个数，x和y就是它的特征和标签，然后我现在就要算w1和b1，怎么样去得到w1和b1，随便用哪种方法。矩阵的运算可以直接运算，把矩阵带过去就能直接做运算。



## 15,16

这个过程让大家联系矩阵怎么去计算。给大家一点提示，什么叫一步达成，偏C/偏bl,就是偏c/偏Zl*偏Zl/偏b。我们留的作业第一个就是计算在chapter2里面single, double variable里面new和pre-new的区别是什么，第二个就是把最后这个求一下，明天我们来解答这个问题，明天再把第四章剩下的部分，讲一个单分类。
>>>>>>> 330684af1f7cb046001108d8cde534a87f840779
