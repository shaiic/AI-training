Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
## 6.2 线性二分类的神经网络实现

我们先看看如何用神经网络在两组不同标签的样本之间画一条明显的分界线。这条分界线可以是直线，也可以是曲线。这就是二分类问题。如果只画一条分界线的话，无论是直线还是曲线，我们可以用一支假想的笔（即一个神经元），就可以达到目的，也就是说笔的走向，完全依赖于这一个神经元根据输入信号的判断。

再看楚汉城池示意图，在两个颜色区域之间似乎存在一条分割的直线，即线性可分的。

1. 从视觉上判断是线性可分的，所以我们使用单层神经网络即可；
2. 输入特征是经度和纬度，所以我们在输入层设置两个输入X1=经度，X2=维度；
3. 最后输出的是一个二分类，分别是楚汉地盘，可以看成非0即1的二分类问题，所以我们只用一个输出单元就可以了。

### 6.2.1 定义神经网络结构

根据前面的猜测，看来我们只需要一个二入一出的神经元就可以搞定。这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。

<img src="..\Images\6\BinaryClassifierNN.png">

与上一章的网络结构图的区别是，这次我们在神经元输出时使用了分类函数，所以有个A的输出，而不是以往的Z的直接输出。

#### 输入层

输入经度(x1)和纬度(x2)两个特征：

$$
X=\begin{pmatrix}
x_{1} & x_{2}
\end{pmatrix}
$$

#### 权重矩阵

输入是2个特征，输出一个数，则W的尺寸就是2x1：

$$
W=\begin{pmatrix}
w_{1} \\ w_{2}
\end{pmatrix}
$$

B的尺寸是1x1，行数永远是1，列数永远和W一样。

$$
B=\begin{pmatrix}
b_{1}
\end{pmatrix}
$$

#### 输出层

$$z_i = x_{i1} \cdot w_1 + x_{i2} \cdot w_2 + b \tag{1}$$
$$a_i = Logistic(z_i) \tag{2}$$
写成矩阵形式：
$$Z=X \cdot W+B$$
$$A = Logistic(Z)$$

#### 损失函数

二分类交叉熵函损失数：

$$
loss(w,b) = -[y_ilna_i+(1-y_i)ln(1-a_i)] \tag{3}
$$

### 6.2.2 反向传播

与上一章相比，不一样的地方有两处：

1. 损失函数，从均方差变成了交叉熵
2. 增加了激活函数

我们先试图解决这两个不同处，求一下损失函数J对Z的偏导：

$$
\frac{\partial loss}{\partial z_i}=\frac{\partial loss}{\partial a_i}\frac{\partial a_i}{\partial z_i}
$$
$$
=\frac{a_i-y_i}{a_i(1-a_i)} \cdot a_i(1-a_i)=a_i-y_i \tag{4}
$$

我们惊奇地发现，使用交叉熵函数求导得到的分母，与Sigmoid激活函数求导后的结果，正好可以抵消，最后只剩下了$a_i-y_i$这一项。真的有这么巧合的事吗？实际上这是科学家们的聪明才智，寻找出了这种匹配关系，以满足以下条件：
1. 损失函数满足二分类的要求，无论是正例还是反例，都是单调的；
2. 损失函数可导，以便于开展反向传播算法；
3. 让计算过程非常简单，一个减法就可以搞定。

接下来，我们求J对W的导数：

$$
\frac{\partial loss}{\partial w_j}=\frac{\partial loss}{\partial a_i}\frac{\partial a_i}{\partial z_i}\frac{\partial z_i}{\partial w_j}
$$
$$
=(a_i-y_i) \cdot x_{ij} \tag{5}
$$

公式5中的$i$对应着样本，可以是从1到200（本例共有200个训练样本），$j$对应着特征，从1到2（本例样本特征数为2）。

再向后推导的话，会发现最后的结果与5.2中的公式6是一样的，只不过把Z换成了A而已，所以我们就不再赘述了。

### 6.2.3 代码实现

我们先第5章的HelperClass5中，把一些已经写好的类copy过来，然后稍加改动，就可以满足我们的需要了。

由于以前我们的神经网络只会做线性回归，现在多了一个做分类的技能，所以我们加一个枚举类型，可以让调用者通过指定参数来控制神经网络的功能。

```Python
class NetType(Enum):
    Fitting = 1,
    BinaryClassifier = 2,
    MultipleClassifier = 3,
```

然后在超参类里把这个新参数加在初始化函数里：

```Python
class HyperParameters(object):
    def __init__(self, eta=0.1, max_epoch=1000, batch_size=5, eps=0.1, net_type=NetType.Fitting):
        self.eta = eta
        self.max_epoch = max_epoch
        self.batch_size = batch_size
        self.eps = eps
        self.net_type = net_type
```
再增加一个Logistic分类函数：

```Python
class Logistic(object):
    def forward(self, z):
        a = 1.0 / (1.0 + np.exp(-z))
        return a
```

以前只有均方差函数，现在我们增加了交叉熵函数，所以新建一个类便于管理：

```Python
class LossFunction(object):
    def __init__(self, net_type):
        self.net_type = net_type
    # end def

    def MSE(self, A, Y, count):
        ...

    # for binary classifier
    def CE2(self, A, Y, count):
        ...
```
上面的类是通过初始化时的网络类型来决定何时调用均方差函数(MSE)，何时调用交叉熵函数(CE2)的。

下面修改一下NeuralNet类的前向计算函数，通过判断当前的网络类型，来决定是否要在线性变换后再调用sigmoid分类函数：

```Python
class NeuralNet(object):
    def __init__(self, params, input_size, output_size):
        self.params = params
        self.W = np.zeros((input_size, output_size))
        self.B = np.zeros((1, output_size))

    def __forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Sigmoid().forward(Z)
            return A
        else:
            return Z
```

最后是主过程：

```Python
if __name__ == '__main__':
    # data
    reader = SimpleDataReader()
    reader.ReadData()
    # net
    params = HyperParameters(eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.BinaryClassifier)
    input = 2
    output = 1
    net = NeuralNet(params, input, output)
    net.train(reader, checkpoint=1)
    # inference
    x_predicate = np.array([0.58,0.92,0.62,0.55,0.39,0.29]).reshape(3,2)
    a = net.inference(x_predicate)
    print("A=", a)    
```

与以往不同的是，我们设定了超参中的网络类型是BinaryClassifier。

### 6.2.4 运行结果

损失函数值记录很平稳地下降，说明网络收敛了：

<img src="../Images/6/binary_loss.png">

最后几行的打印输出：

```
epoch=95
95 19 0.2107366003875905
epoch=96
96 19 0.20989241623899846
epoch=97
97 19 0.20905641456530918
epoch=98
98 19 0.20823454980498363
epoch=99
99 19 0.20742586902509108
W= [[-7.66469954]
 [ 3.15772116]]
B= [[2.19442993]]
A= [[0.65791301]
 [0.30556477]
 [0.53019727]]
```

打印出来的W,B的值对我们来说是几个很神秘的数字，下一节再解释。A值是返回的预测结果：

1. 经纬度相对值为(0.58,0.92)时，概率为0.65，属于汉
2. 经纬度相对值为(0.62,0.55)时，概率为0.30，属于楚
3. 经纬度相对值为(0.39,0.29)时，概率为0.53，属于汉

分类的方式是，可以指定当A > 0.5时是正例，A <= 0.5时就是反例。有时候正例反例的比例不一样或者有特殊要求时，也可以用不是0.5的数来当阈值。

### 代码位置

ch06, Level1
