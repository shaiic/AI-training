Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 6.4 实现逻辑与门和或门

我们第4章用线性回归的方式实现了逻辑非门。在学习了第6章的分类后，我们可以用分类的思想来实现下列4个逻辑门：

- 与门 AND
- 与非门 NAND
- 或门 OR
- 或非门 NOR

以逻辑AND为例，下图中的4个点，分别是4个样本数据，蓝色圆点表示负例，红色三角表示正例：

<img src="..\Images\6\LogicAndGateData.png">

如果用分类思想的话，根据前面学到的知识，应该在红色点和蓝色点之间划出一条分割线来，可以正好把正例和负例完全分开。由于样本数据稀疏，所以这条分割线的角度和位置可以比较自由，比如图中的三条直线，都可以是这个问题的解。让我们一起看看神经网络能否给我们带来惊喜。

### 6.4.1 网络模型

依然使用第6.2节中的神经元模型：

<img src="..\Images\6\BinaryClassifierNN.png">

因为输入特征值只有两个，输出一个二分类，所以模型和前一节的一样。

### 6.4.2 训练样本

每个类型的逻辑门都只有4个训练样本。

- 逻辑与门的样本

|样本|x1|x2|y|
|---|---|---|---|
|1|0|0|0|
|2|0|1|0|
|3|1|0|0|
|4|1|1|1|

- 逻辑与非门的样本

|样本|x1|x2|y|
|---|---|---|---|
|1|0|0|1|
|2|0|1|1|
|3|1|0|1|
|4|1|1|0|

- 逻辑或门的样本

|样本|x1|x2|y|
|---|---|---|---|
|1|0|0|0|
|2|0|1|1|
|3|1|0|1|
|4|1|1|1|

- 逻辑或非门的样本

|样本|x1|x2|y|
|---|---|---|---|
|1|0|0|1|
|2|0|1|0|
|3|1|0|0|
|4|1|1|0|

### 6.4.4 代码实现

#### 读取数据
  
```Python
class LogicDataReader(SimpleDataReader):
    def Read_Logic_AND_Data(self):
        X = np.array([0,0,0,1,1,0,1,1]).reshape(4,2)
        Y = np.array([0,0,0,1]).reshape(4,1)
        self.XTrain = self.XRaw = X
        self.YTrain = self.YRaw = Y
        self.num_train = self.XRaw.shape[0]

    def Read_Logic_NAND_Data(self):
        ......

    def Read_Logic_OR_Data(self):
        ......

    def Read_Logic_NOR_Data(self):        
        ......
```

以逻辑AND为例，我们从SimpleDataReader派生出自己的类LogicDataReader，并加入特定的数据读取方法Read_Logic_AND_Data()，其它几个逻辑门的方法类似，在此只列出方法名称。

#### 测试函数

```Python
def Test(net, reader):
    X,Y = reader.GetWholeTrainSamples()
    A = net.inference(X)
    print(A)
    diff = np.abs(A-Y)
    result = np.where(diff < 1e-2, True, False)
    if result.sum() == 4:
        return True
    else:
        return False
```

我们知道了神经网络只能给出近似解，但是这个“近似”能到什么程度，是需要我们在训练时自己指定的。相应地，我们要有测试手段，比如当输入为(1，1)时，AND的结果是1，但是神经网络只能给出一个0.721的概率值，这是不满足精度要求的，必须让4个样本的误差都小于1e-2。

#### 训练函数

```Python
def train(reader, title):
    draw_source_data(reader, title)
    plt.show()
    # net train
    params = HyperParameters(eta=0.5, max_epoch=10000, batch_size=1, eps=2e-3, net_type=NetType.BinaryClassifier)
    num_input = 2
    num_output = 1
    net = NeuralNet(params, num_input, num_output)
    net.train(reader, checkpoint=1)
    # test
    print(Test(net, reader))
    # visualize
    draw_source_data(reader, title)
    draw_split_line(net, reader, title)
    plt.show()
```
在超参中指定了最多10000次的epoch，0.5的学习率，最大的loss停止条件2e-3。在训练结束后，要先调用测试函数，需要返回True才能算满足要求，然后用图形显示分类结果。

### 6.4.5 运行结果

逻辑AND的运行结果的打印输出如下：

```
......
epoch=4233
4233 3 0.0020012229811963577
epoch=4234
4234 3 0.0020007488632026445
epoch=4235
4235 3 0.002000274969668408
epoch=4236
4236 3 0.0019998012999365928
W= [[11.75750515]
 [11.75780362]]
B= [[-17.80473354]]
[[9.96700157e-01]
 [2.35953140e-03]
 [1.85140939e-08]
 [2.35882891e-03]]
True
```
迭代了4236次，达到精度loss < 1e-2。当输入(1,1)(1,0)(0,1)(0,0)四种组合时，输出全都满足精度要求。

把4组数据放入表格中做一个比较：

||True|NOT|
|---|---|---|
|AND|<img src="..\Images\6\LogicAndGateResult.png">|<img src="..\Images\6\LogicNandGateResult.png">|
|epoch|4236|4247|
|W1|11.757|-11.763|
|W2|11.757|-11.763|
|B|-17.804|17.812|
|OR|<img src="..\Images\6\LogicOrGateResult.png">|<img src="..\Images\6\LogicNorGateResult.png">|
|epoch|2263|2258|
|W1|11.743|-11.738|
|W2|11.743|-11.738|
|B|-5.412|5.409|

我们从数值和图形可以得到两个结论：

1. W1和W2的值基本相同而且符号相同，说明分割线一定是135°斜率
2. 精度越高，则分割线的起点和终点越接近四边的中点0.5的位置

以上两点说明神经网络还是很聪明的，它会尽可能优美而鲁棒地找出那条分割线。


### 代码位置

ch06, Level4

### 思考与练习

1. 减小max_epoch的数值，观察神经网络的训练结果。
2. 为什么达到相同的精度，逻辑OR和NOR只用2000次左右的epoch，而逻辑AND和NAND却需要4000次以上？