Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 2.3 梯度下降

### 2.3.1 从自然现象中理解梯度下降

在绝大多数文章中，都以“一个人被困在山上，需要迅速下到谷底”来举例，这个人会“寻找当前所处位置最陡峭的地方向下走”。这个例子中忽略了安全因素，这个人不可能沿着最陡峭的方向走，要考虑坡度。

在自然界中，梯度下降的最好例子，就是泉水下山的过程：

<img src="..\Images\2\gd_water.png"/>

1. 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）
2. 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）
3. 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）

### 2.3.2 梯度下降的数学理解

先抛开神经网络，损失函数，反向传播等内容，用数学概念理解一下梯度下降。

梯度下降的数学公式：

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

其中：
- $\theta_{n+1}$：下一个值
- $\theta_n$：当前值
- $-$：梯度的反向
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长
- $\nabla$：梯度，函数当前位置的最快上升点
- $J(\theta)$：函数

#### 梯度下降的三要素

1. 当前点
2. 方向
3. 步长

#### 为什么说是“梯度下降”？

“梯度下降”包含了两层含义：

1. 梯度：函数当前位置的最快上升点
2. 下降：与导数相反的方向，用数学语言描述就是那个减号

亦即与上升相反的方向运动，就是下降。

<img src="..\Images\2\gd_concept.png">


### 2.3.3 单变量函数的梯度下降

假设一个单变量函数：

$$J(x) = x ^2$$

我们的目的是找到该函数的最小值，于是计算其微分：

$$J'(x) = 2x$$

假设初始位置为：

$$x_0=1.2$$

假设学习率：

$$\eta = 0.3$$

根据公式(1)，迭代公式：

$$x_{n+1} = x_{n} - \eta \cdot \nabla J(x)= x_{n} - \eta \cdot 2x\tag{1}$$

假设终止条件为J(x)<1e-2，迭代过程是：
```
x=0.480000, y=0.230400
x=0.192000, y=0.036864
x=0.076800, y=0.005898
x=0.030720, y=0.000944
```

上面的过程如下图所示：

<img src="..\Images\2\gd_single_variable.png">

### 2.3.4 双变量的梯度下降

假设一个双变量函数：

$$J(x,y) = x^2 + \sin^2(y)$$

我们的目的是找到该函数的最小值，于是计算其微分：

$${\partial{J(x,y)} \over \partial{x}} = 2x$$
$${\partial{J(x,y)} \over \partial{y}} = 2 \sin y \cos y$$

假设初始位置为：

$$(x_0,y_0)=(3,1)$$

假设学习率：

$$\eta = 0.1$$

根据公式(1)，迭代过程是的计算公式：
$$(x_{n+1},y_{n+1}) = (x_n,y_n) - \eta \cdot \nabla J(x,y)$$
$$ = (x_n,y_n) - \eta \cdot (2x,2 \cdot \sin y \cdot \cos y) \tag{1}$$

根据公式(1)，假设终止条件为$J(x,y)<1e-2$，迭代过程：

||x|y|J(x,y)|
|---|---|---|---|
|1|3|1|9.708073|
|2|2.4|0.909070|6.382415|
|3|1.92|0.812114|4.213103|
|...|...|...|...|
|15|0.105553|0.063481|0.015166|
|16|0.084442|0.050819|0.009711|

迭代16次后，J(x,y)的值为0.009711，满足小于1e-2的条件，停止迭代。

上面的过程如下图所示，由于是双变量，所以需要用三维图来解释。请注意看那条隐隐的黑色线，表示梯度下降的过程，从红色的高地一直沿着坡度向下走，直到蓝色的洼地。

|观察角度1|观察角度2|
|---|---|
|<img src="..\Images\2\gd_double_variable.png">|<img src="..\Images\2\gd_double_variable2.png"> |


### 2.3.5 学习率η的选择

在code里，我们把学习率定义为learning_rate，或者eta，或者叫 $\alpha$。
针对上面的例子，当初始值为-0.8，学习率为1时，迭代的情况很尴尬：不断在一条水平线上跳来跳去，永远也不能下降。

<img src="..\Images\2\gd100.png">

当学习率=0.8时，会有这种左右跳跃的情况发生，这不利于神经网络的训练。

<img src="..\Images\2\gd080.png">

当学习率=0.6时，也会有跳跃，幅度偏小。

<img src="..\Images\2\gd060.png">

当学习率=0.4时，损失值会从单侧下降，4步以后基本接近了理想值。

<img src="..\Images\2\gd040.png">

当学习率=0.2时，损失值会从单侧下降，但下降速度较慢，8步左右接近极值。

<img src="..\Images\2\gd020.png">

当学习率=0.1时，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。

<img src="..\Images\2\gd010.png">

【课后作业：用Python代码实现以上过程】


代码位置：ch02, Level2,3,4
