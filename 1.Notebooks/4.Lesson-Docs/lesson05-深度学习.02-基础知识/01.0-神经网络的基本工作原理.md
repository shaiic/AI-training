Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 第1章 神经网络的基本工作原理

## 1.0 神经网络的基本工作原理简介

### 1.0.1 神经元细胞的数学模型

神经网络由基本的神经元组成，下图就是一个神经元的数学/计算模型，便于我们用程序来实现。

<img src="..\Images\1\NeuranCell.png">

**输入 input**

(x1,x2,x3) 是外界输入信号，一般是一个训练数据样本的多个属性，比如，我们要预测一套房子的价格，那么在房屋价格数据样本中，x1可能代表了面积，x2可能代表地理位置，x3可能朝向。另外一个例子是，假设(x1,x2,x3)分别代表了(红,绿,蓝)三种颜色，而此神经元用于识别输入的信号是暖色还是冷色。

**权重 weights**

(w1,w2,w3) 是每个输入信号的权重值，以上面的 (x1,x2,x3) 的例子来说，x1的权重可能是0.92，x2的权重可能是0.2，x3的权重可能是0.03。当然权重值相加之后可以不是1。

**偏移 bias**

还有个b是怎么来的？一般的书或者博客上会告诉你那是因为$y=wx+b$，b是偏移值，使得直线能够沿Y轴上下移动。这是用结果来解释原因，并非b存在的真实原因。从生物学上解释，在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个b实际就是那个临界值。亦即当：

$$w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 >= t$$

时，该神经元细胞才会兴奋。我们把t挪到等式左侧来，变成$(-t)$，然后把它写成b，变成了：

$$w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b >= 0$$

于是b诞生了！

**求和计算 sum**

$$Z = w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b = \sum_{i=1}^m(w_i \cdot x_i) + b$$

在上面的例子中m=3。我们把$w_i \cdot x_i$变成矩阵运算的话，就变成了：

$$Z = W \cdot X + b$$

**激活函数 activation**

求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定：

$$A=a{(Z)}$$

如果激活函数是一个阶跃信号的话，那受不了啊，你会觉得脑子里总是一跳一跳的，像继电器开合一样咔咔乱响，所以一般激活函数都是有一个渐变的过程，也就是说是个曲线。

<img src="..\Images\1\activation.png">

激活函数的更多描述在后续的博客中。

至此，一个神经元的工作过程就在电光火石般的一瞬间结束了。

**小结**

- **一个神经元可以有多个输入**
- **一个神经元只能有一个输出，这个输出可以同时输入给多个神经元**
- **一个神经元的w的数量和输入的数量一致**
- **一个神经元只有一个b**
- **w和b有人为的初始值，在训练过程中被不断修改**
- **激活函数不是必须有的，亦即A可以等于Z**
- **一层神经网络中的所有神经元的激活函数必须一致**

### 1.0.2 神经网络的训练过程

#### 单层神经网络模型

这是一个单层的神经网络，有m个输入 (这里m=3)，有n个输出 (这里n=2)。在单个神经元里，b是个值。但是在神经网络中，我们把b的值永远设置为1，而用b到每个神经元的权值来表示实际的偏移值，亦即(b1,b2)，这样便于矩阵运算。也有些作者把b写成x0，其实是同一个意思，只不过x0用于等于1。

- $(x1,x2,x3)$是一个样本数据的三个特征值
- $(w11,w12,w13)$是$(x1,x2,x3)$到$n1$的权重
- $(w21,w22,w23)$是$(x1,x2,x3)$到$n2$的权重
- $b1$是$n1$的偏移
- $b2$是$n2$的偏移

<img src="..\Images\1\OneLayerNN.png">

从这里大家可以意识到，同一个特征x1，对于n1、n2来说，权重是不相同的，因为n1、n2是两个神经元，它们完成不同的任务（特征识别）。我们假设x1,x2,x3分别代表红绿蓝三种颜色，而n1,n2分别用于识别暖色和冷色，那么x1到n1的权重，肯定要大于x1到n2的权重，因为x1代表红色，是暖色。

而对于n1来说，x1，x2，x3输入的权重也是不相同的，因为它要对不同特征有选择地接纳。如同上面的例子，n1对于代表红色的x1，肯定是特别重视，权重值较高；而对于代表蓝色的x3，尽量把权重值降低，才能有正确的输出。

#### 训练流程

从真正的“零”开始学习神经网络时，我没有看到过任何一个流程图来讲述训练过程，大神们写书或者博客时都忽略了这一点，这里给大家画一个简单的流程图：

<img src="..\Images\1\TrainFlow.png">

损失函数和反向传播的更多内容在后续的博客中。

#### 前提条件

 1. 首先是我们已经有了训练数据，否则连目标都没有，训练个啥？
 2. 我们已经根据数据的规模、领域，建立了神经网络的基本结构，比如有几层，每一层有几个神经元
 3. 定义好损失函数来合理地计算误差

#### 步骤

假设我们有以下训练数据样本：

|Id|x1|x2|x3|Y|
|---|---|---|---|---|
|1|0.5|1.4|2.7|3|
|2|0.4|1.3|2.5|5|
|3|0.1|1.5|2.3|9|
|4|0.5|1.7|2.9|1|

其中，x1，x2，x3是每一个样本数据的三个特征值，Y是样本的真实结果值：

1. 随机初始化权重矩阵，可以根据高斯分布或者正态分布等来初始化。这一步可以叫做“蒙”，但不是瞎蒙。
2. 拿一个或一批数据作为输入，带入权重矩阵中计算，再通过激活函数传入下一层，最终得到预测值。在本例中，我们先用Id-1的数据输入到矩阵中，得到一个A值，假设A=5
3. 拿到Id-1样本的真实值Y=3
4. 计算损失，假设用均方差函数 $Loss = (A-Y)^2=(5-3)^2=4$
5. 根据一些神奇的数学公式（反向微分），把Loss=4这个值用大喇叭喊话，告诉在前面计算的步骤中，影响A=5这个值的每一个权重矩阵，然后对这些权重矩阵中的值做一个微小的修改（当然是向着好的方向修改，这一点可以用数学家的名誉来保证）
6. 用Id-2样本作为输入再次训练（goto 2）
7. 这样不断地迭代下去，直到以下一个或几个条件满足就停止训练：损失函数值非常小；迭代了指定的次数；计算机累吐血了......

训练完成后，我们会把这个神经网络中的结构和权重矩阵的值导出来，形成一个计算图（就是矩阵运算加上激活函数）模型，然后嵌入到任何可以识别/调用这个模型的应用程序中，根据输入的值进行运算，输出预测值。

### 1.0.3 神经网络中的矩阵运算

下面这个图是一个两层的神经网络，包含隐藏层和输出层，输入层不算做一层：

<img src="..\Images\1\TwoLayerNN.png">

其中，w1(m,n)，1表示第1层，表示第一层神经网络的权重矩阵，w2(m,n)表示第二层神经网络的权重矩阵。Visio中不容易写上下标，所以形式有所变动。

$$Z^1_1 = w^1_{1,1}x_1+w^1_{1,2}x_2+b^1_1 \tag{上标1表示第1层}$$
$$Z^1_2 = w^1_{2,1}x_1+w^1_{2,2}x_2+b^1_2$$
$$Z^1_{3} = w^1_{3,1}x_1+w^1_{3,2}x_2+b^1_{3}$$

变成矩阵运算：

$$Z^1_1=\begin{pmatrix}w^1_{1,1}&w^1_{1,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_1$$

$$Z^1_2=\begin{pmatrix}w^1_{2,1}&w^1_{2,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_2$$

$$Z^1_{3}=\begin{pmatrix}w^1_{3,1}&w^1_{3,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_3$$

再变成大矩阵：

$$Z1 =
\begin{pmatrix}
w^1_{1,1}&w^1_{1,2} \\
w^1_{2,1}&w^1_{2,2}\\
w^1_{3,1}&w^1_{3,2}
\end{pmatrix}
\begin{pmatrix}x_1 \\
x_2 \end{pmatrix}
+\begin{pmatrix}b^1_1 \\
b^1_2 \\
b^1_3
\end{pmatrix}$$

最后变成矩阵符号：

$$Z1 = W1 \cdot X + B1$$

然后是激活函数运算：

$$A1=a{(Z1)} (有激活函数)|or|A = Z(无激活函数)$$

同理可得：

$$Z2 = W2 \cdot A1 + B2$$

$$A2=a{(Z2)}$$

注意：损失函数不是前向计算的一部分。

### 1.0.4 神经网络的主要功能

- **回归/拟合 Regression/fitting**
- **分类 Classification**

单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。

比如下面这张图，二维平面中有两类点，红色的和蓝色的，用一条直线肯定不能把两者分开了。

|拟合|分类|
|---|---|
|<img src="..\Images\9\sgd_result.png" width="400">|<img src="..\Images\1\Sample.png" width="400">|

我们使用一个两层的神经网络可以得到一个非常近似的结果，使得分类误差在满意的范围之内。而这个真实的连续函数的原型是：

$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$

哦，my god! 这么复杂的函数，一个两层的神经网络是如何做到的呢？其实从输入层到隐藏层的矩阵计算，就是对输入数据进行了空间变换，使其可以被线性可分，然后输出层画出了一个分界线。而训练的过程，就是确定那个空间变换矩阵的过程。因此，多层神经网络的本质就是对复杂函数的拟合。我们可以在后面的试验中来学习如何拟合上述的复杂函数的。

神经网络的训练结果，是一大堆的权重组成的数组（近似解），并不能得到上面那种精确的数学表达式（数学解析解）。

### 1.0.5 为什么需要激活函数

#### 生理学上的例子

人体骨关节是动物界里最复杂的生理结构，一共有8个重要的大关节：
- 肩关节
- 肘关节
- 腕关节
- 髋关节
- 膝关节
- 踝关节
- 颈关节
- 腰关节

<img src="..\Images\1\bone-joint.jpg" width="400">

人的臂骨，腿骨等，都是一根直线，人体直立时，也是一根直线。但是人在骨关节和肌肉组织的配合下，可以做很多复杂的动作，原因就是关节本身不是线性结构，而是一个在有限范围内可以任意活动的结构，有一定的柔韧性。

比如肘关节，可以完成小臂在一个二维平面上的活动。加上肩关节，就可以完成胳膊在三维空间的活动。再加上其它关节，就可以扩展胳膊活动的三维空间的范围。

用一个表格来比喻：

|人体运动组织|神经网络组织|
|---|---|
|支撑骨骼|网络层次|
|关节|激活函数|
|肌肉韧带|权重参数|
|学习各种运动的动作|前向反向训练过程|

**其实我们的祖先早已经发明了双截棍、三节棍...**

#### 激活函数的作用

激活函数就相当于关节。看以下的例子：

$$Z1 = W1 \cdot X + B1$$

$$Z2 = W2 \cdot Z1 + B2$$

$$Z3 = W3 \cdot Z2 + B3$$

展开：

$$Z3=W3 \cdot(W2 \cdot (W1 \cdot X+B1)+B2)+B3$$

$$=(W3W2W1) \cdot X+ (W3W2B1+W3W2B1+W3B2+B3)$$

$$=W \cdot X+B$$

$Z1,Z2,Z3$分别代表三层神经网络。最后可以看到，不管有多少层，总可以归结到WX+B的形式，这和单层神经网络没有区别。

如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。

我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。

Sigmoid激活函数：

$$a = \frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(wx+b)}}$$

<img src="..\Images\1\activation.png">

非线性函数是那些多次方函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。

而这一切都归结于这一点，我们需要应用激活函数a(z)，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。因此，使用非线性激活函数，我们便能够从输入输出之间生成非线性映射。

激活函数的另一个重要特征是：它应该是可导的。我们需要这样的特性，以便在网络中向后推进以计算相对于权重的误差（损失）梯度时执行反向优化策略，然后相应地使用梯度下降或任何其他优化技术改变权重以减少误差。

<img src="..\Images\1\LinearvsActivation.png" width="600">

### 1.0.6 为什么需要深度神经网络与深度学习

通常我们把三层以上的网络称为深度神经网络。两层的神经网络虽然强大，但可能只能完成二维空间上的一些拟合与线性分类的事情。如果对于图片、语音、文字序列这些复杂的事情，就需要更复杂的网络来理解和处理。第一个方式是增加每一层中神经元的数量，但这是线性的，不够有效。另外一个方式是增加层的数量，每一层都处理不同的事情。

1. **卷积神经网络 CNN (Convolutional Neural Networks)**

对于图像类的机器学习问题，最有效的就是卷积神经网络。

<img src="..\Images\1\cnn.jpg" width="600">

2. **循环神经网络 RNN (Recurrent Neural Networks)**

对于语言类的机器学习问题，最有效的就是循环神经网络。

<img src="..\Images\1\rnn.png" width="600">

### 1.0.7 Deep Learning的训练过程简介

1. 使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）

   采用无标签数据（有标签数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）。
   具体的，先用无标签数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；

2. 自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）

   基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于deep learning的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。

本系列文章主要解释第2种类型（有监督学习）的神经网络机器学习过程。
