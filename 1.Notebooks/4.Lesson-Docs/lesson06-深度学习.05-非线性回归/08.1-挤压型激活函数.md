Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 8.1 挤压型激活函数 Squashing Function

又可以叫饱和型激活函数，因为在输入值域的绝对值较大的时候，它的输出在两端是饱和的。挤压型激活函数中，用的最多的是Sigmoid函数，所谓Sigmoid函数，原意是指一类函数，它们都具有S形的函数曲线以及压缩输入值域的作用，所以又叫挤压型激活函数。

### 8.1.1 对数几率函数 Sigmoid Function

对率函数，在用于激活函数时常常被称为Sigmoid函数，因为它是最常用的Sigmoid函数。

#### 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

#### 导数

$$a^{'}(z) = a(z) \odot (1 - a(z))$$

利用公式33，令：$u=1，v=1+e^{-z}$ 则：

$$
a' = \frac{u'v-v'u}{v^2}=\frac{0-(1+e^{-z})'}{(1+e^{-z})^2}
$$
$$
=\frac{e^{-z}}{(1+e^{-z})^2}
=\frac{1+e^{-z}-1}{(1+e^{-z})^2}
$$
$$
=\frac{1}{1+e^{-z}}-(\frac{1}{1+e^{-z}})^2
$$
$$
=a-a^2=a(1-a)
$$

#### 值域

输入值域：$[-\infty, \infty]$

输出值域：$[0,1]$

#### 函数图像

<img src="..\Images\8\sigmoid.png">

#### 优点

从函数图像来看，sigmoid函数的作用是将输入压缩到(0, 1)这个区间范围内，这种输出在0~1之间的函数可以用来模拟一些概率分布的情况。他还是一个连续函数，导数简单易求。  

从数学上来看，Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。 

从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，
将非重点特征推向两侧区。

分类功能：我们经常听到这样的对白“你觉得这件事情成功概率有多大？”“我有六成把握能成功”。sigmoid函数在这里就起到了如何把一个数值转化成一个通俗意义上的把握的表示。值越大，那么这个神经元对于这张图里有这样一条线段的把握就越大，经过sigmoid函数之后的结果就越接近100%，也就是1这样一个值，表现在图里，也就是这个神经元越兴奋（亮）。

#### 缺点

exp()计算代价大。

反向传播时梯度消失：从梯度图像中可以看到，sigmoid的梯度在两端都会接近于0，根据链式法则，如果传回的误差是$\delta$，那么梯度传递函数是$\delta \cdot a'(z)$，而$a'(z)$这时是零，也就是说整体的梯度是零。这也就很容易出现梯度消失的问题，并且这个问题可能导致网络收敛速度比较慢，比如采取MSE作为损失函数算法时。

给个纯粹数学的例子吧，假定我们的学习速率是0.2，sigmoid函数值是0.9，如果我们想把这个函数的值降到0.5，需要经过多少步呢？

我们先来做数值计算：

第一步，求出当前输入的值

$$\frac{1}{1 + e^{-z}} = 0.9$$
$$e^{-z} = \frac{1}{9}$$
$$x = ln{9}$$

第二步，求出当前梯度

$$\Delta = a(z)\times(1 - a(z)) = 0.9 \times 0.1= 0.09$$

第三步，根据梯度更新当前输入值

$$z_{new} = z - \eta \times \Delta = ln{9} - 0.2 \times 0.09 = ln(9) - 0.018$$

第四步，判断当前函数值是否接近0.5

$$\frac{1}{1 + e^{-z_{new}}} = 0.898368$$

第五步，重复步骤2-3直到当前函数值接近0.5

说得如果不够直观，那我们来看看图，

<img src="..\Images\8\decay_sigmoid.png">

上半部分那条五彩斑斓的曲线就是迭代更新的过程了，一共迭代了多少次呢？根据程序统计，sigmoid迭代了67次才从0.9衰减到了接近0.5的水准。有同学可能会说了，才67次嘛，这个次数也不是很多啊！确实，从1层来看，这个速度还是可以接受的，但是神经网络只有这一层吗？多层叠加之后的sigmoid函数，因为反向传播的链式法则，两层的梯度相乘，每次更新的步长更小，需要的次数更多，也就是速度更加慢。如果还是没有反应过来的同学呢，可以先向下看relu函数的收敛速度。

此外，如果输入数据是(-1, 1)范围内的均匀分布的数据会导致什么样的结果呢？经过sigmoid函数处理之后这些数据的均值就从0变到了0.5，导致了均值的漂移，在很多应用中，这个性质是不好的。

### 8.1.2 Tanh函数

TanHyperbolic，双曲正切函数。

#### 公式  
$$a(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1$$

$$a(z) = 2 \cdot Sigmoid(2z) - 1$$

#### 导数公式

$$a'(z) = (1 + a(z)) \odot (1 - a(z))$$

利用基本导数公式23，令：$u={e^{z}-e^{-z}}，v=e^{z}+e^{-z}$ 则

$$
a'=\frac{u'v-v'u}{v^2} \tag{71}
$$
$$
=\frac{(e^{z}-e^{-z})'(e^{z}+e^{-z})-(e^{z}+e^{-z})'(e^{z}-e^{-z})}{(e^{z}+e^{-z})^2}
$$
$$
=\frac{(e^{z}+e^{-z})(e^{z}+e^{-z})-(e^{z}-e^{-z})(e^{z}-e^{-z})}{(e^{z}+e^{-z})^2}
$$
$$
=\frac{(e^{z}+e^{-z})^2-(e^{z}-e^{-z})^2}{(e^{z}+e^{-z})^2}
$$
$$
=1-(\frac{(e^{z}-e^{-z}}{e^{z}+e^{-z}})^2=1-a^2
$$

#### 值域

输入值域：$[-\infty, \infty]$

输出值域：$[-1,1]$

#### 函数图像

<img src="..\Images\8\tanh.png">

#### 优点

具有Sigmoid的所有优点。

无论从理论公式还是函数图像，这个函数都是一个和sigmoid非常相像的激活函数，他们的性质也确实如此。但是比起sigmoid，tanh减少了一个缺点，就是他本身是零均值的，也就是说，在传递过程中，输入数据的均值并不会发生改变，这就使他在很多应用中能表现出比sigmoid优异一些的效果。

#### 缺点

exp()计算代价大。

梯度消失。


### 8.1.3 其它函数

<img src="..\Images\8\others.png">

### 代码位置

ch08, Level1
