Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
 
## 4.4 多样本单特征值计算

在前面的代码中，我们一直使用单样本计算来实现神经网络的训练过程，但是单样本计算有一些缺点：
1. 很有可能前后两个相邻的样本，会对反向传播产生相反的作用而互相抵消。假设样本1造成了误差为0.5，w的梯度计算结果是0.1；紧接着样本2造成的误差为-0.5，w的梯度计算结果是-0.1，那么前后两次更新w就会产生互相抵消的作用。
2. 在样本数据量大时，逐个计算会花费很长的时间。由于我们在本例中样本量不大（200个样本），所以计算速度很快，觉察不到这一点。在实际的工程实践中，动辄10万甚至100万的数据量，轮询一次要花费很长的时间。

如果使用多样本计算，就要涉及到矩阵运算了，而所有的深度学习框架，都对矩阵运算做了优化，会大幅提升运算速度。打个比方：如果200个样本，循环计算一次需要2秒的话，那么把200个样本打包成矩阵，做一次计算也许只需要0.1秒。

下面我们来看看多样本运算会对代码实现有什么影响，假设我们一次用3个样本来参与计算，每个样本只有1个特征值。

### 4.4.1 前向计算

$$
Z = X \cdot w + b \tag{1}
$$

公式1是矩阵运算形式，w和b都是标量，如果把它展开成3个样本（3行）的形式，则是：

$$Z=
\begin{pmatrix}
    z_1 \\ 
    z_2 \\ 
    z_3
\end{pmatrix}
=
\begin{pmatrix}
    x_1 \\ 
    x_2 \\ 
    x_3
\end{pmatrix}
\cdot w + b
=
\begin{pmatrix}
    x_1 \cdot w + b \\ 
    x_2 \cdot w + b \\ 
    x_3 \cdot w + b
\end{pmatrix} \tag{2}
$$

所以，根据公式1和公式2，我们的前向计算python代码可以写成：

```Python
    def __forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        return Z
```

### 4.4.2 损失函数

用传统的均方差函数，其中，z是每一次迭代的预测输出，y是样本标签数据。我们使用m个样本参与计算，因此损失函数为：

$$J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(z_i - y_i) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

我们假设每次有3个样本参与计算，实例化后的情形是：

$$
J(w,b) = \frac{1}{2\times3}[(z_1-y_1)^2+(z_2-y_2)^2+(z_3-y_3)^2]
$$
$$
=\frac{1}{2 \times 3}sum(Z-Y)^2 \tag{3}
$$

公式3中大写的Z和Y都是矩阵形式，用代码实现：

```Python
    def __checkLoss(self, dataReader):
        X,Y = dataReader.GetWholeTrainSamples()
        m = X.shape[0]
        Z = self.__forwardBatch(X)
        LOSS = (Z - Y)**2
        loss = LOSS.sum()/m/2
        return loss
```

### 4.4.3 求w的梯度

我们用J的值作为基准，去求w对它的影响，也就是J对w的偏导数，就可以得到w的梯度了。从公式3看J的计算过程，$z_1,z_2,z_3$都对它有贡献，再从公式2看$z_1,z_2,z_3$的生成过程，都有w的参与。所以，J对w的偏导应该是这样的：

$$
\frac{\partial{J}}{\partial{w}}=\frac{\partial{J}}{\partial{z_1}}\frac{\partial{z_1}}{\partial{w}}+\frac{\partial{J}}{\partial{z_2}}\frac{\partial{z_2}}{\partial{w}}+\frac{\partial{J}}{\partial{z_3}}\frac{\partial{z_3}}{\partial{w}}
$$
$$
=\frac{1}{3}[(z_1-y_1)x_1+(z_2-y_2)x_2+(z_3-y_3)x_3]
$$
$$
=\frac{1}{3}
\begin{pmatrix}
    x_1 & x_2 & x_3
\end{pmatrix}
\begin{pmatrix}
    z_1-y_1 \\
    z_2-y_2 \\
    z_3-y_3 
\end{pmatrix} \tag{m=3}
$$
$$
=\frac{1}{m} \sum^m_{i=1} (z_i-y_i)x_i \tag{4}
$$
$$
=\frac{1}{m} X^T \cdot (Z-Y) \tag{5}
$$

其中：
$$X = 
\begin{pmatrix}
    x_1 \\ 
    x_2 \\ 
    x_3
\end{pmatrix}, X^T =
\begin{pmatrix}
    x_1 & x_2 & x_3
\end{pmatrix}
$$

公式4和公式5其实是等价的，只不过公式4用求和方式计算每个样本，公式5用矩阵方式做一次性计算。

### 4.4.4 求b的梯度

$$
\frac{\partial{J}}{\partial{b}}=\frac{\partial{J}}{\partial{z_1}}\frac{\partial{z_1}}{\partial{b}}+\frac{\partial{J}}{\partial{z_2}}\frac{\partial{z_2}}{\partial{b}}+\frac{\partial{J}}{\partial{z_3}}\frac{\partial{z_3}}{\partial{b}}
$$
$$
=\frac{1}{3}[(z_1-y_1)+(z_2-y_2)+(z_3-y_3)]
$$
$$
=\frac{1}{m} \sum^m_{i=1} (z_i-y_i) \tag{6}
$$
$$
=\frac{1}{m}\cdot sum(Z-Y) \tag{7}
$$

公式6和公式7也等价，在python中，可以直接用公式7求矩阵的和，免去了一个个计算$z_i-y_i$最后再求和的麻烦，速度还快。

```Python
    def __backwardBatch(self, batch_x, batch_y, batch_z):
        m = batch_x.shape[0]
        dZ = batch_z - batch_y
        dW = np.dot(batch_x.T, dZ)/m
        dB = dZ.sum(axis=0, keepdims=True)/m
        return dW, dB
```

### 代码位置

ch04, HelperClass//NeuralNet.py
