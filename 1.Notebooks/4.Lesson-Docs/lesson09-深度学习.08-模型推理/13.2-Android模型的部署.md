Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 知识点

- 可以部署在Android/IOS端的模型
- 将模型转换到Android端
- 将模型部署到Android

# 提出问题

模型仅仅能在电脑上运行显然是不够的呀！这样一个几乎人手至少一部手机的时代，如果能让模型在手机端直接运行，岂不是至少让我们的用户量提高一倍？

当然，我们可以把模型放在云端，让云端去进行处理，手机去拿云端处理的结果。但是，计算能力强的云端实在是，太！贵！了！

有没有比较节省的方法同时又能让深度学习在手机端看到效果呢？经过一番调研，我们决定采用让手机自己去跑深度学习的模型这样一条路来实现我们的目的。

## 什么格式的模型可以在Android端进行处理呢？

在这个方面，经过一些调研（调研，可能有吧），我们发现caffe2和tflite这两个格式的模型可以支持在Android手机端进行运行。经过验证，两个模型都可以在Android端顺利运行。相对而言，caffe2历史更悠久一些，对运算符的支持更好一些，所以这里我们采用caffe2的模型作为目标模型。

## 模型转换

在这里呢，遵循教程承前启后的脉络，我们选择将之前训练的模型(onnx格式的模型)转换成caffe2的模型格式。当然，这里如果不想要进行这一步操作，我们也可以直接使用caffe2进行模型的训练操作。

因为具体onnx模型的内容和大家的训练代码有关，我们提供了一个onnx模型供大家[下载](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/%E5%BE%AE%E8%BD%AF-%E6%96%B9%E6%A1%881/11.3/trained_model)。

如何将一个onnx模型转化成caffe2的模型呢？这一步的操作其实比较简单，只需要按照onnx-caffe2的官方例程一步一步操作即可。这里我们也给出一个示例代码

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

from caffe2.proto import caffe2_pb2
from caffe2.python import core
from onnx_caffe2.backend import Caffe2Backend
from onnx_caffe2.helper import c2_native_run_net, save_caffe2_net, load_caffe2_net, \
    benchmark_caffe2_model, benchmark_pytorch_model
import logging
import numpy as np
import onnx

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# load onnx model
onnx_model = onnx.load("model.onnx")

# Check whether the onnx_model is valid or not.
log.info("Check the ONNX model.")
onnx.checker.check_model(onnx_model)

# Convert the ONNX model to a Caffe2 model.
log.info("Convert the model to a Caffe2 model.")
init_net, predict_net = Caffe2Backend.onnx_graph_to_caffe2_net(onnx_model.graph, device="CPU")

# Save the converted Caffe2 model in the protobuf files.
log.info("Save the Caffe2 models as pb files.")
init_file = "./model_init.pb"
predict_file = "./model_predict.pb"
save_caffe2_net(init_net, init_file, output_txt=False)
save_caffe2_net(predict_net, predict_file, output_txt=True)
```

通过运行上述脚本，我们会发现在目录结构下多出了`model_predict.pb`和`model_init.pb`这样两个文件，也就是我们通过转换操作得到的caffe2的model了。

通过caffe2的官方文档，我们可以知道这样两个文件分别存储了网络的结构和对应的参数。也就是说，这两个文件保存了我们所需要使用的网络结构。

## 将模型转换到Android

在这一部分，首先我们需要Android Studio和对应的样例程序。(安装这一部分的教程请参考[这里](https://caffe2.ai/docs/AI-Camera-demo-android.html))

这里因为采用了样例程序，所以我们并不需要十分熟练的Java或者Android技能就可以轻松完成这一部分的教程。

在使用Android Studio打开AI-Camera这个样子的时候，请完全按照Android Studio提示安装所需要的各种依赖。

在这个过程中可能会遇到"Expected NDK STL shared object file libgnustl_shared.so"这样一个错误，请参考这个[issue](https://github.com/caffe2/AICamera/issues/64)对NDK进行降级即可解决。

现在，让我们来分析以下这份代码各个模块的功能。

<img src="../Images/13/Android.PNG">

经过一番研究，画出红框的部分是我们需要注意的部分，分别是这个应用的入口，对应的标签和模型函数，以及需要使用的模型文件。

从应用的入口开始进行分析，

```java
    static {
        System.loadLibrary("native-lib");
    }

    public native String classificationFromCaffe2(int h, int w, byte[] Y, byte[] U, byte[] V,
                                                  int rowStride, int pixelStride, boolean r_hwc);
    public native void initCaffe2(AssetManager mgr);
    private class SetUpNeuralNetwork extends AsyncTask<Void, Void, Void> {
        @Override
        protected Void doInBackground(Void[] v) {
            try {
                initCaffe2(mgr);
                predictedClass = "Neural net loaded! Inferring...";
            } catch (Exception e) {
                Log.d(TAG, "Couldn't load neural network.");
            }
            return null;
        }
    }
```

这一部分的代码，也就是用于将caffe2的model初始化的代码，具体的实现细节并不需要具体探究，我们可以保持这一部分的代码不变。具体这样一份代码是如何将数据传递给模型的呢？看到这里涉及到模型推理的代码是`classificationFromCaffe2`，那么我们在寻找这份文件是在哪里进行调用的呢？我们就找到了这样一个函数

```java
public void onImageAvailable(ImageReader reader) {
                    try {

                        image = reader.acquireNextImage();
                        if (processing) {
                            image.close();
                            return;
                        }
                        processing = true;
                        int w = image.getWidth();
                        int h = image.getHeight();
                        ByteBuffer Ybuffer = image.getPlanes()[0].getBuffer();
                        ByteBuffer Ubuffer = image.getPlanes()[1].getBuffer();
                        ByteBuffer Vbuffer = image.getPlanes()[2].getBuffer();
                        // TODO: use these for proper image processing on different formats.
                        int rowStride = image.getPlanes()[1].getRowStride();
                        int pixelStride = image.getPlanes()[1].getPixelStride();
                        byte[] Y = new byte[Ybuffer.capacity()];
                        byte[] U = new byte[Ubuffer.capacity()];
                        byte[] V = new byte[Vbuffer.capacity()];
                        Ybuffer.get(Y);
                        Ubuffer.get(U);
                        Vbuffer.get(V);

                        predictedClass = classificationFromCaffe2(h, w, Y, U, V,
                                rowStride, pixelStride, run_HWC);
                        runOnUiThread(new Runnable() {
                            @Override
                            public void run() {
                                tv.setText(predictedClass);
                                processing = false;
                            }
                        });

                    } finally {
                        if (image != null) {
                            image.close();
                        }
                    }
                }
```

虽然我们对Java并不熟悉，但是从命名上就可以看出，这份代码的主要目的就是在处理图像，将数据传递给模型去进行处理。

下面我们来看一看`native-lib.cpp`这样一个文件的内容。这个文件中需要修改的部分也不是很多，在这份文件中我们也可以轻易找到涉及到推理的部分其实也就是

```java
caffe2::TensorCPU input;
    if (infer_HWC) {
        input.Resize(std::vector<int>({IMG_H, IMG_W, IMG_C}));
    } else {
        input.Resize(std::vector<int>({1, IMG_C, IMG_H, IMG_W}));
    }
    memcpy(input.mutable_data<float>(), input_data, IMG_H * IMG_W * IMG_C * sizeof(float));
    caffe2::Predictor::TensorVector input_vec{&input};
    caffe2::Predictor::TensorVector output_vec;
    caffe2::Timer t;
    t.Start();
    _predictor->run(input_vec, &output_vec);
```

这样一些代码。

下面，我们来把之前训练得到的模型进行加载。我们先来改变一下前端涉及到的代码。

我们从简单的方案入手，前端就仅仅使用一个手写板，一个检测按钮，一个显示结果的文本框，利用Android Studio的拖拽功能，直接拖出一个界面，

<img src="../Images/13/Android-layout.PNG">

在这个简单的布局里面，我们需要用到的组件也就是一个ImageView， 两个Button（或者一个也是可以的），外加一个用来显示结果的TextView也就可以了。

这里，为了方面起见，我们可以把`ImageView`命名为`image`，`TextView`命名为`result`，两个Button分别为`click`和`detect`， 此时我们已经完成了前端页面的布局。进入后端页面之后，就是一个大删除的操作了，我们只需要保留一些基本的组件（也就是说，保留启动运行这个模型的必须代码），其余的代码我们可以直接忽略掉。

下面我们来分别添加各个组件的功能实现。

首先是手写模块的代码实现

```java
mBitmap = Bitmap.createBitmap(280, 280, Bitmap.Config.ARGB_8888);
        canvas = new Canvas(mBitmap);
        canvas.drawColor(Color.BLACK);
        paint = new Paint();
        paint.setColor(Color.WHITE);
        paint.setStrokeWidth(35);
        paint.setStyle(Paint.Style.STROKE);
        canvas.drawBitmap(mBitmap, new Matrix(), paint);
        image.setImageBitmap(mBitmap);
        image.setOnTouchListener(new View.OnTouchListener() {
            int startX;
            int startY;
            @Override
            public boolean onTouch(View view, MotionEvent motionEvent) {
                switch (motionEvent.getAction()) {
                    case MotionEvent.ACTION_DOWN:
                        // 获取手按下时的坐标
                        startX = (int) motionEvent.getX();
                        startY = (int) motionEvent.getY();
                        break;
                    case MotionEvent.ACTION_MOVE:
                        // 获取手移动后的坐标
                        int endX = (int) motionEvent.getX();
                        int endY = (int) motionEvent.getY();
                        // 在开始和结束坐标间画一条线
                        canvas.drawLine(startX, startY, endX, endY, paint);
                        // 刷新开始坐标
                        startX = (int) motionEvent.getX();
                        startY = (int) motionEvent.getY();
                        image.setImageBitmap(mBitmap);
                        break;
                }
                return true;
            }
        });
```

接着我们来实现按下detect按钮之后触发的动作，

```java
btnDetectObject.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

                Bitmap bitmap = mBitmap;

                bitmap = Bitmap.createScaledBitmap(bitmap, INPUT_SIZE, INPUT_SIZE, false);

                int[] data = new int[28 * 28];    //通过位图的大小创建像素点数组
                bitmap.getPixels(data, 0, 28, 0, 0, 28, 28);
                // 此时因为图片是黑白图，只需要取其中一个通道进行操作即可
                for (int i = 0; i < 28; i++) {
                    for (int j = 0; j < 28; j++) {
                        int grey = data[28 * i + j];

                        grey = (grey >> 8) & 0xFF;

                        data[28 * i + j] = grey;
                    }
                }
                int w = 28;
                int h = 28;
                predictedClass = classificationFromCaffe2(h, w, data, run_HWC);
                result.setText(predictedClass);
            }
        });
```

注意，在这里的修改过程中我们修改了传入推理函数的参数，从三个`JbyteArray`改成了一个`JIntArray`。这里修改之后也需要对`cpp`中的函数入口进行对应的修改，也就是让参数匹配。这个稍后再说。

clear按钮所需要做的也就是将图片上的笔迹清空，也就是，

```java
btnToggleCamera.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                canvas.drawColor(Color.BLACK);
                image.setImageBitmap(mBitmap);
            }
        });
```

因为我们传入的是一个用于识别0，1的模型，因此需要将`classes.h`文件中的类进行对应的修改，即修改成0和1两类。

<img src="../Images/13/classes.PNG">

最后要修改的就是推理中使用的函数，也就是`native-lib.cpp`这样一个文件啦。这里的修改，要注意的是对入口参数的修改和对数据的处理。因为模型训练时接受的数据是处于[-0.5,0.5]这样一个范围的，所以我们需要对传递进来的数据进行同样的归一化操作。

```cpp
Java_facebook_f8demo_ClassifyCamera_classificationFromCaffe2(
        JNIEnv *env,
        jobject /* this */,
        jint h, jint w, jintArray D, // 此处使用jintArray
        jboolean infer_HWC) {
    if (!_predictor) {
        return env->NewStringUTF("Loading...");
    }

    // 对传入数组的指针进行处理，获得数据
    jboolean flag = false;
    jsize D_len = env->GetArrayLength(D);
    assert(D_len <= MAX_DATA_SIZE);
    jint * arr = env->GetIntArrayElements(D, &flag);


#define min(a,b) ((a) > (b)) ? (b) : (a)
#define max(a,b) ((a) > (b)) ? (a) : (b)

    auto h_offset = max(0, (h - IMG_H) / 2);
    auto w_offset = max(0, (w - IMG_W) / 2);

    auto iter_h = IMG_H;
    auto iter_w = IMG_W;
    if (h < IMG_H) {
        iter_h = h;
    }
    if (w < IMG_W) {
        iter_w = w;
    }

    for (auto i = 0; i < iter_h; ++i) {
        // 获得每一行的数据指针
        int* D_row = (int*)&arr[(h_offset + i) * w];
        for (auto j = 0; j < iter_w; ++j) {
            // 归一化数据，此时因为手绘，可以确定最大值255
            // 不然此处应当选取数据中最大值作为除数
            float y = D_row[w_offset + j] / 255.0 - 0.5;
            int index = i * w + j;
            input_data[index] = y;
        }
    }

    caffe2::TensorCPU input;
    // 对输入数据格式进行调整
    if (infer_HWC) {
        input.Resize(std::vector<int>({IMG_H, IMG_W, IMG_C}));
    } else {
        input.Resize(std::vector<int>({1, 1, IMG_H, IMG_W}));
    }
    memcpy(input.mutable_data<float>(), input_data, IMG_H * IMG_W * IMG_C * sizeof(float));

    // 原始代码，无需改动
    caffe2::Predictor::TensorVector input_vec{&input};
    caffe2::Predictor::TensorVector output_vec;
    caffe2::Timer t;
    t.Start();
    _predictor->run(input_vec, &output_vec);
    float fps = 1000/t.MilliSeconds();
    total_fps += fps;
    avg_fps = total_fps / iters_fps;
    total_fps -= avg_fps;

    constexpr int k = 2;
    float max[k] = {0};
    int max_index[k] = {0};
    // Find the top-k results manually.
    if (output_vec.capacity() > 0) {
        for (auto output : output_vec) {
            for (auto i = 0; i < output->size(); ++i) {
                for (auto j = 0; j < k; ++j) {
                    if (output->template data<float>()[i] > max[j]) {
                        for (auto _j = k - 1; _j > j; --_j) {
                            max[_j - 1] = max[_j];
                            max_index[_j - 1] = max_index[_j];
                        }
                        max[j] = output->template data<float>()[i];
                        max_index[j] = i;
                        goto skip;
                    }
                }
                skip:;
            }
        }
    }
    std::ostringstream stringStream;
    stringStream << avg_fps << " FPS\n";

    for (auto j = 0; j < k; ++j) {
        stringStream << j << ": " << imagenet_classes[max_index[j]] << " - " << max[j] << "%\n";
    }
    return env->NewStringUTF(stringStream.str().c_str());
}
```

好了，这样一份Android Studio的工程就修改完啦!完整的代码文件请参看[这里](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/%E5%BE%AE%E8%BD%AF-%E6%96%B9%E6%A1%881/11.3/AICamera)。下面要做的就是把自己的手机连上电脑，运行查看结果啦！