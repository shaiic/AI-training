Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 12.1 三层神经网络的实现

### 12.1.1 定义神经网络

<img src='../Images/12/nn3.png'/>

### 输入层

28x28=784个特征值：

$$
X=\begin{pmatrix}
    x_1 & x_2 & ... & x_{784}
  \end{pmatrix}
$$

#### 隐层1

- 权重矩阵w1形状为784x64

$$
W1=\begin{pmatrix}
    w^1_{1,1} & w^1_{1,2} & ... & w^1_{1,64} \\
    ... & ... & & ... \\
    w^1_{784,1} & w^1_{784,2} & ... & w^1_{784,64} 
  \end{pmatrix}
$$

- 偏移矩阵b1的形状为1x64

$$
B1=\begin{pmatrix}
    b^1_{1} & b^1_{2} & ... & b^1_{64}
  \end{pmatrix}
$$

- 隐层1由64个神经元构成，其结果为1x64的矩阵

$$
Z1=\begin{pmatrix}
    z^1_{1} & z^1_{2} & ... & z^1_{64}
  \end{pmatrix}
$$
$$
A1=\begin{pmatrix}
    a^1_{1} & a^1_{2} & ... & a^1_{64}
  \end{pmatrix}
$$

#### 隐层2

- 权重矩阵w2形状为64x16

$$
W2=\begin{pmatrix}
    w^2_{1,1} & w^2_{1,2} & ... & w^2_{1,16} \\
    ... & ... & & ... \\
    w^2_{64,1} & w^2_{64,2} & ... & w^2_{64,16} 
  \end{pmatrix}
$$

- 偏移矩阵b2的形状是1x16

$$
B2=\begin{pmatrix}
    b^2_{1} & b^2_{2} & ... & b^2_{16}
  \end{pmatrix}
$$

- 隐层2由16个神经元构成

$$
Z2=\begin{pmatrix}
    z^2_{1} & z^2_{2} & ... & z^2_{16}
  \end{pmatrix}
$$
$$
A2=\begin{pmatrix}
    a^2_{1} & a^2_{2} & ... & a^2_{16}
  \end{pmatrix}
$$

#### 输出层

- 权重矩阵w3的形状为16x10

$$
W3=\begin{pmatrix}
    w^3_{1,1} & w^3_{1,2} & ... & w^3_{1,10} \\
    ... & ... & & ... \\
    w^3_{16,1} & w^3_{16,2} & ... & w^3_{16,10} 
  \end{pmatrix}
$$

- 输出层的偏移矩阵b3的形状是1x10

$$
B3=\begin{pmatrix}
    b^3_{1}& b^3_{2} & ... & b^3_{10}
  \end{pmatrix}
$$

- 输出层有10个神经元使用Softmax函数进行分类

$$
Z3=\begin{pmatrix}
    z^3_{1} & z^3_{2} & ... & z^3_{10}
  \end{pmatrix}
$$
$$
A3=\begin{pmatrix}
    a^3_{1} & a^3_{2} & ... & a^3_{10}
  \end{pmatrix}
$$

### 12.1.2 前向计算

我们都是用大写符号的矩阵形式的公式来描述，在每个矩阵符号的右上角是其形状。

#### 隐层1

$$X^{(1,784)} \cdot W1^{(784,64)} + B1^{(1,64)} => Z1^{(1,64)} \tag{1}$$

$$Sigmoid(Z1) => A1^{(1,64)} \tag{2}$$

#### 隐层2

$$A1^{(1,64)} \cdot W2^{(64,16)} + B2^{(1,16)} => Z2^{(1,16)} \tag{3}$$

$$Tanh(Z2) => A2^{(1,16)} \tag{4}$$

#### 输出层

$$A2^{(1,16)} \cdot W3^{(16,10)}  + B3^{(1,10)} => Z3^{(1,10)} \tag{5}$$

$$Softmax(Z3) => A3^{(1,10)} \tag{6}$$

我们的约定是行为样本，列为一个样本的所有特征，这里是784个特征，因为图片高和宽是28x28，总共784个点，把每一个点的值做为特征向量。

两个隐层，分别定义64个神经元和16个神经元。第一个隐层用Sigmoid激活函数，第二个隐层用Tanh激活函数。

输出层10个神经元，再加上一个Softmax计算，最后有a1,a2,...a10十个输出，分别代表0-9的10个数字。

### 12.1.3 反向传播

和以前的两层网络没有多大区别，只不过多了一层，而且用了tanh激活函数，目的是想把更多的梯度值回传，因为tanh函数比sigmoid函数稍微好一些，比如原点对称，零点梯度值大。

#### 输出层

$$dZ3 = A3-Y \tag{7}$$
$$dW3 = A2^T \times dZ3 \tag{8}$$
$$dB3=dZ3 \tag{9}$$

#### 隐层2

$$dA2 = dZ3 \times W3^T \tag{10}$$
$$dZ2 = dA2 \odot (1-A2 \odot A2) \tag{11}$$
$$dW2 = A1^T \times dZ2 \tag{12}$$
$$dB2 = dZ2 \tag{13}$$

#### 隐层1

$$dA1 = dZ2 \times W2^T \tag{14}$$
$$dZ1 = dA1 \odot A1 \odot (1-A1) \tag{15}$$
$$dW1 = X^T \times dZ1 \tag{16}$$
$$dB1 = dZ1 \tag{17}$$

### 12.1.4 代码实现

在HelperClass3\NeuralNet3.py中，下面主要列出与两层网络不同的代码。

#### 初始化

```Python
class NeuralNet3(object):
    def __init__(self, hp, model_name):
        ...
        self.wb1 = WeightsBias(self.hp.num_input, self.hp.num_hidden1, self.hp.init_method, self.hp.eta)
        self.wb1.InitializeWeights(self.subfolder, False)
        self.wb2 = WeightsBias(self.hp.num_hidden1, self.hp.num_hidden2, self.hp.init_method, self.hp.eta)
        self.wb2.InitializeWeights(self.subfolder, False)
        self.wb3 = WeightsBias(self.hp.num_hidden2, self.hp.num_output, self.hp.init_method, self.hp.eta)
        self.wb3.InitializeWeights(self.subfolder, False)
```
初始化部分需要构造三组WeightsBias对象，请注意各组的输入输出数量，决定了矩阵的形状。

#### 前向计算

```Python
    def forward(self, batch_x):
        # 公式1
        self.Z1 = np.dot(batch_x, self.wb1.W) + self.wb1.B
        # 公式2
        self.A1 = Sigmoid().forward(self.Z1)
        # 公式3
        self.Z2 = np.dot(self.A1, self.wb2.W) + self.wb2.B
        # 公式4
        self.A2 = Tanh().forward(self.Z2)
        # 公式5
        self.Z3 = np.dot(self.A2, self.wb3.W) + self.wb3.B
        # 公式6
        if self.hp.net_type == NetType.BinaryClassifier:
            self.A3 = Logistic().forward(self.Z3)
        elif self.hp.net_type == NetType.MultipleClassifier:
            self.A3 = Softmax().forward(self.Z3)
        else:   # NetType.Fitting
            self.A3 = self.Z3
        #end if
        self.output = self.A3
```
前向计算部分增加了一层，并且使用Tanh()做为激活函数。

- 反向传播
```Python
    def backward(self, batch_x, batch_y, batch_a):
        # 批量下降，需要除以样本数量，否则会造成梯度爆炸
        m = batch_x.shape[0]

        # 第三层的梯度输入 公式7
        dZ3 = self.A3 - batch_y
        # 公式8
        self.wb3.dW = np.dot(self.A2.T, dZ3)/m
        # 公式9
        self.wb3.dB = np.sum(dZ3, axis=0, keepdims=True)/m 

        # 第二层的梯度输入 公式10
        dA2 = np.dot(dZ3, self.wb3.W.T)
        # 公式11
        dZ2,_ = Tanh().backward(None, self.A2, dA2)
        # 公式12
        self.wb2.dW = np.dot(self.A1.T, dZ2)/m 
        # 公式13
        self.wb2.dB = np.sum(dZ2, axis=0, keepdims=True)/m 

        # 第一层的梯度输入 公式8
        dA1 = np.dot(dZ2, self.wb2.W.T) 
        # 第一层的dZ 公式10
        dZ1,_ = Sigmoid().backward(None, self.A1, dA1)
        # 第一层的权重和偏移 公式11
        self.wb1.dW = np.dot(batch_x.T, dZ1)/m
        self.wb1.dB = np.sum(dZ1, axis=0, keepdims=True)/m 

    def update(self):
        self.wb1.Update()
        self.wb2.Update()
        self.wb3.Update()
```
反向传播也相应地增加了一层，注意要用对应的Tanh()的反向公式。梯度更新时也是三组权重值同时更新。

- 主过程

```Python
from HelperClass3.MnistImageDataReader import *
from HelperClass3.HyperParameters3 import *
from HelperClass3.NeuralNet3 import *

if __name__ == '__main__':

    dataReader = MnistImageDataReader(mode="vector")
    dataReader.ReadData()
    dataReader.NormalizeX()
    dataReader.NormalizeY(YNormalizationMethod.MultipleClassifier, base=0)
    dataReader.Shuffle()
    dataReader.GenerateValidationSet()

    n_input = dataReader.num_feature
    n_hidden1 = 64
    n_hidden2 = 16
    n_output = dataReader.num_category
    eta = 0.2
    eps = 0.01
    batch_size = 128
    max_epoch = 40

    hp = HyperParameters3(n_input, n_hidden1, n_hidden2, n_output, eta, max_epoch, batch_size, eps, NetType.MultipleClassifier, InitialMethod.Xavier)
    net = NeuralNet3(hp, "MNIST_64_16")
    net.train(dataReader, 0.5, True)
    net.ShowTrainingTrace(xline="iteration")
```
超参配置：第一隐层64个神经元，第二隐层16个神经元，学习率0.2，批大小128，Xavier初始化，最大训练40个epoch。

### 12.1.4 运行结果

损失函数值和准确度值变化曲线：

<img src='../Images/12/loss.png'/>

打印输出部分：

```
...
epoch=38, total_iteration=16769
loss_train=0.012860, accuracy_train=1.000000
loss_valid=0.100281, accuracy_valid=0.969400
epoch=39, total_iteration=16984
loss_train=0.008067, accuracy_train=1.000000
loss_valid=0.097571, accuracy_valid=0.971400
epoch=39, total_iteration=17199
loss_train=0.006867, accuracy_train=1.000000
loss_valid=0.098164, accuracy_valid=0.971000
time used: 25.697904109954834
testing...
0.9749
```

在测试集上得到的准确度为97.49%，比较理想。

### 代码位置

ch12, Level1

### 思考与练习

1. 我们在前面说过，隐层的神经元数要大于输入的特征值数，才能很好地处理多个特征值的输入。但是在这个问题里，我们一共有784个特征值输入，但是隐层只使用了64个神经元，远远小于特征值数？这是为什么？
2. 在隐层1使用256个神经元会得到更好的效果吗？
