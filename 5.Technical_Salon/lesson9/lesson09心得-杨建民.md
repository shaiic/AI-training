# 微软-仪电人工智能高阶人才培训(第一期）学习心得之九：神经网络基本原理简明教程
## 作者：杨建民

晓武和婷婷两位老师联合讲授的晓武老师编著的《神经网络基本原理简明教程》将我们学员们领进了人工智能领域，为我们后续探索和思考如何将人工智能技术与各行业融合奠定了基础，在此感谢两位老师！

以下整理了个人认为比较重要的知识点（部分内容和图片源于网络）

## 重点知识点
### 1、学习率
晓武老师文中对学习率的重要性引用了业内人士的流传的一句话：如果所有超参中，只需要调整一个参数，那么就是学习率。学习率很重要，但是学习率是一个非常难调的超参。我们会经碰到以下问题：
- 很难选择出合适的学习率：太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。

- 相同的学习率并不适用于所有的参数更新：如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。

- 避免陷于多个局部最小值中：实际上，问题并非源于局部最小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。

#### 学习率设置经验
在训练过程中，一般根据训练轮数设置动态变化的学习率。
- 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。
- 一定轮数过后：逐渐减缓。
- 接近训练结束：学习速率的衰减应该在100倍以上。
- 若是迁移学习，由于模型已在原始数据上收敛，应该设置较小学习率 (≤10−4) 在新数据上进行 微调 。

如何快速找到合适的初始学习率，晓武老师文中提到了Leslie N. Smith 在2015年的一篇论文Cyclical Learning Rates for Training Neural Networks，估计网络允许的最小学习率和最大学习率，我们也可以用来找我们的最优初始学习率。

#### 结合损失函数曲线，把脉学习率的设置
<img src="image\learning_rate.png">

- 曲线初始时上扬 [黄线]： 解决办法：初始学习率过大导致振荡，应减小学习率，并从头开始训练；
- 曲线初始时强势下降没多 归于水平 [红线]：解决办法：后期学习率过大导致无法拟合，应减小学习率，并重新训练后几轮 ；
- 曲线全程缓慢 [紫线]：解决办法：初始 学习率过小 导致收敛慢，应增大学习率，并从头开始训练；
- 绿色曲线：正确的学习率设置。

### 2、损失函数
损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：

$\theta^{*}=\arg \min _{\theta} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i} ; \theta\right)\right)+\lambda \Phi(\theta)$

其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ。

#### LogLoss对数损失函数（逻辑回归，交叉熵损失）

log损失函数的标准形式：

$L(Y, P(Y | X))=-\log P(Y | X)$

逻辑回归目标式子如下：

$J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]$

#### 平方损失函数（最小二乘法）
平方损失（Square loss）的标准形式如下：

$L(Y, f(X))=(Y-f(X))^{2}$

在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：

$M S E=\frac{1}{n} \sum_{i=1}^{n}\left(\tilde{Y}_{i}-Y_{i}\right)^{2}$

#### 指数损失函数（Adaboost）
指数损失函数（exp-loss）的标准形式：

$L(y, f(x))=\exp [-y f(x)]$

#### Hinge损失函数（SVM）
Hinge 损失函数的标准形式：

$L(y)=\max (0,1-y \tilde{y}), y=\pm 1$
#### 0-1损失函数

$L(Y, f(X))=\left\{\begin{array}{ll}{1,} & {Y \neq f(X)} \\ {0,} & {y=f(X)}\end{array}\right.$

#### 绝对值损失函数

$L(Y, f(X))=|Y-f(X)|$

几种损失函数的可视化图像，可以帮助我们理解一下：

<img src="image\loss_function.png">

### 3、梯度下降优化算法
梯度下降是一种寻找函数极小值的优化方法，在深度学习模型中常常用来在反向传播过程中更新神经网络的权值。通过梯度下降，优化算法可以在如下三个主要方面起作用，从方程可见：
$w_{\mathrm{new}}=w-\alpha \frac{\partial L}{\partial w}$

1. 修改学习率成分，α；【主要通过在学习率之上乘一个0到1之间的因子从而使得学习率降低（例如RMSprop）】
2. 修改梯度成分 ∂L/∂w；【通常会使用梯度的滑动平均（也可称之为“动量”）而不是纯梯度来决定下降方向。】
3. 或二者兼有；【结合两者，例如Adam和AMSGrad】

下图展现了梯度下降法的演化：

<img src="image\gradient.png">

优化算法从最简单的纯梯度下降（SGD）演化成Adam的各类变种的。SGD一开始分别往两个方向演变：
- 一类是AdaGrad，主要是调整学习率（learning rate）。
- 一类是Momentum，主要调整梯度（gradient）的构成要素。

### 4、批次大小
batch_size可以理解为批处理参数，它的极限值为训练集样本总数，当数据量比较少时，可以将batch_size值设置为全数据集（Full batch cearning）。在深度学习中所涉及到的数据都是比较多的，一般都采用小批量数据处理原则。
#### 小批量训练网络的优点
相对海量的的数据集和内存容量，小批量处理需要更少的内存就可以训练网络。
通常小批量训练网络速度更快
全数据集确定的方向能够更好地代表样本总体，从而能够更准确地朝着极值所在的方向；但是不同权值的梯度值差别较大，因此选取一个全局的学习率很困难。

#### 小批量训练网络的缺点
批次越小，梯度的估值就越不准确;
极端特例batch_size = 1，也成为在线学习（online learning）,每次修正方向以各自样本的梯度方向修正，这就造成了波动较大，难以达到收敛效果。

<img src="image\batch_size.png">

stochastic(红色)表示在线学习，batch_size = 1；
mini_batch(绿色)表示批梯度下降法，batch_size = 100；
batch(蓝色)表示全数据集梯度下降法，batch_size = 1100；

设置mini_batch大小是一种艺术，太小时可能会使学习过于随机，虽然训练速率很快，但会收敛到不可靠的模型；

#### 如何选择合适的batch_size值
采用批梯度下降法mini batch learning时，如果数据集足够充分，用一半（甚至少的多）的数据训练算出来的梯度与全数据集训练full batch learning出来的梯度几乎一样。

在合理的范围内，增大batch_size可以提高内存利用率，大矩阵乘法的并行化效率提高；跑完一次epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快；在适当的范围内，batch_size越大，其确定的下降方向越准，引起训练波动越小。注意，当batch_size增大到一定程度，其确定的下降方向基本不会变化。
batch_size值增大到超过合理范围时，和全数据训练full batch learning就会表现出相近的症候；内存容量占有率增加，跑完一次epoch（全数据集）所需的迭代次数减少，达到相同的精度所耗损的时间增加，从而对参数的修正也就显得更加缓慢。

### 5、数据预处理

数据预处理在构建网络模型时是很重要的，往往能够决定训练结果。当然对于不同的数据集，预处理的方法都会有或多或少的特殊性和局限性。
#### 零均值
这是数据处理阶段最为常用的方法。顾名思义，就是将每一维原始数据减去每一维数据的平均值，将结果代替原始数据，这就是零均值。
```Python
X -= np.mean(X,axis = 0)
```
#### 归一化（normalization）

归一化就是将原始数据归一到相同尺度，当前有两种归一化的方法：
- 第一种就是先将原始数据进行零均值，再将每一维的数据除以每一维数据的标准差；
```Python
X -= np.mean(X,axis = 0)
X /= np.std(X,axis = 0)
```
- 第二种就是将不同维度的数据归一到相同的数值区间；

<img src="image\dataprocess_normalized.png">

#### PCA和白化（whitened）

首先我们将数据变成0均值的，然后计算数据的协方差矩阵来得到数据不同维度之间的相关性：
```python
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
```

协方差矩阵的第(i,j)个元素表示数据第i维和第j维特征的相关性，特别地，对角线上的元素表示方差。另外，协方差矩阵是对称并且半正定的。我们可以对上述协方差矩阵进行SVD分解： 
```python
U,S,V = np.linalg.svd(cov)
```
其中U矩阵的列为特征向量，S对角线上的元素是奇异值（等同于特征值的平方）。为了对数据去相关，我们首先将数据（0均值后的）投影到特征向量上：
```python
Xrot = np.dot(X, U) # decorrelate the data
```
np.linalg.svd的一个很好的特性在于返回的U是按照其特征值的大小排序的，排在前面的就是主方向，因此我们可以通过选取前几个特征向量来减少数据的维度，这就是PCA的降维方法： 
```python
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100] 
```

<img src="image\dataprocess_whitened.png">

### 6、卷积神经网络
卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，是传统神经网络的一个改进。

<img src="image\cnn_car.jpg">

卷积神经网络的层级结构：
- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层 / ReLU layer
- 池化层 / Pooling layer
- 全连接层 / FC layer

#### 数据输入层
该层要做的处理主要是对原始图像数据进行预处理，其中包括：
- 去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。
- 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。
- PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化

#### 卷积计算层
这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。在这个卷积层，有两个关键操作：
- 局部关联。每个神经元看做一个滤波器(filter)
- 窗口(receptive field)滑动， filter对局部数据计算
- 
<img src="image\cnn_map.png">

蓝色的矩阵(输入图像)对粉色的矩阵（filter）进行矩阵内积计算并将三个内积运算的结果与偏置值b相加（比如上面图的计算：2+（-2+1-2）+（1-2-2） + 1= 2 - 3 - 3 + 1 = -3），计算后的值就是绿框矩阵的一个元素。

参数共享机制
　　•	在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。

<img src="image\cnn_share.png">

#### 激励层
把卷积层输出结果做非线性映射。

<img src="image\cnn_activation.jpg">

CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱

#### 池化层
池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。

<img src="image\cnn_pooling.jpg">

- 特征不变性
- 特征降维
- 在一定程度上防止过拟合，更方便优化。

#### 全连接层
两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。

#### 卷积神经网络优缺点
优点：
- 共享卷积核，对高维数据处理无压力
- 无需手动选取特征，训练好权重，即得特征分类效果好

缺点：
- 需要调参，需要大样本量，训练最好要GPU

#### 卷积神经网络之典型CNN
- LeNet，这是最早用于数字识别的CNN
- AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比
- LeNet更深，用多层小卷积层叠加替换单大卷积层。
- ZF Net， 2013 ILSVRC比赛冠军
- GoogLeNet， 2014 ILSVRC比赛冠军
- VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好

# 关于
微软-仪电人工智能创新院
微软-仪电人工智能创新院将由微软和仪电共同运营和管理，致力于为微软和仪电在人工智能方面的联合研究活动和项目提供支持，为当地企业提供基于微软技术的人工智能研发平台服务和培训服务。

# 关于人工智能高阶人才培训班
微软和仪电共同打造的人工智能高阶人才培训班由创新院运营，第一期历时三个月，授课老师包括来自微软和仪电的多位专家，内容涵盖人工智能导论、数学基础、深度学习、应用实例等课程，以及关于强化学习、自然语言处理、计算机视觉等热门方向的专题研讨会，希望帮助学员掌握人工智能的理论与实践，培养具备前瞻视野和实践能力的创新型人才。

更多信息，请关注微信公众号
<img src="image\barcode.jpg">