# 微软-仪电人工智能高阶人才培训学习心得之四：301课程-《深度学习》《神经网络》
## 作者：刘恒

经过了1个月的课程，今天微软亚洲研究院秦婷婷老师正式带我们进入了神经网络的学习篇章。

## 神经网络 

### 什么是神经网络
>- 神经网络可以指向两种，一个是生物神经网络，一个是人工神经网络，我们要学习了解的肯定是后者。
>- 人工神经网络（Artificial Neural Networks，简写为ANNs）也简称为神经网络（NNs）或称作连接模型（Connection Model），它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。
>- 人工神经网络：是一种应用类似于大脑神经突触联接的结构进行信息处理的数学模型。在工程与学术界也常直接简称为“神经网络”或类神经网络。
>
>神经元的数学/计算模型
>![20190627000001.jpg](image/20190627000001.jpg)
>-输入(数据)+权重(w)+偏移(b临界值)+激活函数
>
>重点
>- 一个神经元可以有多个输入
>- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元
>- 一个神经元的w的数量和输入的数量一致
>- 一个神经元只有一个b
>- w和b有人为的初始值，在训练过程中被不断修改
>- 激活函数不是必须有的，亦即A可以等于Z
>- 一层神经网络中的所有神经元的激活函数必须一致

### 训练流程
>![20190627000002.png](image/20190627000002.png)

>神经网络训练的最基本的思想就是：先“蒙”一个结果，我们叫预测结果a，看看这个预测结果和事先标记好的训练集中的真实结果y之间的差距，然后调整策略，再试一次，这一次就不是“蒙”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即|a-y|->0，就结束训练。

>- “蒙”，但不是瞎蒙(随机初始化权重矩阵，可以根据高斯分布或者正态分布等来初始化)
>- 拿一个或一批数据作为输入，带入权重矩阵中计算，再通过激活函数传入下一层，最终得到预测值。
>- 计算损失(均方差公式)
>- 根据一些神奇的数学公式（反向微分）
>- 不断地迭代下去，直到以下一个或几个条件满足就停止训练：损失函数值非常小；迭代了指定的次数；
>- 训练完成后，我们会把这个神经网络中的结构和权重矩阵的值导出来，形成一个计算图（就是矩阵运算加上激活函数）模型，然后嵌入到任何可以识别/调用这个模型的应用程序中，根据输入的值进行运算，输出预测值。

### 主要功能
>回归/拟合 Regression/fitting
>分类 Classification

### 为什么需要深度神经网络与深度学习
>- 通常我们把三层以上的网络称为深度神经网络。两层的神经网络虽然强大，但可能只能完成二维空间上的一些拟合与线性分类的事情。如果对于图片、语音、文字序列这些复杂的事情，就需要更复杂的网络来理解和处理。第一个方式是增加每一层中神经元的数量，但这是线性的，不够有效。另外一个方式是增加层的数量，每一层都处理不同的事情。

>- [卷积神经网络 CNN (Convolutional Neural Networks)](https://yq.aliyun.com/articles/637953)
>- [循环神经网络 RNN (Recurrent Neural Networks)](https://www.jianshu.com/p/87aa03352eb9)


## 三大概念

### 反向传播，梯度下降，损失函数

>这三个概念是前后紧密相连的，讲到一个，肯定会牵涉到另外一个。

#### 梯度下降

>- 在自然界中，梯度下降的最好例子，就是泉水下山的过程：
>- 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）
>- 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）
>- 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）

>梯度下降的数学公式：
>- $$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$
其中：
>- $\theta_{n+1}$：下一个值
>- $\theta_n$：当前值
>- $-$：梯度的反向
>- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长
>- $\nabla$：梯度，函数当前位置的最快上升点
>- $J(\theta)$：函数

>梯度下降的三要素
>- 当前点,方向和步长

>为什么说是“梯度下降”？
>- “梯度下降”包含了两层含义：
>- 1.梯度：函数当前位置的最快上升点
>- 2.下降：与导数相反的方向，用数学语言描述就是那个减号

#### 损失函数

> 损失就是所有样本的误差的总和

>- 数学公式 $$损失 = \sum^m_{i=1}误差_i$$

>作用:计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行

>计算步骤
>- 用随机值初始化前向计算公式的参数
>- 代入样本，计算输出的预测值
>- 用损失函数计算预测值和标签值（真实值）的误差
>- 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值
>- goto 2, 直到损失函数值达到一个满意的值就停止迭代

>神经网络中常用的损失函数
>- 均方差函数，主要用于回归。详见3.1-均方差损失函数
>- 交叉熵函数，主要用于分类。详见3.2-交叉熵损失函数。



>教材中有[三个通熟易懂的例子描述了三者之间的关系](https://github.com/microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Step1%20-%20BasicKnowledge/02.0-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.md)

>1.猜数
>- 目的：猜到乙心中的数字
>- 初始化：甲猜5
>- 前向计算：甲每次猜的新数字
>- 损失函数：乙在根据甲猜的数来和自己心中想的数做比较
>- 反向传播：乙告诉甲“太小了”、“有点儿大”
>- 梯度下降：甲根据乙的反馈中的含义自行调整下一轮的猜测值

>2.黑盒子
>- 目的：猜到一个输入值，使得黑盒子的输出是4
>- 初始化：输入1
>- 前向计算：黑盒子内部的数学逻辑
>- 损失函数：在输出端，用输出值减4
>- 反向传播：告诉猜数的人差值，包括正负号和值
>- 梯度下降：在输入端，根据正负号和值，确定下一次的猜测值，goto前向计算

>3.打靶
>- 目的：打中靶心
>- 初始化：随便打一枪，能上靶就行，但是要记住当时的步枪的姿态
>- 前向计算：让子弹飞一会儿，击中靶子
>- 损失函数：环数，偏离角度
>- 反向传播：把靶子拉回来看
>- 梯度下降：根据本次的偏差，调整步枪的射击角度，goto前向计算

>通俗的总结
>- 初始化
>- 在正向计算完成后
>- 损失函数为我们提供了计算损失的方法
>- 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向
>- 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重
>- 再次正向计算


## 反向传播四大公式推导

>著名的反向传播四大公式是：

>- $$\delta^{L} = \nabla_{a}C \odot \sigma_{'}(Z^L) \tag{80}$$
>- $$\delta^{l} = ((W^{l + 1})^T\delta^{l+1})\odot\sigma_{'}(Z^l) \tag{81}$$
>- $$\frac{\partial{C}}{\partial{b_j^l}} = \delta_j^l \tag{82}$$
>- $$\frac{\partial{C}}{\partial{w_{jk}^{l}}} = a_k^{l-1}\delta_j^l \tag{83}$$




# 关于微软-仪电人工智能创新院
微软-仪电人工智能创新院将由微软和仪电共同运营和管理，致力于为微软和仪电在人工智能方面的联合研究活动和项目提供支持，为当地企业提供基于微软技术的人工智能研发平台服务和培训服务。

# 关于培训
微软和仪电共同打造的微人工智能高阶人才培训第一期培训班由创新院运营，历时三个月，授课老师包括来自微软和上海仪电的多位专家，内容涵盖人工智能导论、数学基础、深度学习、应用实例等课程，以及关于强化学习、自然语言处理、计算机视觉等热门方向的专题研讨会，希望帮助学员掌握人工智能的理论与实践，培养具备前瞻视野和实践能力的创新型人才。

更多信息，请关注微信公众号
![二维码](./image/barcode.jpg)