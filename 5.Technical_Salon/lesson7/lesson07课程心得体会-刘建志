# 微软-仪电人工智能高阶人才培训(第一期）学习心得之一：自然语言处理课程
## 作者：刘建志（上海仪电中央研究院）

今天一天是自然语言处理（NLP）专题，周明院长对自然语言处理的历史，关键技术及体系，未来发展方向等方面做了系统的总结和梳理，听了之后收获良多。以下是周明院长相关报告的笔记。
# 自然语言处理及历史沿革

## 1. 什么是NLP？ NLP的研究内容？

自然语言处理就是用计算机对人类语言进行处理，使得计算机具备人类的听、说、读、写能力，它是未来人工智能技术最为关键的核心之一。周明老师引用了比尔·盖茨的一句话，“自然语言处理是人工智能皇冠上的明珠，如果我们能够推进自然语言处理，就可以再造一个微软。“ 来说明自然语言处理的难度及重要性。

那么自然语言处理研究问题是什么呢，听到这里还是没有概念，只是感觉自然语言处理听起来很厉害的样子。[这篇博客](https://blog.csdn.net/meihao5/article/details/79592667) 将NLP的主要研究问题总结为一下几个方面：

 |  |  | |  | 
- | :-: | :-: | :-: | :-: | :-: | -:
信息检索 | 机器翻译 | 文档分类 | 问答系统 
| 语音识别|  自动文摘 | 信息抽取| 文本挖掘 |
 舆情分析 | 机器写作| 信息过滤 | 。。。

从分析对象和分析内容来看NLP所包含的知识点，可以通过一张[参考图](https://blog.csdn.net/valada/article/details/80892583)来有个更加具体的了解

![](./image/kl_points.jpg)

# NLP技术的历史沿革
过去40年，自然语言处理在技术上基本经历了从规则方法到统计方法再到现在的流行的神经网络方法的变化。


![](./image/nlp_history.jpg)

而且随着数据越来越多，计算能力越来越高，也促使了各种更加先进的神经网络架构、预训练模型等的出现，基于神经网络的自然语言处理得到快速发展，也取的了一些突破性的进展。

![](./image/nlp_bkt.jpg)

# 神经网络自然语言处理技术体系
神经网络自然语言处理的技术体系主要包括了以下几个方面：

+ 词的编码。词编码的目的是用多维向量来表征词的语义。方法有两个，一个是 CBOW（(Continuous Bag-of-Words），用周围的词预测当前的词；另一个是 Skip-gram，用当前的词预测周围的词。规。


+ 句子的编码。一般通过RNN（循环神经网络）或者CNN（卷积神经网络）来做。RNN从左到右对句子进行建模，每个词对应一个隐状态，该引状态代表了从句首到当前词的语义信息，句尾的状态就代表了全句的信息。CNN从理论上分别进行词嵌入+位置嵌入+卷积，加上一个向量表示，对应句子的语义。

+ 编码、解码机制。全句的语义信息，来进行解码，可以从一种语言翻译成另一种语言，凡是从一个序列串变成另外一个序列串都可以通过编码、解码机制来运行。

+ 注意力模型。它综合考量了在当前状态下对应的编码的每一个隐状态，加权平均，来体现当前的动态输入。

+ Transformer。Transformer引入了自编码，一个词跟周围的词建立相似，引入多头，可以引入多种特征表达，所以编码效果或者编码的信息更加丰富。

+ 语言模型。它有几个方法，第一个是ELMo，从左到右对句子编码，也可以从右到左对句子编码，每一层对应的节点并起来，就形成了当前这个词在上下文的语义表示。用的时候就用这个语义加上词本身的词嵌入，来做后续的任务，性能便得到相应的提高。

+ BERT。它用左边、右边的信息来预测最外部的词的信息，同时它也可以判断下一句是真的下一句还是伪造的下一句，用两种方式对句子每一个词进行编码，得到的训练结果就表征了这个词在上下文中的语义表示。基于这样的语义表示，就可以判断两个句子的关系，比如说是不是附属关系，判断一个句子的分类（例如Q&A中，判断回答对应的边界是不是对应提问），以及对输入的每一个词做一个标注，结果就得到一个词性标注。

+ 一系列的新的方法，比如说GPT-2，以及最近的XLNET，以及UNILM、MASS、MT-DNN、XLM，都是基于这种思路的扩充，解决相应的任务各有所长。

## NLP 的新范式
我们可以针对大规模的语料，提前训练好一个模型，这个模型既代表了语言的结构信息，也有可能代表了所在领域甚至常识的信息，只不过我们看不懂。加上我们未来的预定的任务，这个任务只有很小的训练样本，把通过大训练样本得到的预训练模型，做到小训练样本上，效果就得到了非常好的提升。这也就是NLP的新范式： 预训练 + 细调整

![](./image/nlp_nf.jpg)

# 目前存在的问题
虽然神经网络自然语言处理已经取得了巨大的进步，但是仍然存在着很多的问题。我个人感觉包括但不限于以下3个方面

+ 催生泡沫。现在很多巨大的模型，训练过程需要几百甚至上千台GPU同时训练几个月。间接导致了一个问题就是只要模型够复杂，算力要求够高，别人就根本无法复现结果，也无法判断结果的正确与否，滋生了一大批的灌水结果，催生了AI泡沫，甚至加剧了学术造假。

+ 落地困难。很多的研究都是在刷榜，搞一堆机器，弄一堆模型，为了提高排行榜排名想尽各种办法，结果导致训练的模型只适用于这些排行榜的数据，无法应用于实际场景，无法产生经济效益，训练出的模型沦为玩具，最终只是产生了几篇别人看不懂的paper和好看的排名，无甚用。

+ 看着还行，不能细究。 各种各样的语言，有点语料很足，有点则很少。语言本身的复杂多样性，比如中文里的多义词，成语、缩写词等。这些都导致模型在理解语言的时候总体上会感觉挺好的，但是总是存在着一些小问题，包括翻错词，丢词，不合语法，甚至不知所云。多轮对话中如何保持逻辑一致、推理可解释等问题也还没有解决。

# 自然语言处理的未来发展

对于自然语言的未来，周明院长认为要做出可解释、有知识、有道德、可自我学习的NLP系统。并且总结了6个角度：


第一是计算机的能力，刚才说到芯片、存储器、云计算和管理，与之有关的还有模型压缩和加速问题。


第二是数据方面，数据非常重要，全社会都要贡献自己的数据，然后取长补短，大家一起努力。数据上面还有一个隐私保护下的学习，这是非常重要的一点。


第三是模型，刚才说到很多，有监督的学习、无监督的学习、少监督的学习，然后是预训练模型，还有神经网络跟人类知识和常识如何结合，把推理和可解释性融入到我们的学习体系之中。

第四是人才培养，一定要靠人来实现整体的过程。人才如何进行培养呢？要注重实践性，让他们有很强的实践意识，而不是天天去推公式，还要有逻辑上的理解。

第五是合作，校企合作、不同学科的合作、国家的合作、企业界、投资界、政府各个方面的合作，形成一个生态，大家在里面各得其所，来稳步推进。

第六是强调应用，通过应用获得真实的数据、用户的反馈，然后改进我们的系统，也通过应用提升学生的动手能力，也是通过应用使我们了解人和机器在一个真实的系统里如何相得益彰、互相配合，实现人工智能和人类智能的双向结合。

![](./image/nlp_future.jpg)



# 关于
微软-仪电人工智能创新院
微软-仪电人工智能创新院将由微软和仪电共同运营和管理，致力于为微软和仪电在人工智能方面的联合研究活动和项目提供支持，为当地企业提供基于微软技术的人工智能研发平台服务和培训服务。

# 关于人工智能高阶人才培训班
微软和仪电共同打造的人工智能高阶人才培训班由创新院运营，第一期历时三个月，授课老师包括来自微软和仪电的多位专家，内容涵盖人工智能导论、数学基础、深度学习、应用实例等课程，以及关于强化学习、自然语言处理、计算机视觉等热门方向的专题研讨会，希望帮助学员掌握人工智能的理论与实践，培养具备前瞻视野和实践能力的创新型人才。

更多信息，请关注微信公众号
![二维码](./image/barcode.jpg)