2019年7月5日



# 强化学习专题

### 讲课人：刘铁岩

本次课程，刘铁岩老师介绍了强化学习技术的理论及应用，为我们理解和学习强化学习开了一个好头。

首先，刘铁岩老师简要介绍了强化学习概要以及其他机械学习的区别、优劣势。

## 强化学习概要

强化学习的特点：

​	1、从与环境的互动中学习 

​	2、来自环境的反馈/奖励可能会延迟

​	3、奖励可能比人的标签弱，但是，互动可能是动态的，因此比I.I.D.培训数据更具信息性

强化学习的优势领域：

​	1、二阶效应问题

​	2、远期效益问题

强化学习是从动物学习、参数扰动自适应控制等理论发展而来，其基本原理是： 

如果Agent的某个行为策略导致环境正的奖赏(强化信号)，那么Agent以后产生这个行为策略的趋势便会加强。Agent的目标是在每个离散状态发现最优策略以使期望的折扣奖赏和最大。

强化学习把学习看作试探评价过程，Agent选择一个动作用于环境，环境接受该动作后状态发生变化，同时产生一个强化信号(奖或惩)反馈给Agent，Agent根据强化信号和环境当前状态再选择下一个动作，选择的原则是使受到正强化(奖)的概率增大。选择的动作不仅影响立即强化值，而且影响环境下一时刻的状态及最终的强化值。

强化学习不同于连接主义学习中的监督学习，主要表现在教师信号上，强化学习中由环境提供的强化信号是Agent对所产生动作的好坏作一种评价(通常为标量信号)，而不是告诉Agent如何去产生正确的动作。由于外部环境提供了很少的信息，Agent必须靠自身的经历进行学习。通过这种方式，Agent在行动一一评价的环境中获得知识，改进行动方案以适应环境。

强化学习系统学习的目标是动态地调整参数，以达到强化信号最大。若已知r/A梯度信息，则可直接可以使用监督学习算法。因为强化信号r与Agent产生的动作A没有明确的函数形式描述，所以梯度信息r/A无法得到。因此，在强化学习系统中，需要某种随机单元，使用这种随机单元，Agent在可能动作空间中进行搜索并发现正确的动作。

强化学习的常见模型是标准的马尔可夫决策过程（Markov Decision Process, MDP），按给定条件，强化学习可分为基于模式的强化学习（model-based RL）和无模式强化学习（model-free RL）。

## 基础和表格法

### 1） 马尔夫决策

马尔可夫决策过程（Markov Decision Process, MDP）是序贯决策（sequential decision）的数学模型，用于在系统状态具有马尔可夫性质的环境中模拟智能体可实现的随机性策略与回报。MDP的得名来自于俄国数学家安德雷·马尔可夫（Андрей Андреевич Марков），以纪念其为马尔可夫链所做的研究

MDP是在环境中模拟智能体的随机性策略（policy）与回报的数学模型，且环境的状态具有马尔可夫性质。

MDP包含一组交互对象，即智能体和环境 ： 

- 智能体（agent）：MDP中进行机器学习的代理，可以感知外界环境的状态进行决策、对环境做出动作并通过环境的反馈调整决策。
- 环境（environment）：MDP模型中智能体外部所有事物的集合，其状态会受智能体动作的影响而改变，且上述改变可以完全或部分地被智能体感知。环境在每次决策后可能会反馈给智能体相应的奖励。

按定义，MDP包含5个模型要素，状态（state）、动作（action）、策略（policy）、奖励（reward）和回报（return）

按给定条件，强化学习可分为基于模式的强化学习（model-based RL）和无模式强化学习（model-free RL），以及主动强化学习（active RL）和被动强化学习（passive RL）。强化学习的变体包括逆向强化学习、阶层强化学习和部分可观测系统的强化学习。求解强化学习问题所使用的算法可分为策略搜索算法和值函数（value function）算法两类。深度学习模型可以在强化学习中得到使用，形成深度强化学习 

## 表格法

顺序决策中的两个基本问题

计划：

​	已知环境模型

​	Agent使用其模型进行计算

​	Agent进行了决策

​	A.K.A.思考、推理、反省、思考、思考、搜索 

强化学习：

​	环境最初是未知的

​	Agent与环境交互

​	Agent通过探索环境来改进策略

​	

## 基于值和决策的算法

基于值得算法有：

​	DQN：以神经网络作为函数逼近得Q-learning算法

AI玩电子游戏不能直接使用Q-learning，因为电子游戏的每一帧图片就是一种状态，游戏中的角色又可以有多种动作。如果用Q表来记录每一个动作所对应的状态，那么这张Q表将异常巨大。

DQN不用Q表记录Q值，而是用神经网络来预测Q值，并通过不断更新神经网络从而学习到最优的行动路径。

actor-critic： 是policy-iteration方法的参数化，包含两个步骤：1）policy evaluation：估计策略的值函数(相当于critic)；2）policy improvement：根据值函数得到一个更优的新策略(相当于actor)。在复杂问题中，很难让某一个单独收敛，所以采用联合优化方式。

## 应用

强化学习可应用与9大板块：

1、游戏

AlphGo 以及 AlphaStar

AI与人类进行围棋、电子竞技等复杂的游戏中应用。例如在星际争霸2中，AI必须谨慎地平衡宏观经济的大局管理和微观经济的低水平控制才能战胜人类玩家。



2、神经科学

DeepMind团队提出，多巴胺的作用不仅仅是通过奖励来学习过去行为的价值，而且它在帮助我们高效、快速和灵活地学习新任务方面发挥着不可或缺的作用，特别是在前额叶皮层区域。  

3、艺术&娱乐

1）基于LSTM及RL.tuner的AI音乐编曲。

2）基于深度强化学习的地形适应运动技能。 

3）SFV：从视频中加强物理技能的学习。  

4、健康医疗 

1）医疗保健中的顺序决策 

2）人工胰腺 

3）化学反应优化

5、交易 

1）交易中的顺序决策 

2）反复强化学习（RRL）

6、自然语言处理 

1）富结构NLP

2）会话智能体

7、车辆&机器人 

1）机器人的顺序决策

2）深度监管决策的端到端训练

3）自动驾驶

8、交通&能源系统 

1）基于不确定性正则化的密集交通驾驶模型预测策略学习 

2）基于分布式多智能体Q-learning的智能交通灯控制 

3）基于深度强化学习的自适应电力系统紧急控制

9、 物流

供需计划预测



## 正在做的

OpenAI--Gym

提供各种游戏环境，包括离散和连续控制问题

Algorithmic:
提供了学习算法的环境，比如翻转序列这样的问题，虽然能很容易用直接编程实现，但是单纯用例子来训练RL模型有难度的。这些问题有一个很好的特性： 能够通过改变序列长度改变难度。 
Atari:
这里提供了一些小游戏，比如我们小时候玩过的小蜜蜂，弹珠等等。这些问题对RL研究有着很大影响！ 
Board games:
提供了Go这样一个简单的下棋游戏，由于这个问题是多人游戏，Gym提供有opponent与你训练的agent进行对抗。 
2D and 3D robots:
机器人控制环境。 这些问题用 MuJoCo 作为物理引擎。 

## 挑战与机遇

1、不完全信息博弈与多智能体博弈

2、鲁棒性-跨任务

3、样品效率

4、安全强化学习







