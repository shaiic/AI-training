# 微软-仪电人工智能高阶人才培训(第一期）学习心得之六：线性分类与非线性分类课程
## 作者：施海全（上海仪电人工智能创新院有限公司）

[TOC]

     想不到在离开校园10多年后，还会有机会重新走进课堂进行机器学习方面的学习。在IT行业工作这些年也没用到什么很复杂的数学知识，但由于机器学习又让我捡起了这些知识，让我重新认识了数学的魅力和重要性！经过2个月的学习，重新拾起了高等数学、线性代数这些大学的知识(说实话这些数学概念以前只是学了，不知道学了有什么用，学完也就还给老师了)，也学习了简明扼要而又神奇的Python，由浅入深得学习了机器学习方面得各种知识。感谢集团提供难能可贵的学习机会，感谢微软各位老师得辛勤指导，也为我指明了未来的工作和学习的方向。


    记得刚学编程的时候，写了个简单的程序，自己感觉很神奇啊，能指挥机器干活了。现在机器学习又让我神奇了一次，机器居然能自己通过数据来学习了。
    曾经，潜意识里也有过那么一两次问过自己，为什么不能让机器自己去学习数据的规律，探索数据里未曾发觉的秘密呢？但是一刹那后就放弃了，由于程序员的固定思维，认为机器只是人类的工具(如计算器)，只能根据人为设定好的程序进行计算和处理。
    数据无可否认得现在变得越来越重要了。从事数据分析相关的工作这么多年来，以前只能根据人为的经验假设数据符合某一种统计模型，然后依据样本数据来估计模型的一些参数，以此了解数据的特征，完成这种有针对性和目的性的数据分析。但实际中有很多数据并不符合假设的统计模型，这导致数据分析结果不理想。传统的数据分析是基于人的，人的业务理解能力、数学等知识以及相关的其他经验都决定了数据分析的好坏和成功与否。但是有了机器学习，尤其是近几年很火的深度神经网络后，它们具备高级的抽象能力，在经过多层的迭代后可以挖掘出数据后面的规律。



以下是晓武老师关于线性分类和非线性分类的笔记
# 1.线行分类

## 1.1神奇的线性分类
经过数学推导后可以知道，神经网络实际也是在做这样一件事：经过调整w和b的值，把所有正例的样本都归纳到大于0.5的范围内，所有负例都小于0.5。但是如果只说大于或者小于，无法做准确的量化计算，所以用一个对率函数来模拟。

说到对率函数，还有一个问题，它为啥叫做“对数几率”函数呢？从哪里看出是“对数”了？“几率”是什么意思呢？

我们举例说明：假设有一个硬币，抛出落地后，得到正面的概率是0.5，得到反面的概率是0.5，这两个概率叫做probability。如果用正面的概率除以反面的概率，0.5/0.5=1，这个数值叫做odds，几率。

泛化一下，如果正面的概率是a，则反面的概率就是1-a，则几率等于：

$$odds = \frac{a}{1-a} $$

上式中，如果a是把样本x的预测为正例的可能性，那么1-a就是其负例的可能性，a/(1-a)就是正负例的比值，称为几率(odds)，反映了x作为正例的相对可能性，而对几率取对数就叫做对数几率(log odds, logit)。

如果假设概率如下表：

|a|0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1|
|--|--|--|--|--|--|--|--|--|--|--|--|
|1-a|1|0.9|0.8|0.7|0.6|0.5|0.4|0.3|0.2|0.1|0|
|odds|0|0.11|0.25|0.43|0.67|1|1.5|2.33|4|9|无穷大|
|ln(odds)|N/A|-2.19|-1.38|-0.84|-0.4|0|0.4|0.84|1.38|2.19|N/A|

可以看到0dds的值不是线性的，不利于分析问题，所以在表中第4行对odds取对数，可以得到一组成线性关系的值，即：

$$\ln \frac{a}{1-a} = xw + b \tag{10}$$

对公式10两边取自然指数：

$$\frac{a}{1-a}=e^{xw+b} \tag{11}$$

进过一系列数学推导后
令$z=e^{-(xw+b)}$：

$$a=\frac{1}{1+e^{-z}} \tag{12}$$

对数几率的函数形式就这样神奇的出来了。

## 1.2线性二分类
对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

此函数实际上是一个概率计算，它把$(-\infty, \infty)$之间的任何数字都压缩到$(0,1)$之间，返回一个概率值，这个概率值接近1时，认为是正例，否则认为是负例。

训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Logistic计算后，输出一个$(0,1)$之间的预测值。我们假设这个样本的标签值为0属于负类，如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。

推理时，我们预先设定一个阈值比如0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反，准确度越低，召回率越高。


$$
z = x \cdot w + b
=\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
\begin{pmatrix}
    w_1 \\ w_2
\end{pmatrix}
$$
$$
=x_1 \cdot w_1 + x_2 \cdot w_2 + b \tag{1}
$$
$$a = Logistic(z) \tag{2}$$

#### 损失函数

二分类交叉熵函损失数：

$$
loss(w,b) = -[yln a+(1-y)ln(1-a)] \tag{3}
$$

## 1.3线性多分类
### 1.3.1 多分类函数
#### 为什么叫做Softmax？

假设输入值是：[3,1,-3]，如果取max操作会变成：[1,0,0]，这符合我们的分类需要。但是有两个不足：
1. 分类结果是[1，0，0]，只保留的非0即1的信息，没有各元素之间相差多少的信息，可以理解是“Hard-Max”
2. max操作本身不可导，无法用在反向传播中。

所以Softmax加了个"soft"来模拟max的行为，但同时又保留了相对大小的信息。

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}}
$$

假设j=1，m=3，上式为：

$$a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}$$

#### 矩阵运算

$$
z=x \cdot w + b \tag{1}
$$

#### 分类计算

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} \tag{2}
$$

#### 损失函数计算

计算单样本时，m是分类数：
$$
loss(w,b)=-\sum_{i=1}^m y_i \ln a_i \tag{3}
$$


# 2.非线性回归
有一个定理：任意一个函数在一个较小的范围内，都可以用多项式任意逼近。因此在实际工程实践中，有时候可以不管y值与x值的数学关系究竟是什么，而是强行用回归分析方法进行近似的拟合。

那么如何得到更多的特征值呢？对于只有一个特征值的问题，人们发明了一种聪明的办法，就是把特征值的高次方作为另外的特征值，加入到回归分析中，用公式描述：

$$z = x w_1 + x^2 w_2 + ... + x^m w_m + b \tag{3}$$

上式中x是原有的唯一特征值，$x^m$是利用x的m次方作为额外的特征值，这样就把特征值的数量从1个变为m个。
## 2.1双层神经网络实现非线性回归
### 2.1.2 万能近似定理
简言之：两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。当然这个函数需要是单调递增有界的。

- 注意，它要求的是挤压性质的激活函数，也就是类似Sigmoid的函数，如果用ReLU函数不能实现这个效果。

万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的MLP一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。即使MLP能够表示该函数，学习也可能因两个不同的原因而失败。
### 2.1.3 定义神经网络结构
根据万能近似定理的要求，我们定义一个两层的神经网络，输入层不算，一个隐藏层，含3个神经元，一个输出层。

为什么用3个神经元呢？因为输入层只有一个特征值，我们不需要在隐层放很多的神经元，先用3个神经元试验一下。如果不够的话再增加，神经元数量是由超参控制的。

<img src="./Image/nn.png">![Identity](./image/l

### 2.1.3 前向计算

#### 隐层

- 线性计算

$$
z^1_{1} = x \cdot w^1_{11} + b^1_{1}
$$
$$
z^1_{2} = x \cdot w^1_{12} + b^1_{2}
$$
$$
z^1_{3} = x \cdot w^1_{13} + b^1_{3}
$$
$$
Z1 = X \cdot W1 + B1 \tag{1}
$$

- 激活函数

$$
a^1_{1} = Sigmoid(z^1_{1})
$$
$$
a^1_{2} = Sigmoid(z^1_{2})
$$
$$
a^1_{3} = Sigmoid(z^1_{3})
$$
$$
A1 = Sigmoid(Z1) \tag{2}
$$

#### 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元：

$$
z=a^1_{1}w^2_{11}+a^1_{2}w^2_{21}+a^1_{3}w^2_{31}+b^2_{1}
$$

矩阵形式：

$$Z=A1 \cdot W2+B2 \tag{3}$$

#### 损失函数

均方差损失函数：

$$loss(w,b) = \frac{1}{2} (z-y)^2 \tag{4}$$

其中，$z$是样本预测值，$y$是样本的标签值，这里的z是第二层的输出Z。
#### 求损失函数对隐层的反向误差

下面的内容是双层神经网络独有的内容，也是深度神经网络的基础，请大家仔细阅读体会。我们先看看正向计算和反向计算图：

<img src="./Image/backward.png">

图中：
- 蓝色矩形表示数值或矩阵
- 蓝色圆形表示计算单元
- 蓝色的箭头表示正向计算过程
- 红色的箭头表示反向计算过程

如果想计算W1和B1的反向误差，必须先得到Z1的反向误差，再向上追溯，可以看到Z1->A1->Z2->Loss这条线，Z1->A1是一个激活函数的运算，比较特殊，所以我们先看Loss->Z->A1如何解决。

----
# 关于
微软-仪电人工智能创新院
微软-仪电人工智能创新院将由微软和仪电共同运营和管理，致力于为微软和仪电在人工智能方面的联合研究活动和项目提供支持，为当地企业提供基于微软技术的人工智能研发平台服务和培训服务。

# 关于人工智能高阶人才培训班
微软和仪电共同打造的人工智能高阶人才培训班由创新院运营，第一期历时三个月，授课老师包括来自微软和仪电的多位专家，内容涵盖人工智能导论、数学基础、深度学习、应用实例等课程，以及关于强化学习、自然语言处理、计算机视觉等热门方向的专题研讨会，希望帮助学员掌握人工智能的理论与实践，培养具备前瞻视野和实践能力的创新型人才。

更多信息，请关注微信公众号
![二维码](./image/barcode.jpg)
