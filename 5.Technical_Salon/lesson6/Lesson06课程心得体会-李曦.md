# 微软-仪电人工智能高阶人才培训(第一期）学习心得之六：线性分类与线性非分类课程
## 作者：***（上海仪电人工智能创新院有限公司）

# 本周只听了两个上午的课程...

这周因为有重要工作安排，所以只听了周五上午半天与周六上午半天的课程，写这篇心得的过程，也是学习的过程。

分类问题在很多资料中都称之为逻辑回归，Logistic Regression，其原因是使用了线性回归中的线性模型，加上一个Logistic二分类函数，共同构造了一个分类器，感觉一个是从方法出发，应该是从目的出发，后者在神经网络中更容易理解。

首先是线性二分类，还是找实际场景来描述问题，这次是楚河汉界。这次我们学习的是逻辑回归。回忆线性回归，使用一条直线拟合样本数据，而逻辑回归是“拟合”0或1两个数值，而不是具体的连续数值，所以它叫广义线性模型。逻辑回归又称logistic回归分析，常用于数据挖掘，疾病早筛，产业地图等实际应用场景。逻辑回归分为线性和分线性，二分和多分两两组合。

线性回归用代数方式来看，通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5。

老师最后一段的总结很干活，本质上是用线性回归模型的预测结果来逼近样本分类的对数几率。这就是为什么它叫做逻辑回归(logistic regression)，但其实是分类学习的方法。这种方法的优点如下：

- 直接对分类可能性建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题---尽量少猜，少造数据
- 不仅预测出类别，而是得到了近似的概率，这对许多需要利用概率辅助决策的任务很有用
- 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解

至于线性二分类的神经网络实现有两处变化，损失函数，从均方差变成了交叉熵，增加了激活函数，看上去就是方便计算.....用双曲正切函数做二分函数是我上午听的最后一个topic，据说下午老师又讲了一遍，而对率函数相对于双曲正切函数又扩大了输出值域。

线性多分类问题，场景描述就从楚汉相争变成三国鼎立，二分类方法可以演变为多分类，有三种方法：一对一，一对多，多对多。

接着就是Softmax函数，max函数只有1和0，差异性不够，同时因为不可导无法应用于反向传播。字面意思就是软件定义max的意思，所以更flexibility吧。

线性多分类与前面的单层网络不同的是，输出层还多出来一个Softmax分类函数，这是多分类任务中的标准配置，可以看作是输出层的激活函数，并不单独成为一层，与二分类中的Logistic函数一样。

周六的课程正式从单层神经网络进入了两层神经网络，需要增加激活函数来加入非线性因素，提高神经网络的能力。

激活函数的基本性质：

- 非线性：线性的激活函数和没有激活函数一样
- 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性
- 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出

挤压型激活函数 感觉就是S曲线函数，所以适合激活模拟概率分布问题，但是计算复杂度较高。本身零均值的tanh在传递过程中，不会影响输入数据的均值。非线性回归的二层网络在神经网络中，使用带有一个隐层的两层神经网络。

用多项式回归法拟合正弦曲线，这里面感觉又引入一个需要经验式来处理的，多项式次数。

双层神经网络实现非线性回归，这里引入了万能近似定理，最终通过函数实现和曲线拟合验证，最终我们得出结论只要是数值拟合问题，确定不能用线性回归的话，都可以用非线性回归来尝试解决。

最后一个概念是超参优化，参数设置可以用人工搜索、网格搜索和随机搜索。这其中LR是最重要的参数。

感悟就是真不能落下课，自己啃资料很容易云里雾里，期待视频回放早日出炉...



# 关于
微软-仪电人工智能创新院
微软-仪电人工智能创新院将由微软和仪电共同运营和管理，致力于为微软和仪电在人工智能方面的联合研究活动和项目提供支持，为当地企业提供基于微软技术的人工智能研发平台服务和培训服务。

# 关于人工智能高阶人才培训班
微软和仪电共同打造的人工智能高阶人才培训班由创新院运营，第一期历时三个月，授课老师包括来自微软和仪电的多位专家，内容涵盖人工智能导论、数学基础、深度学习、应用实例等课程，以及关于强化学习、自然语言处理、计算机视觉等热门方向的专题研讨会，希望帮助学员掌握人工智能的理论与实践，培养具备前瞻视野和实践能力的创新型人才。

更多信息，请关注微信公众号
![二维码](./image/barcode.jpg)